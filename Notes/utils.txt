the_aichemist_codex/
└── backend/
    └── utils/
         ├── __init__.py
         ├── common/
         │    ├── __init__.py
         │    ├── common.py           # General helper functions (e.g., canonicalize_name, summarization, etc.)
         │    ├── patterns.py         # File ignore pattern matching utilities (from current patterns.py)
         │    └── safety.py           # Safe file operations and directory checks (from current safety.py)
         ├── io/
         │    ├── __init__.py
         │    ├── async_io.py         # Asynchronous file operations
         │    ├── sqlasync_io.py      # Asynchronous SQLite operations
         │    └── mime_type_detector.py  # MIME type detection utilities
         ├── concurrency/
         │    ├── __init__.py
         │    ├── concurrency.py      # Concurrency primitives, thread pool executor, etc.
         │    └── batch_processor.py  # Batch processing for async tasks
         ├── cache/
         │    ├── __init__.py
         │    └── cache_manager.py    # In-memory LRU and disk-based caching
         ├── environment/
         │    ├── __init__.py
         │    ├── environment.py      # Environment detection (development mode, import mode, etc.)
         │    ├── validator.py        # Validation utilities (e.g., get_project_name)
         │    └── validate_data_dir.py  # Data directory structure validation and repair
         └── errors/
              ├── __init__.py
              └── errors.py         # Custom exceptions (e.g., CodexError, MaxTokenError, NotebookProcessingError)


================================================
File: src/the_aichemist_codex/backend/utils/common/__init__.py
================================================
"""Utility functions for The Aichemist Codex."""

from .async_io import AsyncFileIO, AsyncFileReader, AsyncFileTools
from .errors import CodexError, MaxTokenError, NotebookProcessingError
from .patterns import pattern_matcher
from .safety import SafeFileHandler
from .sqlasync_io import AsyncSQL
from .validator import get_project_name

__all__ = [
    "AsyncSQL",
    "AsyncFileIO",
    "AsyncFileTools",
    "AsyncFileReader",
    "CodexError",
    "MaxTokenError",
    "NotebookProcessingError",
    "SafeFileHandler",
    "get_project_name",
    "pattern_matcher",
]


================================================
File: src/the_aichemist_codex/backend/utils/io/async_io.py
================================================
"""Asynchronous file operations for The Aichemist Codex."""

import asyncio
import logging
import os
from collections.abc import AsyncGenerator, AsyncIterable, Callable
from pathlib import Path
from typing import Any, cast

import aiofiles  # type: ignore
import aiofiles.os  # type: ignore

from the_aichemist_codex.backend.utils.common.safety import SafeFileHandler

logger = logging.getLogger(__name__)


class AsyncFileIO:
    """Provides comprehensive asynchronous file I/O utilities."""

    @staticmethod
    async def read_text(file_path: Path) -> str:
        """Reads file content asynchronously.

        Args:
            file_path: Path to the file to read

        Returns:
            The file content as a string, or an error message if the file can't be read
        """
        if SafeFileHandler.should_ignore(file_path):
            logger.info(f"Skipping ignored file: {file_path}")
            return f"# Skipped ignored file: {file_path}"

        try:
            async with aiofiles.open(file_path, encoding="utf-8") as f:
                content = await f.read()
                return cast(str, content)
        except UnicodeDecodeError as e:
            logger.error(f"Encoding error in {file_path}: {e}")
            return f"# Encoding error: {e}"
        except Exception as e:
            logger.error(f"Error reading {file_path}: {e}")
            return f"# Error reading file: {e}"

    @staticmethod
    async def read_binary(file_path: Path) -> bytes:
        """Reads binary file content asynchronously.

        Skips files that match the default ignore patterns using `SafeFileHandler`.

        Args:
            file_path: Path to the binary file to read

        Returns:
            The file content as bytes, or empty bytes if the file can't be read
        """
        if SafeFileHandler.should_ignore(file_path):
            logger.info(f"Skipping ignored binary file: {file_path}")
            return b""

        try:
            async with aiofiles.open(file_path, "rb") as f:
                content = await f.read()
                return cast(bytes, content)
        except Exception as e:
            logger.error(f"Error reading binary file {file_path}: {e}")
            return b""

    @staticmethod
    async def write(file_path: Path, content: str, encoding: str = "utf-8") -> bool:
        """Writes string content to a file asynchronously.

        Args:
            file_path: Path where the file should be written
            content: String content to write
            encoding: File encoding to use

        Returns:
            True if successful, False otherwise
        """
        try:
            # Create directory if it doesn't exist
            os.makedirs(file_path.parent, exist_ok=True)

            async with aiofiles.open(file_path, "w", encoding=encoding) as f:
                await f.write(content)
            return True
        except Exception as e:
            logger.error(f"Error writing to {file_path}: {e}")
            return False

    @staticmethod
    async def write_binary(file_path: Path, content: bytes) -> bool:
        """Writes binary content to a file asynchronously.

        Args:
            file_path: Path where the file should be written
            content: Binary content to write

        Returns:
            True if successful, False otherwise
        """
        try:
            # Create directory if it doesn't exist
            os.makedirs(file_path.parent, exist_ok=True)

            async with aiofiles.open(file_path, "wb") as f:
                await f.write(content)
            return True
        except Exception as e:
            logger.error(f"Error writing binary data to {file_path}: {e}")
            return False

    @staticmethod
    async def append(file_path: Path, content: str, encoding: str = "utf-8") -> bool:
        """Appends string content to a file asynchronously.

        Args:
            file_path: Path where the content should be appended
            content: String content to append
            encoding: File encoding to use

        Returns:
            True if successful, False otherwise
        """
        try:
            # Create directory if it doesn't exist
            os.makedirs(file_path.parent, exist_ok=True)

            async with aiofiles.open(file_path, "a", encoding=encoding) as f:
                await f.write(content)
            return True
        except Exception as e:
            logger.error(f"Error appending to {file_path}: {e}")
            return False

    @staticmethod
    async def read_lines(file_path: Path) -> list[str]:
        """Reads file lines asynchronously.

        Args:
            file_path: Path to the file to read

        Returns:
            List of lines from the file, or empty list on error
        """
        if SafeFileHandler.should_ignore(file_path):
            logger.info(f"Skipping ignored file: {file_path}")
            return []

        try:
            async with aiofiles.open(file_path, encoding="utf-8") as f:
                lines = await f.readlines()
                return cast(list[str], lines)
        except Exception as e:
            logger.error(f"Error reading lines from {file_path}: {e}")
            return []

    @staticmethod
    async def read_json(file_path: Path) -> dict[Any, Any]:
        """Reads and parses JSON file asynchronously.

        Args:
            file_path: Path to the JSON file

        Returns:
            Parsed JSON object or empty dict on error
        """
        import json

        try:
            content = await AsyncFileIO.read_text(file_path)
            if content.startswith("#"):  # Error reading file
                return {}
            parsed_json = json.loads(content)
            return cast(dict[Any, Any], parsed_json)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in {file_path}: {e}")
            return {}
        except Exception as e:
            logger.error(f"Error parsing JSON from {file_path}: {e}")
            return {}

    @staticmethod
    async def write_json(file_path: Path, data: Any, indent: int = 4) -> bool:
        """Writes data as JSON to a file asynchronously.

        Args:
            file_path: Path where the JSON should be written
            data: Data to serialize as JSON
            indent: Number of spaces for indentation

        Returns:
            True if successful, False otherwise
        """
        import json

        try:
            json_str = json.dumps(data, indent=indent)
            return await AsyncFileIO.write(file_path, json_str)
        except Exception as e:
            logger.error(f"Error writing JSON to {file_path}: {e}")
            return False

    @staticmethod
    async def exists(file_path: Path) -> bool:
        """Checks if a file exists asynchronously.

        Args:
            file_path: Path to check

        Returns:
            True if the file exists, False otherwise
        """
        return file_path.exists()

    @staticmethod
    async def copy(source: Path, destination: Path) -> bool:
        """Copies a file asynchronously.

        Args:
            source: Source file path
            destination: Destination file path

        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure destination directory exists
            os.makedirs(destination.parent, exist_ok=True)

            if await AsyncFileIO.exists(source):
                if source.is_file():
                    content = await AsyncFileIO.read_binary(source)
                    return await AsyncFileIO.write_binary(destination, content)
                else:
                    logger.error(f"Source {source} is not a file")
                    return False
            else:
                logger.error(f"Source file {source} does not exist")
                return False
        except Exception as e:
            logger.error(f"Error copying {source} to {destination}: {e}")
            return False

    @staticmethod
    async def read_chunked(
        file_path: Path, chunk_size: int = 8192, buffer_limit: int | None = None
    ) -> AsyncGenerator[bytes]:
        """
        Read a file in chunks to limit memory usage.

        Args:
            file_path: Path to the file to read
            chunk_size: Size of each chunk in bytes
            buffer_limit: Maximum number of chunks to buffer (None for no limit)

        Yields:
            Chunks of file content as bytes
        """
        if SafeFileHandler.should_ignore(file_path):
            logger.info(f"Skipping ignored file: {file_path}")
            return

        try:
            if buffer_limit:
                semaphore = asyncio.Semaphore(buffer_limit)

            async with aiofiles.open(file_path, "rb") as f:
                while True:
                    if buffer_limit:
                        await semaphore.acquire()

                    chunk = await f.read(chunk_size)
                    if not chunk:
                        break

                    yield chunk

                    if buffer_limit:
                        semaphore.release()
        except Exception as e:
            logger.error(f"Error reading {file_path} in chunks: {e}")

    @staticmethod
    async def write_chunked(
        file_path: Path,
        content_iterator: AsyncIterable[bytes],
        buffer_limit: int | None = None,
    ) -> bool:
        """Writes content to a file asynchronously in chunks.

        Args:
            file_path: Path to the file to write
            content_iterator: Iterator of content chunks to write
            buffer_limit: Maximum number of chunks to buffer at once

        Returns:
            True if successful, False otherwise
        """
        try:
            file_path.parent.mkdir(parents=True, exist_ok=True)
            async with aiofiles.open(file_path, "wb") as f:
                async for chunk in content_iterator:
                    await f.write(chunk)
                    if buffer_limit:
                        await asyncio.sleep(0)
            return True
        except Exception as e:
            logger.error(f"Error writing to {file_path}: {e}")
            return False

    @staticmethod
    async def copy_chunked(
        source: Path,
        destination: Path,
        chunk_size: int = 8192,
        buffer_limit: int | None = None,
    ) -> bool:
        """
        Copy a file in chunks to limit memory usage.

        Args:
            source: Source file path
            destination: Destination file path
            chunk_size: Size of each chunk in bytes
            buffer_limit: Maximum number of chunks to buffer (None for no limit)

        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure destination directory exists
            os.makedirs(destination.parent, exist_ok=True)

            if not await AsyncFileIO.exists(source):
                logger.error(f"Source file {source} does not exist")
                return False

            if not source.is_file():
                logger.error(f"Source {source} is not a file")
                return False

            async for chunk in AsyncFileIO.read_chunked(
                source, chunk_size, buffer_limit
            ):
                if not await AsyncFileIO.write_chunked(
                    destination,
                    AsyncFileIO.list_to_async_iterable([chunk]),
                    buffer_limit,
                ):
                    return False

            return True
        except Exception as e:
            logger.error(f"Error copying {source} to {destination}: {e}")
            return False

    @staticmethod
    async def process_large_file(
        file_path: Path,
        processor: Callable[[bytes], Any],
        chunk_size: int = 8192,
        buffer_limit: int | None = None,
    ) -> bool:
        """
        Process a large file in chunks with a custom processor function.

        Args:
            file_path: Path to the file to process
            processor: Async function that processes each chunk
            chunk_size: Size of each chunk in bytes
            buffer_limit: Maximum number of chunks to buffer (None for no limit)

        Returns:
            True if successful, False otherwise
        """
        try:
            async for chunk in AsyncFileIO.read_chunked(
                file_path, chunk_size, buffer_limit
            ):
                await processor(chunk)
            return True
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            return False

    @staticmethod
    async def get_file_size(file_path: Path) -> int | None:
        """
        Get file size without reading the entire file.

        Args:
            file_path: Path to the file

        Returns:
            File size in bytes or None if error
        """
        try:
            stats = await aiofiles.os.stat(file_path)
            return cast(int, stats.st_size)
        except Exception as e:
            logger.error(f"Error getting size of {file_path}: {e}")
            return None

    @staticmethod
    async def stream_append(
        file_path: Path,
        content_iterator: AsyncIterable[bytes],
        buffer_limit: int | None = None,
    ) -> bool:
        """
        Append content to a file in streaming fashion.

        Args:
            file_path: Path to the file
            content_iterator: Async iterator yielding content chunks
            buffer_limit: Maximum number of chunks to buffer (None for no limit)

        Returns:
            True if successful, False otherwise
        """
        try:
            # Create directory if it doesn't exist
            os.makedirs(file_path.parent, exist_ok=True)

            if buffer_limit:
                semaphore = asyncio.Semaphore(buffer_limit)

            async with aiofiles.open(file_path, "ab") as f:
                async for chunk in content_iterator:
                    if buffer_limit:
                        await semaphore.acquire()

                    await f.write(chunk)

                    if buffer_limit:
                        semaphore.release()
            return True
        except Exception as e:
            logger.error(f"Error appending chunks to {file_path}: {e}")
            return False

    @staticmethod
    async def list_to_async_iterable(items: list[bytes]) -> AsyncGenerator[bytes]:
        """Converts a list to an async iterable.

        Args:
            items: List of items to convert

        Yields:
            Items from the list one by one
        """
        for item in items:
            yield item


# Create aliases for backward compatibility
AsyncFileTools = AsyncFileIO
AsyncFileReader = AsyncFileIO


================================================
File: src/the_aichemist_codex/backend/utils/concurrency/batch_processor.py
================================================
"""Provides batch processing capabilities for efficient operations."""

import asyncio
import logging
from collections.abc import Awaitable, Callable
from typing import Any, TypeVar

logger = logging.getLogger(__name__)
T = TypeVar("T")


class BatchProcessor:
    """Handles batch processing with async support and error handling."""

    @staticmethod
    async def process_batch(
        items: list[Any],
        operation: Callable[[Any], Awaitable[Any]],
        batch_size: int = 10,
        timeout: int = 30,
    ) -> list[Any]:
        """
        Process items in batches using the provided async operation.

        Args:
            items: List of items to process
            operation: Async function to apply to each item
            batch_size: Number of items to process in each batch
            timeout: Maximum wait time in seconds

        Returns:
            List of results from successful operations
        """
        results = []
        for i in range(0, len(items), batch_size):
            batch = items[i : i + batch_size]
            batch_tasks = [operation(item) for item in batch]
            try:
                batch_results = await asyncio.gather(
                    *batch_tasks, return_exceptions=True
                )
                # Filter out exceptions and add successful results
                for result in batch_results:
                    if not isinstance(result, Exception):
                        results.append(result)
                    else:
                        logger.error(f"Batch operation error: {result}")
            except TimeoutError:
                logger.error(f"Batch operation timed out after {timeout} seconds")

        return results


================================================
File: src/the_aichemist_codex/backend/utils/cache/cache_manager.py
================================================
"""Provides caching capabilities for performance optimization."""

import logging
import os
import re
import time
from collections import OrderedDict
from pathlib import Path
from typing import Any

from the_aichemist_codex.backend.utils.io.async_io import AsyncFileIO

logger = logging.getLogger(__name__)


def get_cache_dir() -> Path:
    """
    Get the cache directory path dynamically to avoid circular imports.

    Returns:
        Path: The cache directory path
    """
    # Check for environment variable first
    env_cache_dir = os.environ.get("AICHEMIST_CACHE_DIR")
    if env_cache_dir:
        return Path(env_cache_dir).resolve()

    # Check for environment variable for data directory
    env_data_dir = os.environ.get("AICHEMIST_DATA_DIR")
    if env_data_dir:
        data_dir = Path(env_data_dir).resolve()
        return data_dir / "cache"

    # Fallback to a relative path from the current file
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent.parent.parent
    return project_root / "data" / "cache"


class LRUCache:
    """Limited size in-memory LRU cache implementation."""

    def __init__(self, max_size: int = 1000):
        """Initialize LRU cache with maximum size."""
        self.cache: OrderedDict[str, Any] = OrderedDict()
        self.max_size = max_size

    def get(self, key: str) -> Any | None:
        """Get value from cache, moving item to end (most recently used)."""
        if key not in self.cache:
            return None

        # Move to end (mark as recently used)
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: str, value: Any) -> None:
        """Add or update value in cache, evicting oldest item if full."""
        # If already exists, update and move to end
        if key in self.cache:
            self.cache.move_to_end(key)
            self.cache[key] = value
            return

        # Add new item
        self.cache[key] = value

        # Evict oldest item if needed
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)


class CacheManager:
    """Manages both in-memory and disk-based caching."""

    def __init__(
        self,
        cache_dir: Path | None = None,
        memory_cache_size: int = 1000,
        disk_cache_ttl: int = 3600,
    ):  # 1 hour TTL by default
        """Initialize the cache manager with both memory and disk caches."""
        # Use the provided cache_dir or get it dynamically
        self.cache_dir = cache_dir if cache_dir is not None else get_cache_dir()

        # Ensure the cache directory exists
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.memory_cache = LRUCache(max_size=memory_cache_size)
        self.disk_cache_ttl = disk_cache_ttl
        self.async_io = AsyncFileIO()

        logger.debug(f"Cache manager initialized with directory: {self.cache_dir}")

    @staticmethod
    def sanitize_key(key: str) -> str:
        """
        Sanitize cache key to ensure it can be used as a filename.

        Replaces invalid characters with underscores.

        Args:
            key: The original cache key

        Returns:
            Sanitized key safe for use in filenames
        """
        # Replace characters that are illegal in filenames
        # This includes: <, >, :, ", /, \, |, ?, and *
        sanitized = re.sub(r'[<>:"/\\|?*]', "_", key)

        # Limit maximum length to avoid overly long filenames
        if len(sanitized) > 200:
            # Keep the beginning and end, replacing the middle with a hash
            import hashlib

            middle_hash = hashlib.md5(key.encode()).hexdigest()[:8]
            sanitized = sanitized[:95] + "_" + middle_hash + "_" + sanitized[-95:]

        return sanitized

    async def get(self, key: str) -> Any | None:
        """
        Get item from cache (memory first, then disk).

        Args:
            key: Cache key to retrieve

        Returns:
            Cached item or None if not found or expired
        """
        # Try memory cache first
        memory_result = self.memory_cache.get(key)
        if memory_result is not None:
            return memory_result

        # Try disk cache
        sanitized_key = self.sanitize_key(key)
        cache_file = self.cache_dir / f"{sanitized_key}.json"
        if await self.async_io.exists(cache_file):
            try:
                # Check if cache entry is expired
                stats = os.stat(cache_file)
                if time.time() - stats.st_mtime > self.disk_cache_ttl:
                    await self.invalidate(key)
                    return None

                data = await self.async_io.read_json(cache_file)
                if data:
                    # Update memory cache
                    self.memory_cache.put(key, data)
                    return data
            except Exception as e:
                logger.error(f"Error reading cache file {cache_file}: {e}")

        return None

    async def put(self, key: str, value: Any) -> bool:
        """
        Store item in both memory and disk cache.

        Args:
            key: Cache key
            value: Data to cache (must be JSON serializable)

        Returns:
            True if successful, False otherwise
        """
        try:
            # Update memory cache
            self.memory_cache.put(key, value)

            # Update disk cache with sanitized key
            sanitized_key = self.sanitize_key(key)
            cache_file = self.cache_dir / f"{sanitized_key}.json"
            return await self.async_io.write_json(cache_file, value)
        except Exception as e:
            logger.error(f"Error writing to cache for key {key}: {e}")
            return False

    async def invalidate(self, key: str) -> None:
        """
        Remove item from both memory and disk cache.

        Args:
            key: Cache key to invalidate
        """
        # Remove from memory cache
        if key in self.memory_cache.cache:
            del self.memory_cache.cache[key]

        # Remove from disk cache with sanitized key
        sanitized_key = self.sanitize_key(key)
        cache_file = self.cache_dir / f"{sanitized_key}.json"
        if await self.async_io.exists(cache_file):
            try:
                os.remove(cache_file)
            except Exception as e:
                logger.error(f"Error removing cache file {cache_file}: {e}")

    async def invalidate_pattern(self, pattern: str) -> None:
        """
        Remove all cache entries matching a pattern.

        Args:
            pattern: Pattern to match against cache keys
        """
        # Remove from memory cache
        keys_to_remove = [
            key for key in self.memory_cache.cache.keys() if pattern in key
        ]
        for key in keys_to_remove:
            del self.memory_cache.cache[key]

        # Remove from disk cache - use sanitized pattern
        sanitized_pattern = self.sanitize_key(pattern)
        try:
            for cache_file in self.cache_dir.glob(f"*{sanitized_pattern}*.json"):
                try:
                    os.remove(cache_file)
                except Exception as e:
                    logger.error(f"Error removing cache file {cache_file}: {e}")
        except Exception as e:
            logger.error(f"Error removing cache files matching pattern {pattern}: {e}")

    async def clear(self) -> None:
        """Clear all cache entries from both memory and disk."""
        # Clear memory cache
        self.memory_cache.cache.clear()

        # Clear disk cache
        try:
            for cache_file in self.cache_dir.glob("*.json"):
                try:
                    os.remove(cache_file)
                except Exception as e:
                    logger.error(f"Error removing cache file {cache_file}: {e}")
        except Exception as e:
            logger.error(f"Error clearing disk cache: {e}")

    async def get_stats(self) -> dict[str, Any]:
        """
        Get cache statistics.

        Returns:
            Dictionary containing cache statistics
        """
        disk_cache_size = 0
        disk_cache_count = 0
        try:
            for cache_file in self.cache_dir.glob("*.json"):
                disk_cache_size += os.path.getsize(cache_file)
                disk_cache_count += 1
        except Exception as e:
            logger.error(f"Error calculating disk cache stats: {e}")

        return {
            "memory_cache_size": len(self.memory_cache.cache),
            "memory_cache_max_size": self.memory_cache.max_size,
            "disk_cache_size_bytes": disk_cache_size,
            "disk_cache_count": disk_cache_count,
            "disk_cache_ttl": self.disk_cache_ttl,
        }


# Create a singleton instance for application-wide use
cache_manager = CacheManager()


================================================
File: src/the_aichemist_codex/backend/utils/concurrency/concurrency.py
================================================
"""Provides enhanced concurrency tools for efficient parallel processing."""

import asyncio
import functools
import logging
from collections.abc import Callable, Coroutine
from concurrent.futures import ThreadPoolExecutor
from enum import Enum
from typing import Any, TypeVar, cast

logger = logging.getLogger(__name__)
T = TypeVar("T")


class TaskPriority(Enum):
    """Task priority levels for scheduling."""

    LOW = 0
    MEDIUM = 1
    HIGH = 2


class AsyncThreadPoolExecutor:
    """Thread pool executor with async interface and priority scheduling."""

    def __init__(self, max_workers: int | None = None):
        """Initialize the executor with a maximum number of workers.

        Args:
            max_workers: Maximum number of worker threads
        """
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        # Default to 10 workers if not specified
        worker_count = 10 if max_workers is None else max_workers
        self.semaphore = asyncio.Semaphore(worker_count)
        self._tasks: dict[asyncio.Future[Any], TaskPriority] = {}

    async def submit(
        self,
        func: Callable[..., T],
        *args: Any,
        priority: TaskPriority = TaskPriority.MEDIUM,
        **kwargs: Any,
    ) -> T:
        """
        Submit a function for execution with priority scheduling.

        Args:
            func: Function to execute
            *args: Arguments for the function
            priority: Task priority level
            **kwargs: Keyword arguments for the function

        Returns:
            Result of the function execution
        """
        async with self.semaphore:
            loop = asyncio.get_event_loop()

            # Create a future for the executor task
            future = loop.run_in_executor(
                self.executor, functools.partial(func, *args, **kwargs)
            )

            # Store future with priority
            self._tasks[future] = priority

            try:
                result = await future
                return cast(T, result)
            finally:
                if future in self._tasks:
                    del self._tasks[future]

    async def submit_batch(
        self,
        func: Callable[[Any], T],
        items: list[Any],
        priority: TaskPriority = TaskPriority.MEDIUM,
        max_concurrent: int | None = None,
    ) -> list[T]:
        """
        Submit a batch of items for concurrent processing.

        Args:
            func: Function to execute on each item
            items: List of items to process
            priority: Task priority level
            max_concurrent: Maximum number of concurrent tasks

        Returns:
            List of results in the order of input items
        """
        # Use a separate semaphore for batch processing if specified
        semaphore = (
            asyncio.Semaphore(max_concurrent or 10)
            if max_concurrent
            else self.semaphore
        )

        async def process_item(item):
            async with semaphore:
                return await self.submit(func, item, priority=priority)

        tasks = [asyncio.create_task(process_item(item)) for item in items]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out exceptions and return only successful results
        return [r for r in results if not isinstance(r, BaseException)]

    def shutdown(self, wait: bool = True):
        """
        Shutdown the executor.

        Args:
            wait: Whether to wait for pending futures to complete
        """
        self.executor.shutdown(wait=wait)


class RateLimiter:
    """Rate limiter for controlling operation frequency."""

    def __init__(self, max_rate: float, time_period: float = 1.0):
        """
        Initialize rate limiter.

        Args:
            max_rate: Maximum number of operations per time period
            time_period: Time period in seconds
        """
        self.max_rate = max_rate
        self.time_period = time_period
        self.tokens = max_rate
        self.last_update = asyncio.get_event_loop().time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """Acquire a token, waiting if necessary."""
        async with self.lock:
            while self.tokens <= 0:
                now = asyncio.get_event_loop().time()
                time_passed = now - self.last_update
                self.tokens = min(
                    self.max_rate,
                    self.tokens + time_passed * (self.max_rate / self.time_period),
                )
                self.last_update = now
                if self.tokens <= 0:
                    await asyncio.sleep(self.time_period / self.max_rate)
            self.tokens -= 1


class TaskQueue:
    """Priority task queue with rate limiting."""

    def __init__(
        self,
        max_concurrent: int = 10,
        max_rate: float | None = None,
        time_period: float = 1.0,
    ):
        """
        Initialize task queue.

        Args:
            max_concurrent: Maximum number of concurrent tasks
            max_rate: Maximum tasks per time period (None for no limit)
            time_period: Time period in seconds for rate limiting
        """
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.rate_limiter = (
            RateLimiter(max_rate, time_period) if max_rate is not None else None
        )
        self.tasks: dict[TaskPriority, list[asyncio.Task]] = {
            priority: [] for priority in TaskPriority
        }

    async def add_task(
        self,
        coro: Callable[..., T],
        *args: Any,
        priority: TaskPriority = TaskPriority.MEDIUM,
        **kwargs: Any,
    ) -> T:
        """
        Add a task to the queue.

        Args:
            coro: Coroutine to execute
            *args: Arguments for the coroutine
            priority: Task priority level
            **kwargs: Keyword arguments for the coroutine

        Returns:
            Result of the coroutine execution
        """
        async with self.semaphore:
            if self.rate_limiter is not None:
                await self.rate_limiter.acquire()

            task: asyncio.Task[T] = asyncio.create_task(
                cast(Coroutine[Any, Any, T], coro(*args, **kwargs))
            )
            self.tasks[priority].append(task)

            try:
                result = await task
                return cast(T, result)
            finally:
                self.tasks[priority].remove(task)

    async def add_batch(
        self,
        coro: Callable[[Any], T],
        items: list[Any],
        priority: TaskPriority = TaskPriority.MEDIUM,
        **kwargs: Any,
    ) -> list[T | BaseException]:
        """
        Add a batch of items for processing.

        Args:
            coro: Coroutine to execute for each item
            items: List of items to process
            priority: Task priority level
            **kwargs: Additional keyword arguments for the coroutine

        Returns:
            List of results in the order of input items
        """
        tasks = [
            self.add_task(coro, item, priority=priority, **kwargs) for item in items
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return cast(list[T | BaseException], results)


# Create singleton instances for application-wide use
thread_pool = AsyncThreadPoolExecutor()
task_queue = TaskQueue()


================================================
File: src/the_aichemist_codex/backend/utils/environment/environment.py
================================================
"""Environment detection utilities for dual-mode operation."""

import os
from pathlib import Path

from the_aichemist_codex.backend.config.settings import determine_project_root


def is_development_mode() -> bool:
    """
    Detect if running in development mode (not installed as package).

    This checks if we're running from source directories or as an installed package.

    Returns:
        bool: True if running from source, False if installed as package
    """
    # Check explicit environment variable first
    if os.environ.get("AICHEMIST_DEV_MODE"):
        return True

    # Check if running from source directory structure
    module_path = Path(__file__).resolve()
    src_parent = "src"

    # If we're in a src/the_aichemist_codex structure, we're in development mode
    return src_parent in module_path.parts


def get_import_mode() -> str:
    """
    Determine how the package was imported.

    Returns:
        str: "package" if installed and imported as a package,
             "standalone" if running from source,
             "editable" if installed in development/editable mode
    """
    if is_development_mode():
        return "standalone"

    # Check for editable install
    try:
        import importlib.metadata
        import sys
        from importlib.util import find_spec

        # More reliable way to detect editable installs
        # If the package is installed in editable mode, the spec will point to the source directory
        spec = find_spec("the_aichemist_codex")
        if spec and spec.origin:
            origin_path = Path(spec.origin).resolve()
            if "site-packages" not in str(origin_path) and "src" in origin_path.parts:
                return "editable"
    except (ImportError, ModuleNotFoundError):
        pass

    return "package"


def get_project_root() -> Path:
    """
    Get the project root directory regardless of execution context.

    This function builds on top of determine_project_root() from settings
    but adds additional logic specific to package vs. standalone mode.

    Returns:
        Path: The project root directory
    """
    # Use the existing project root detection
    return determine_project_root()


def get_package_dir() -> Path:
    """
    Get the package installation directory when running as an installed package.

    Returns:
        Path: The package installation directory
    """
    # If in development mode, return the src/the_aichemist_codex directory
    if is_development_mode():
        return Path(__file__).resolve().parents[2]

    # If installed, return the site-packages directory for the package
    import the_aichemist_codex

    return Path(the_aichemist_codex.__file__).resolve().parent


================================================
File: src/the_aichemist_codex/backend/utils/errors/errors.py
================================================
"""Custom exceptions for The Aichemist Codex."""


class CodexError(Exception):
    """Base exception for project-related errors."""
    pass


class MaxTokenError(CodexError):
    """Raised when a file exceeds the token limit."""
    def __init__(self, file_path, max_tokens):
        super().__init__(f"{file_path} exceeds {max_tokens} token limit.")


class NotebookProcessingError(CodexError):
    """Raised when an error occurs while processing a notebook."""
    def __init__(self, file_path):
        super().__init__(f"Failed to process notebook: {file_path}")


class InvalidVersion(CodexError):
    """Raised when an invalid version string is encountered."""
    pass


================================================
File: src/the_aichemist_codex/backend/utils/io/mime_type_detector.py
================================================
"""MIME type detection utilities."""

import logging
import mimetypes
from pathlib import Path

logger = logging.getLogger(__name__)

# Initialize mimetypes
mimetypes.init()


class MimeTypeDetector:
    """Detects MIME types of files."""

    @staticmethod
    def get_mime_type(file_path: Path) -> tuple[str, float]:
        """
        Determine the MIME type of a file.

        Args:
            file_path: Path to the file

        Returns:
            Tuple of (mime_type, confidence)
        """
        try:
            # Try using the mimetypes library first
            mime_type, _ = mimetypes.guess_type(str(file_path))

            if mime_type:
                return mime_type, 1.0

            # Fall back to extension-based detection
            suffix = file_path.suffix.lower()

            if suffix == ".md":
                return "text/markdown", 1.0
            elif suffix == ".py":
                return "text/x-python", 1.0
            elif suffix == ".txt":
                return "text/plain", 1.0
            elif suffix == ".json":
                return "application/json", 1.0
            elif suffix == ".html" or suffix == ".htm":
                return "text/html", 1.0
            elif suffix == ".css":
                return "text/css", 1.0
            elif suffix == ".js":
                return "application/javascript", 1.0
            elif suffix == ".xml":
                return "application/xml", 1.0
            elif suffix == ".csv":
                return "text/csv", 1.0
            elif suffix == ".yaml" or suffix == ".yml":
                return "application/x-yaml", 1.0
            elif suffix == ".toml":
                return "application/toml", 1.0

            # Unknown type
            return "application/octet-stream", 0.5

        except Exception as e:
            logger.error(f"Error detecting MIME type for {file_path}: {e}")
            return "application/octet-stream", 0.1


================================================
File: src/the_aichemist_codex/backend/utils/common/patterns.py
================================================
"""Manages file ignore patterns for The Aichemist Codex."""

import fnmatch
import os

from the_aichemist_codex.backend.config.config_loader import config


class PatternMatcher:
    """Checks if files should be ignored based on configured patterns."""

    def __init__(self):
        self.ignore_patterns = set(config.get("ignore_patterns"))

    def add_patterns(self, patterns: set):
        """Allows dynamically adding more ignore patterns."""
        self.ignore_patterns.update(patterns)

    def should_ignore(self, path: str) -> bool:
        """Determines if a given path should be ignored."""
        norm_path = os.path.normpath(path)
        base_name = os.path.basename(norm_path)

        return any(
            fnmatch.fnmatch(base_name, pattern) or fnmatch.fnmatch(norm_path, pattern)
            for pattern in self.ignore_patterns
        )


pattern_matcher = PatternMatcher()


================================================
File: src/the_aichemist_codex/backend/utils/common/safety.py
================================================
"""Ensures file operations remain within safe directories and ignore patterns."""

import logging
from pathlib import Path

from the_aichemist_codex.backend.config.config_loader import config

logger = logging.getLogger(__name__)


class SafeFileHandler:
    """Provides validation utilities to ensure safe file operations."""

    @staticmethod
    def is_safe_path(target: Path, base: Path) -> bool:
        """Ensures that a target path is within the base directory."""
        try:
            return base.resolve() in target.resolve().parents
        except (FileNotFoundError, RuntimeError):
            # If there's an error resolving paths, consider it unsafe
            return False

    @staticmethod
    def should_ignore(file_path: Path) -> bool:
        """
        Checks if a file should be ignored based on default and user-configured ignore patterns.

        Uses both the default ignore patterns and any additional patterns
        specified in the config file.
        """
        # Get combined ignore patterns (default + user configured)
        ignore_patterns = config.get("ignore_patterns", [])

        # Check against all patterns
        for pattern in ignore_patterns:
            if file_path.match(pattern):
                logger.info(f"Skipping ignored file: {file_path} (matched {pattern})")
                return True
            if any(
                part == pattern for part in file_path.parts
            ):  # Check parent directories
                logger.info(
                    f"Skipping ignored directory: {file_path} (matched {pattern})"
                )
                return True

        # Check against user-defined directory exclusions
        ignored_directories = config.get("ignored_directories", [])
        for ignored_dir in ignored_directories:
            ignored_path = Path(ignored_dir).resolve()
            try:
                if (
                    ignored_path in file_path.resolve().parents
                    or ignored_path == file_path.resolve()
                ):
                    logger.info(
                        f"Skipping file in ignored directory: {file_path} (in {ignored_dir})"
                    )
                    return True
            except (FileNotFoundError, RuntimeError):
                continue

        return False

    @staticmethod
    def is_binary_file(file_path: Path) -> bool:
        """Determines if a file is binary by checking its extension."""
        binary_extensions = {
            ".png",
            ".jpg",
            ".jpeg",
            ".exe",
            ".dll",
            ".zip",
            ".tar",
            ".gz",
        }
        return file_path.suffix in binary_extensions


================================================
File: src/the_aichemist_codex/backend/utils/io/sqlasync_io.py
================================================
"""Asynchronous SQLite operations for The Aichemist Codex."""

import logging
from pathlib import Path
from typing import Any

import aiosqlite

logger = logging.getLogger(__name__)


class AsyncSQL:
    """Provides asynchronous SQL operations."""

    def __init__(self, db_path: Path):
        """Initialize with database path."""
        self.db_path = db_path

    async def execute(
        self, query: str, params: tuple[Any, ...] = (), commit: bool = False
    ) -> None:
        """Execute a SQL query asynchronously."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute(query, params)
                if commit:
                    await db.commit()
        except Exception as e:
            logger.error(f"Error executing query: {query} with params {params}: {e}")

    async def fetchall(
        self, query: str, params: tuple[Any, ...] = ()
    ) -> list[tuple[Any, ...]]:
        """Fetch all rows from a query asynchronously."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute(query, params) as cursor:
                    rows = await cursor.fetchall()
                    # Convert aiosqlite.Row objects to tuples
                    return [tuple(row) for row in rows]
        except Exception as e:
            logger.error(
                f"Error fetching rows for query: {query} with params {params}: {e}"
            )
            return []

    async def fetchone(
        self, query: str, params: tuple[Any, ...] = ()
    ) -> tuple[Any, ...] | None:
        """Fetch one row from a query asynchronously."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute(query, params) as cursor:
                    row = await cursor.fetchone()
                    # Convert aiosqlite.Row object to tuple if not None
                    return tuple(row) if row is not None else None
        except Exception as e:
            logger.error(
                f"Error fetching row for query: {query} with params {params}: {e}"
            )
            return None

    async def executemany(self, query: str, params_list: list[tuple]) -> None:
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.executemany(query, params_list)
                await db.commit()
        except Exception as e:
            logger.error(
                f"Error executing many for query: {query} with params {params_list}: {e}"
            )



================================================
File: src/the_aichemist_codex/backend/utils/environment/validate_data_dir.py
================================================
"""
Utility for validating and repairing the data directory structure.

This module provides functions to check if the data directory structure
is correct and fix any missing or incorrect components.
"""

import logging
import os

from the_aichemist_codex.backend.config.settings import directory_manager

logger = logging.getLogger(__name__)


def validate_data_directory() -> dict[str, bool]:
    """
    Validate the data directory structure, checking all required subdirectories.

    Returns:
        Dict[str, bool]: Status of each required directory
    """
    results = {}

    # Check main data directory
    data_dir = directory_manager.base_dir
    results["base"] = data_dir.exists() and data_dir.is_dir()

    # Check all standard subdirectories
    for subdir in directory_manager.STANDARD_DIRS:
        dir_path = directory_manager.get_dir(subdir)
        results[subdir] = dir_path.exists() and dir_path.is_dir()

    # Check for required files
    rollback_path = directory_manager.get_file_path("rollback.json")
    results["rollback.json"] = rollback_path.exists() and rollback_path.is_file()

    version_metadata_path = directory_manager.get_file_path("version_metadata.json")
    results["version_metadata.json"] = (
        version_metadata_path.exists() and version_metadata_path.is_file()
    )

    return results


def repair_data_directory() -> list[str]:
    """
    Repair the data directory structure, creating any missing components.

    Returns:
        List[str]: List of fixes applied
    """
    fixes = []
    validation = validate_data_directory()

    # Ensure base directory exists
    if not validation["base"]:
        os.makedirs(directory_manager.base_dir, exist_ok=True)
        fixes.append(f"Created base data directory at {directory_manager.base_dir}")

    # Ensure all standard subdirectories exist
    for subdir in directory_manager.STANDARD_DIRS:
        if not validation.get(subdir, False):
            dir_path = directory_manager.get_dir(subdir)
            os.makedirs(dir_path, exist_ok=True)
            fixes.append(f"Created {subdir} directory at {dir_path}")

    # Create required files if missing
    if not validation.get("rollback.json", False):
        rollback_path = directory_manager.get_file_path("rollback.json")
        with open(rollback_path, "w") as f:
            f.write("{}\n")  # Empty JSON object
        fixes.append(f"Created empty rollback.json at {rollback_path}")

    if not validation.get("version_metadata.json", False):
        version_metadata_path = directory_manager.get_file_path("version_metadata.json")
        with open(version_metadata_path, "w") as f:
            f.write("{}\n")  # Empty JSON object
        fixes.append(f"Created empty version_metadata.json at {version_metadata_path}")

    return fixes


def get_directory_status() -> tuple[bool, dict[str, bool], list[str]]:
    """
    Get the complete status of the data directory.

    Returns:
        Tuple[bool, Dict[str, bool], List[str]]:
            - Overall status (True if valid)
            - Status of each component
            - List of issues found
    """
    validation = validate_data_directory()
    issues = []

    # Compile list of issues
    for component, status in validation.items():
        if not status:
            if component == "base":
                issues.append(
                    f"Base data directory missing: {directory_manager.base_dir}"
                )
            elif component in directory_manager.STANDARD_DIRS:
                issues.append(f"Directory '{component}' missing")
            else:
                issues.append(f"File '{component}' missing")

    # Overall status is True only if all components are valid
    overall_status = all(validation.values())

    return overall_status, validation, issues


================================================
File: src/the_aichemist_codex/backend/utils/environment/validator.py
================================================
"""Provides validation and general utilities."""

from pathlib import Path


def get_project_name(directory: Path) -> str:
    """Returns the project name based on the directory name."""
    return directory.name
