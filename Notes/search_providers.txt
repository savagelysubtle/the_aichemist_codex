
================================================
File: src/the_aichemist_codex/backend/search/providers/__init__.py
================================================
"""Search provider implementations for different search types."""

from typing import List, Protocol, runtime_checkable


@runtime_checkable
class SearchProvider(Protocol):
    """Protocol defining the interface for search providers."""

    async def search(self, query: str, **kwargs) -> list[str]:
        """
        Perform a search using this provider.

        Args:
            query: The search query
            **kwargs: Additional provider-specific parameters

        Returns:
            List of matching file paths
        """
        ...



================================================
File: src/the_aichemist_codex/backend/search/providers/regex_provider.py
================================================
"""Regex-based search provider for pattern matching in file contents."""

import asyncio
import logging
import re
import time
from pathlib import Path
from typing import cast

from the_aichemist_codex.backend.config.settings import (
    REGEX_MAX_COMPLEXITY,
    REGEX_TIMEOUT_MS,
)
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
from the_aichemist_codex.backend.utils.batch_processor import BatchProcessor
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

logger = logging.getLogger(__name__)


class RegexSearchProvider:
    """
    Provides regex-based search capabilities for file contents.

    Features:
    - Regex pattern validation to prevent catastrophic backtracking
    - Streaming file reading for memory efficiency
    - Caching of search results
    - Parallel processing of files
    """

    def __init__(self, cache_manager: CacheManager | None = None):
        """
        Initialize the regex search provider.

        Args:
            cache_manager: Optional cache manager for caching search results
        """
        self.cache_manager = cache_manager
        self.batch_processor = BatchProcessor()
        self._compiled_patterns: dict[str, re.Pattern] = {}

    async def search(
        self,
        query: str,
        file_paths: list[Path] | None = None,
        max_results: int = 100,
        case_sensitive: bool = False,
        whole_word: bool = False,
        **kwargs,
    ) -> list[str]:
        """
        Search for files matching the regex pattern.

        Args:
            query: The regex pattern to search for
            file_paths: List of file paths to search (empty list means no files)
            max_results: Maximum number of results to return
            case_sensitive: Whether to perform case-sensitive search
            whole_word: Whether to match whole words only
            **kwargs: Additional arguments for future extensions

        Returns:
            List of file paths containing matches
        """
        if not query:
            logger.warning("Empty query provided for regex search")
            return []

        # Ensure file_paths is a list, not None
        if file_paths is None:
            file_paths = []

        # Create a unique key for caching
        cache_key = f"regex_{query}_{case_sensitive}_{whole_word}"

        # Check cache first if cache manager is available
        if self.cache_manager and len(file_paths) > 0:
            cached_results = await self.cache_manager.get(cache_key)
            if cached_results:
                logger.info(f"Retrieved regex search results from cache for '{query}'")
                return cast(list[str], cached_results)

        # Validate and compile the regex pattern
        try:
            pattern = self._prepare_pattern(query, case_sensitive, whole_word)
            logger.info(f"Compiled regex pattern: {pattern.pattern}")
        except (re.error, ValueError) as e:
            logger.error(f"Invalid regex pattern '{query}': {e}")
            return []

        # Process files in batches
        start_time = time.time()
        logger.info(f"Starting regex search on {len(file_paths)} files")
        results = await self._search_files(pattern, file_paths, max_results)
        elapsed = time.time() - start_time
        logger.info(
            f"Regex search for '{query}' completed in {elapsed:.2f}s with {len(results)} results"
        )

        # Cache the results
        if self.cache_manager:
            # Create a safe cache key without characters that are invalid in file paths
            safe_query = query.replace("\\", "_").replace("/", "_").replace(":", "_")
            cache_key = f"regex_{safe_query}_{case_sensitive}_{whole_word}"
            await self.cache_manager.put(cache_key, results)

        return results[:max_results]

    def _prepare_pattern(
        self, query: str, case_sensitive: bool, whole_word: bool
    ) -> re.Pattern:
        """
        Prepare and validate the regex pattern.

        Args:
            query: The regex pattern string
            case_sensitive: Whether the pattern should be case sensitive
            whole_word: Whether to match whole words only

        Returns:
            Compiled regex pattern

        Raises:
            ValueError: If the pattern is too complex
            re.error: If the pattern is invalid
        """
        # Check if we've already compiled this pattern
        pattern_key = f"{query}:{case_sensitive}:{whole_word}"
        if pattern_key in self._compiled_patterns:
            return self._compiled_patterns[pattern_key]

        # Modify pattern for whole word matching if needed
        if whole_word and not (query.startswith(r"\b") and query.endswith(r"\b")):
            query = rf"\b{query}\b"

        # Validate pattern complexity to prevent catastrophic backtracking
        if self._estimate_complexity(query) > REGEX_MAX_COMPLEXITY:
            raise ValueError(f"Regex pattern too complex: '{query}'")

        # Compile the pattern
        flags = 0 if case_sensitive else re.IGNORECASE
        pattern = re.compile(query, flags)

        # Cache the compiled pattern
        self._compiled_patterns[pattern_key] = pattern
        return pattern

    def _estimate_complexity(self, pattern: str) -> int:
        """
        Estimate the complexity of a regex pattern.

        Args:
            pattern: The regex pattern string

        Returns:
            Complexity score (higher is more complex)
        """
        # Simple heuristic for pattern complexity
        complexity = len(pattern) * 2

        # Penalize potentially expensive operations
        complexity += pattern.count("*") * 10
        complexity += pattern.count("+") * 8
        complexity += pattern.count("{") * 10
        complexity += pattern.count("?") * 5
        complexity += pattern.count("|") * 15
        complexity += pattern.count("[") * 5
        complexity += pattern.count("(") * 8

        # Nested groups are particularly expensive
        depth = 0
        max_depth = 0
        for char in pattern:
            if char == "(":
                depth += 1
                max_depth = max(max_depth, depth)
            elif char == ")":
                depth = max(0, depth - 1)

        complexity += max_depth * 20

        return complexity

    async def _search_files(
        self, pattern: re.Pattern, file_paths: list[Path], max_results: int
    ) -> list[str]:
        """
        Search for the pattern in the given files.

        Args:
            pattern: Compiled regex pattern
            file_paths: List of file paths to search
            max_results: Maximum number of results to return

        Returns:
            List of file paths containing matches
        """
        # Use a set to avoid duplicates
        matching_files: set[str] = set()
        logger.info(f"Searching {len(file_paths)} files for pattern: {pattern.pattern}")

        # Check if the pattern is looking for file extensions
        is_extension_search = pattern.pattern.endswith("$") and "." in pattern.pattern

        # Process files in batches
        async def process_file(file_path: Path) -> str | None:
            if len(matching_files) >= max_results:
                return None

            try:
                # For extension searches, just check the filename
                if is_extension_search and pattern.search(file_path.name):
                    logger.debug(f"Found match in filename: {file_path}")
                    return str(file_path)

                # Skip binary files for content searches
                if not self._is_text_file(file_path):
                    return None

                # Log when processing a Markdown file
                if file_path.suffix.lower() == ".md":
                    logger.debug(f"Processing Markdown file: {file_path}")

                # Read file in chunks to handle large files
                async for chunk in AsyncFileIO.read_chunked(file_path):
                    chunk_str = chunk.decode("utf-8", errors="ignore")

                    # Use a timeout to prevent regex from hanging
                    match = await self._regex_search_with_timeout(pattern, chunk_str)
                    if match:
                        logger.debug(f"Found match in file content: {file_path}")
                        return str(file_path)

            except Exception as e:
                logger.error(f"Error searching file {file_path}: {e}")

            return None

        # Process files in parallel
        batch_results = await self.batch_processor.process_batch(
            items=file_paths,
            operation=process_file,
            batch_size=min(10, len(file_paths)),
            timeout=30,
        )

        # Collect results
        for result in batch_results:
            if result and len(matching_files) < max_results:
                matching_files.add(result)

        logger.info(f"Found {len(matching_files)} matching files")
        return list(matching_files)

    async def _regex_search_with_timeout(self, pattern: re.Pattern, text: str) -> bool:
        """
        Perform regex search with a timeout to prevent hanging.

        Args:
            pattern: Compiled regex pattern
            text: Text to search in

        Returns:
            True if a match was found, False otherwise
        """
        # Run the regex search in a separate thread with a timeout
        try:
            # Create a future for the regex search
            loop = asyncio.get_event_loop()
            future = loop.run_in_executor(None, pattern.search, text)

            # Wait for the result with a timeout
            result = await asyncio.wait_for(future, timeout=REGEX_TIMEOUT_MS / 1000)
            return bool(result)
        except TimeoutError:
            logger.warning(f"Regex search timed out after {REGEX_TIMEOUT_MS}ms")
            return False

    def _is_text_file(self, file_path: Path) -> bool:
        """
        Check if a file is likely to be a text file based on extension.

        Args:
            file_path: Path to the file

        Returns:
            True if the file is likely to be a text file, False otherwise
        """
        # Common text file extensions
        text_extensions = {
            ".txt",
            ".md",
            ".py",
            ".js",
            ".html",
            ".css",
            ".json",
            ".xml",
            ".yaml",
            ".yml",
            ".ini",
            ".cfg",
            ".conf",
            ".sh",
            ".bat",
            ".ps1",
            ".c",
            ".cpp",
            ".h",
            ".hpp",
            ".java",
            ".cs",
            ".go",
            ".rs",
            ".php",
            ".rb",
            ".pl",
            ".pm",
            ".t",
            ".sql",
            ".log",
            ".csv",
            ".tsv",
        }

        return file_path.suffix.lower() in text_extensions



================================================
File: src/the_aichemist_codex/backend/search/providers/similarity_provider.py
================================================
"""Similarity-based search provider for finding related files.

This module provides classes and functions for finding similar files
using vector embeddings based on file content.
"""

import logging
import os
from pathlib import Path
from typing import cast

import numpy as np
from sklearn.cluster import AgglomerativeClustering  # type: ignore

from the_aichemist_codex.backend.config.settings import get_settings
from the_aichemist_codex.backend.models.embeddings import (
    TextEmbeddingModel,
    VectorIndex,
    compute_similarity_matrix,
)
from the_aichemist_codex.backend.utils.async_io import AsyncFileTools
from the_aichemist_codex.backend.utils.batch_processor import BatchProcessor
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

logger = logging.getLogger(__name__)


class SimilarityProvider:
    """Provider for similarity-based search capabilities.

    This class enables finding files related to a query or another file
    by comparing their vector embeddings.

    Features:
    - Vector-based similarity search
    - File-to-file similarity comparison
    - Text-to-file search
    - File grouping based on similarity
    - Caching of results
    """

    def __init__(
        self,
        embedding_model: TextEmbeddingModel | None = None,
        vector_index: VectorIndex | None = None,
        path_mapping: list[str] | None = None,
        cache_manager: CacheManager | None = None,
    ):
        """Initialize the similarity provider.

        Args:
            embedding_model: Model for creating text embeddings
            vector_index: Index for storing and searching vectors
            path_mapping: List of file paths corresponding to vectors in the index
            cache_manager: Manager for caching search results
        """
        self.embedding_model = embedding_model
        self.vector_index = vector_index
        self.path_mapping = path_mapping or []
        self.cache_manager = cache_manager
        self.cache_ttl = get_settings().get(
            "SIMILARITY_CACHE_TTL", 300
        )  # Default 5 minutes
        self.batch_processor = BatchProcessor()

    async def search(
        self,
        query: str,
        file_paths: list[str | Path] | None = None,
        threshold: float = 0.7,
        max_results: int = 10,
    ) -> list[str]:
        """Search for files similar to the query text.

        Args:
            query: Text query to search for
            file_paths: Optional list of file paths to search in
            threshold: Minimum similarity score (0.0-1.0)
            max_results: Maximum number of results to return

        Returns:
            List of file paths matching the query
        """
        if not query or not self.embedding_model:
            return []

        # Check cache first
        cache_key = f"similarity_search_{query}_{threshold}_{max_results}"
        if self.cache_manager:
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result:
                return cast(list[str], cached_result)

        try:
            # Calculate query embedding
            if self.embedding_model is None:
                return []

            query_vector = self.embedding_model.encode(query)

            # If we have a vector index, use it
            if self.vector_index is None or self.vector_index.vectors is None:
                return []

            distances, indices = self.vector_index.search(query_vector, max_results)
            # Convert indices to numpy array if they're not already
            np_indices = np.array(indices, dtype=np.int64)
            results = self.vector_index.get_paths(np_indices)

            # Filter by threshold
            filtered_results = []
            for i, path in enumerate(results):
                if i < len(distances) and distances[i] >= threshold:
                    filtered_results.append(path)

            # Cache results
            if self.cache_manager:
                await self.cache_manager.put(cache_key, filtered_results)

            return filtered_results

        except Exception as e:
            logger.error(f"Error in similarity search: {e}")
            return []

    async def find_similar_files(
        self, file_path: str | Path, threshold: float = 0.7, max_results: int = 10
    ) -> list[tuple[str, float]]:
        """Find files similar to the given file.

        Args:
            file_path: Path to the reference file
            threshold: Minimum similarity score (0.0-1.0)
            max_results: Maximum number of results to return

        Returns:
            List of (file_path, similarity_score) tuples
        """
        # Ensure file_path is a Path object
        path_obj = Path(file_path) if isinstance(file_path, str) else file_path

        if not path_obj.exists() or not self.embedding_model:
            return []

        # Check cache
        cache_key = f"similar_files_{str(path_obj)}_{threshold}_{max_results}"
        if self.cache_manager:
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result:
                return cast(list[tuple[str, float]], cached_result)

        try:
            # Read the file content
            content = ""
            async for chunk in AsyncFileTools.read_chunked(path_obj):
                content += chunk.decode("utf-8", errors="ignore")

            # Generate embedding for the file
            if self.embedding_model is None:
                return []

            file_vector = self.embedding_model.encode(content)

            # Search the vector index
            if self.vector_index is None:
                return []

            distances, indices = self.vector_index.search(
                file_vector, k=max_results + 1
            )  # +1 for the file itself

            # Filter by threshold and get paths with scores
            results = []
            for i, idx in enumerate(indices):
                if self.vector_index is None:
                    continue

                if idx < len(self.vector_index.path_mapping):
                    path = self.vector_index.path_mapping[idx]
                else:
                    path = ""

                score = float(distances[i])

                # Skip the file itself and ensure score meets threshold
                if path and path != str(path_obj) and score >= threshold:
                    results.append((path, score))

            # Limit to max_results
            results = results[:max_results]

            # Cache results
            if self.cache_manager:
                await self.cache_manager.put(cache_key, results)

            logger.debug(f"Found {len(results)} similar files for: {str(path_obj)}")
            return results
        except Exception as e:
            logger.error(f"Error finding similar files for {str(path_obj)}: {e}")
            return []

    async def find_file_groups(
        self,
        file_paths: list[str | Path] | None = None,
        threshold: float = 0.7,
        min_group_size: int = 2,
    ) -> list[list[str]]:
        """Find groups of similar files using hierarchical clustering.

        Args:
            file_paths: Optional list of file paths to analyze
            threshold: Similarity threshold for group membership
            min_group_size: Minimum number of files to form a group

        Returns:
            List of lists, where each inner list contains paths of similar files
        """
        if not self.embedding_model:
            return []

        # Use provided file_paths or all paths in the index
        paths_to_process = (
            [str(p) for p in file_paths] if file_paths else self.path_mapping
        )

        if not paths_to_process:
            logger.warning("No files to process for finding file groups")
            return []

        # Check cache first
        cache_key = f"file_groups:{threshold}:{min_group_size}"
        if file_paths:
            cache_key += (
                f":{len(file_paths)}"  # Add count to distinguish different file sets
            )

        if self.cache_manager:
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result:
                logger.debug(f"Cache hit for file groups with threshold {threshold}")
                return cast(list[list[str]], cached_result)

        try:
            # Process files in manageable batches
            batch_size = 100  # Process 100 files at a time
            all_embeddings = []
            valid_paths = []

            # Process files in batches
            for i in range(0, len(paths_to_process), batch_size):
                batch = paths_to_process[i : i + batch_size]

                # Process files to get their embeddings
                results = await self.batch_processor.process_batch(
                    items=batch,
                    operation=self._process_file,
                    batch_size=10,
                    timeout=60,
                )

                # Filter out None results and collect embeddings and paths
                for result in results:
                    if result:
                        path, embedding = result
                        all_embeddings.append(embedding)
                        valid_paths.append(path)

            if not all_embeddings:
                logger.warning("No valid embeddings extracted from files")
                return []

            # Convert list of embeddings to a numpy array
            embeddings_array = np.array(all_embeddings)

            # Compute similarity matrix
            similarity_matrix = compute_similarity_matrix(embeddings_array)

            # Convert similarity to distance (1 - similarity)
            distance_matrix = 1 - similarity_matrix

            # Apply hierarchical clustering
            clustering = AgglomerativeClustering(
                n_clusters=None,
                distance_threshold=1
                - threshold,  # Convert similarity threshold to distance
                affinity="precomputed",
                linkage="average",
            )

            cluster_labels = clustering.fit_predict(distance_matrix)

            # Group files by cluster
            clusters: dict[int, list[str]] = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(valid_paths[i])

            # Filter clusters by minimum size
            file_groups = [
                group for group in clusters.values() if len(group) >= min_group_size
            ]

            # Cache results
            if self.cache_manager:
                await self.cache_manager.put(cache_key, file_groups)

            logger.info(f"Found {len(file_groups)} groups of similar files")
            return file_groups

        except Exception as e:
            logger.error(f"Error finding file groups: {e}")
            return []

    async def _process_file(
        self, file_path: str | Path
    ) -> tuple[str, np.ndarray] | None:
        """Process a file to extract its embedding.

        Args:
            file_path: Path to the file

        Returns:
            Tuple of (file_path, embedding) or None if processing failed
        """
        try:
            file_path_str = str(file_path)
            if not os.path.exists(file_path_str):
                return None

            # Skip large files and non-text files
            if os.path.getsize(file_path_str) > 1_000_000:  # 1MB limit
                logger.debug(f"Skipping large file: {file_path_str}")
                return None

            # Convert to Path for read_chunked
            path_obj = Path(file_path_str)
            content = ""
            async for chunk in AsyncFileTools.read_chunked(path_obj):
                content += chunk.decode("utf-8", errors="ignore")

            # Generate embedding for the file
            if self.embedding_model is None:
                return None

            file_vector = self.embedding_model.encode(content)
            return (file_path_str, file_vector)

        except Exception as e:
            logger.error(f"Error processing file {file_path}: {e}")
            return None

