



================================================
File: src/the_aichemist_codex/backend/file_reader/__init__.py
================================================
"""File reading and parsing module for The Aichemist Codex."""

from .file_metadata import FileMetadata
from .file_reader import FileReader
from .ocr_parser import OCRParser
from .parsers import (
    ArchiveParser,
    BaseParser,
    CodeParser,
    CsvParser,
    DocumentParser,
    JsonParser,
    SpreadsheetParser,
    TextParser,
    VectorParser,
    XmlParser,
    YamlParser,
    get_parser_for_mime_type,
)

__all__ = [
    "ArchiveParser",
    "BaseParser",
    "CodeParser",
    "CsvParser",
    "DocumentParser",
    "FileMetadata",
    "FileReader",
    "JsonParser",
    "OCRParser",
    "SpreadsheetParser",
    "TextParser",
    "VectorParser",
    "XmlParser",
    "YamlParser",
    "get_parser_for_mime_type",
]



================================================
File: src/the_aichemist_codex/backend/file_reader/file_metadata.py
================================================
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any


@dataclass
class FileMetadata:
    """Class for storing file metadata.

    This class holds basic file information as well as rich metadata
    extracted from file content through content analysis.
    """

    path: Path
    mime_type: str
    size: int
    extension: str
    preview: str
    error: str | None = None
    parsed_data: Any | None = None

    # Content-based metadata fields
    tags: list[str] = field(default_factory=list)
    keywords: list[str] = field(default_factory=list)
    topics: list[dict[str, float]] = field(default_factory=list)
    entities: dict[str, list[str]] = field(default_factory=dict)
    language: str | None = None
    content_type: str | None = None
    category: str | None = None
    summary: str | None = None

    # For code files
    programming_language: str | None = None
    imports: list[str] = field(default_factory=list)
    functions: list[str] = field(default_factory=list)
    classes: list[str] = field(default_factory=list)
    complexity_score: float | None = None

    # For document files
    title: str | None = None
    author: str | None = None
    creation_date: str | None = None
    modified_date: str | None = None

    # Extraction metadata
    extraction_complete: bool = False
    extraction_confidence: float = 0.0
    extraction_time: float = 0.0



================================================
File: src/the_aichemist_codex/backend/file_reader/file_reader.py
================================================
"""
File reading and parsing module for The Aichemist Codex.
This module provides functionality for reading and parsing various file types,
including text files, documents, spreadsheets, code files, and more.
"""

import asyncio
import logging
import warnings
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any

# Import python-magic safely
try:
    import magic
except ImportError:
    magic = None

from the_aichemist_codex.backend.config import settings
from the_aichemist_codex.backend.metadata.manager import MetadataManager
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

from .file_metadata import FileMetadata
from .parsers import get_parser_for_mime_type

logger = logging.getLogger(__name__)


class FileReader:
    """Main class for reading and parsing files with MIME type detection."""

    def __init__(
        self,
        max_workers: int = 2,
        preview_length: int = 100,
        cache_manager: CacheManager | None = None,
    ):
        """Initialize FileReader.

        Args:
            max_workers (int): Maximum number of worker threads for
                concurrent operations
            preview_length (int): Maximum length of file previews
            cache_manager (Optional[CacheManager]): Cache manager for
                caching extraction results
        """
        self.max_workers = max_workers
        self.preview_length = preview_length
        self.logger = logging.getLogger(__name__)

        # Initialize the magic library for file type detection
        self._magic_instance = None

        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.cache_manager = cache_manager

        # Initialize the metadata manager
        self.metadata_manager = MetadataManager(cache_manager)

    def get_mime_type(self, file_path: str | Path) -> str:
        """
        Get the MIME type of a file.

        Args:
            file_path: Path to the file

        Returns:
            str: MIME type of the file

        Raises:
            FileNotFoundError: If the file does not exist
        """
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"{path} does not exist.")

        if magic is None:
            logger.warning("Magic library not available, using default MIME type")
            return "application/octet-stream"

        # Simple approach to handle the different implementations
        try:
            # Try direct function call (common in python-magic-bin)
            # Suppress errors from linter about from_file - we're handling this at runtime
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                from_file_attr = getattr(magic, "from_file", None)
                if callable(from_file_attr):
                    return from_file_attr(str(path), mime=True)
            # If we can't use from_file directly, try something else
            return self._fallback_mime_type(path)
        except Exception as e:
            logger.warning(f"Error determining MIME type: {e}")
            return self._fallback_mime_type(path)

    def _fallback_mime_type(self, path: Path) -> str:
        """Fallback method to detect MIME type based on file extension."""
        extension = path.suffix.lower()
        # Common MIME types based on file extensions
        mime_map = {
            ".txt": "text/plain",
            ".html": "text/html",
            ".htm": "text/html",
            ".css": "text/css",
            ".js": "application/javascript",
            ".json": "application/json",
            ".xml": "application/xml",
            ".csv": "text/csv",
            ".md": "text/markdown",
            ".pdf": "application/pdf",
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
            ".svg": "image/svg+xml",
            ".mp3": "audio/mpeg",
            ".mp4": "video/mp4",
            ".zip": "application/zip",
            ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
        }
        return mime_map.get(extension, "application/octet-stream")

    def get_mime_types(self, file_paths: list[str | Path]) -> dict[str, str]:
        """
        Get MIME types for multiple files.

        Args:
            file_paths: List of file paths to process

        Returns:
            Dict[str, str]: Dictionary mapping file paths to their MIME types
        """
        return {str(path): self.get_mime_type(path) for path in file_paths}

    async def read_files(self, file_paths: list[Path]) -> list[FileMetadata]:
        """Read multiple files and return their metadata.

        Args:
            file_paths (List[Path]): List of paths to read

        Returns:
            List[FileMetadata]: List of metadata for each file
        """
        results = []
        for path in file_paths:
            try:
                # Use AsyncFileIO to check for file existence.
                if not await AsyncFileIO.exists(path):
                    raise FileNotFoundError(f"Cannot find the file: {path}")

                stats = path.stat()
                size = stats.st_size
                mime_type = self.get_mime_type(path)

                # Generate preview for text files
                preview = ""
                if mime_type.startswith("text/"):
                    content = await AsyncFileIO.read_text(path)
                    if content.startswith("# "):  # Error message or skipped file
                        error_msg = content[2:]  # Remove "# " prefix
                        raise RuntimeError(error_msg)

                    preview = (
                        content[: self.preview_length] + "..."
                        if len(content) > self.preview_length
                        else content
                    )

                metadata = FileMetadata(
                    path=path,
                    mime_type=mime_type,
                    size=size,
                    extension=path.suffix,
                    preview=preview,
                    error=None,
                    parsed_data=None,
                )
                results.append(metadata)

            except Exception as e:
                # Handle errors gracefully
                error_msg = f"Error processing file {path}: {str(e)}"
                self.logger.error(error_msg)
                metadata = FileMetadata(
                    path=path,
                    mime_type="unknown",
                    size=-1,
                    extension=path.suffix if path else "",
                    preview="",
                    error=error_msg,
                    parsed_data=None,
                )
                results.append(metadata)

        return results

    async def process_file(self, file_path: Path) -> FileMetadata:
        """
        Process a single file to extract its metadata and preview.

        Args:
            file_path: Path to the file to process

        Returns:
            FileMetadata: Metadata and preview information for the file
        """
        try:
            # Use AsyncFileIO to check file existence.
            if not await AsyncFileIO.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")

            # Run MIME detection in thread pool to avoid blocking
            mime_type = await asyncio.get_event_loop().run_in_executor(
                self.executor, self.get_mime_type, file_path
            )

            metadata = FileMetadata(
                path=file_path,
                mime_type=mime_type,
                size=file_path.stat().st_size,
                extension=file_path.suffix.lower(),
                preview="",  # Added preview parameter
                error=None,  # Added error parameter
                parsed_data=None,
            )

            # Get preview and basic parsed data if possible
            try:
                preview, parsed_data = await self._get_preview_and_data(
                    file_path, mime_type
                )
                metadata.preview = preview if preview else ""
                metadata.parsed_data = parsed_data
            except Exception as e:
                metadata.error = f"Preview generation failed: {str(e)}"
                metadata.preview = ""
                logger.warning(f"Failed to generate preview for {file_path}: {e}")

            # Extract enhanced metadata if the feature is enabled
            if (
                hasattr(settings, "ENABLE_ENHANCED_METADATA")
                and settings.ENABLE_ENHANCED_METADATA
            ):
                try:
                    # Get the file content if we don't have it already
                    content = None
                    if mime_type.startswith("text/"):
                        content = await AsyncFileIO.read_text(file_path)

                    # Extract enhanced metadata
                    enhanced_metadata = await self.metadata_manager.extract_metadata(
                        file_path=file_path,
                        content=content,
                        mime_type=mime_type,
                        metadata=metadata,
                    )

                    # Use the enhanced metadata
                    metadata = enhanced_metadata

                    logger.debug(f"Enhanced metadata extracted for {file_path}")
                except Exception as e:
                    logger.warning(
                        f"Enhanced metadata extraction failed for {file_path}: {e}"
                    )
                    # We still have the basic metadata, so continue

            return metadata

        except Exception as e:
            error_msg = f"Error processing file {file_path}: {str(e)}"
            logger.error(error_msg)
            return FileMetadata(
                path=file_path,
                mime_type="unknown",
                size=-1,
                extension=file_path.suffix.lower(),
                error=error_msg,
                preview="",
            )

    async def _get_preview_and_data(
        self, file_path: Path, mime_type: str
    ) -> tuple[str, dict[str, Any] | None]:
        """
        Get a preview of the file contents and parsed data based on its MIME type.

        Args:
            file_path: Path to the file
            mime_type: MIME type of the file

        Returns:
            Tuple of (preview_string, parsed_data_dict)
        """
        parser = get_parser_for_mime_type(mime_type)

        if parser:
            try:
                parsed_data = await parser.parse(file_path)
                preview = parser.get_preview(parsed_data, self.preview_length)
                return preview, parsed_data
            except Exception as e:
                self.logger.warning(f"Parser failed for {file_path}: {e}")
                # Fall back to basic text preview if parser fails
                return await self._read_text_preview(file_path), None

        # For unsupported file types, return a basic message
        return f"[Binary file of type: {mime_type}]", None

    async def _read_text_preview(self, file_path: Path) -> str:
        """Read a text preview of a file.

        Args:
            file_path: Path to the file

        Returns:
            A string preview of the file content
        """
        try:
            content = await AsyncFileIO.read_text(file_path)
            if content.startswith("# "):  # Error message or skipped file
                return f"[Preview error: {content[2:]}]"

            return (
                content[: self.preview_length] + "..."
                if len(content) > self.preview_length
                else content
            )
        except Exception as e:
            self.logger.error(f"Error reading text preview from {file_path}: {e}")
            return f"[Error reading preview: {e}]"



================================================
File: src/the_aichemist_codex/backend/file_reader/ocr_parser.py
================================================
import logging
from pathlib import Path
from typing import Any

from kreuzberg import extract_file

logger = logging.getLogger(__name__)


class OCRParser:
    """Parser for performing OCR on image files using kreuzberg.
    This parser extracts text from image files.
    """

    async def parse(self, file_path: Path) -> dict[str, Any]:
        """Perform OCR on the image file.

        Args:
            file_path (Path): The path to the image file.

        Returns:
            Dict[str, Any]: A dictionary containing the extracted text under 'content'.

        Raises:
            Exception: If the image cannot be read or OCR fails.
        """
        try:
            # Pass the file path directly to extract_file
            result = await extract_file(str(file_path))
            return {"content": result.content}
        except Exception as e:
            logger.error(f"OCR parsing failed for {file_path}: {e}", exc_info=True)
            raise

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        """Generate a preview of the OCR extracted text."""
        text = parsed_data.get("content", "")
        return text[:max_length] + "..." if len(text) > max_length else text



================================================
File: src/the_aichemist_codex/backend/file_reader/parsers.py
================================================
"""
File parsers module for handling different file types.
This module provides specialized parsers for various file formats including:
- Text files (TXT, CSV, YAML, JSON, XML)
- Document files (PDF, DOCX)
- Spreadsheets (XLSX, ODS)
- Code/config files (PY, JS, TOML, etc.)
- Vector/CAD files (DWG, DXF, SVG)
- Archives (ZIP, TAR, RAR, 7Z)
"""

import ast
import asyncio
import csv
import json
import logging
import tarfile
import xml.etree.ElementTree as ET
import zipfile
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any

import pandas as pd
import py7zr
import rarfile  # RAR support
import tomli
import yaml
from docx import Document
from pypdf import PdfReader

from the_aichemist_codex.backend.utils.async_io import AsyncFileIO

logger = logging.getLogger(__name__)


class BaseParser(ABC):
    """Abstract base class for file parsers."""

    @abstractmethod
    async def parse(self, file_path: Path) -> dict[str, Any]:
        """
        Parse a file asynchronously and return its structured data.

        :param file_path: Path to the file to parse
        :return: A dictionary containing parsed data (keys may differ by parser).
        """
        pass

    @abstractmethod
    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        """
        Generate a textual preview for the parsed data.

        :param parsed_data: The dictionary returned by `parse()`
        :param max_length: Maximum length of the preview string
        :return: A string preview of the data
        """
        pass


class TextParser(BaseParser):
    """Parser for basic text files (TXT, MD, etc.)."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        try:
            content = await AsyncFileIO.read_text(file_path)  # Async read
            if content.startswith("# "):
                # Means either file was ignored or an error occurred
                return {
                    "error": content,
                    "content": "",
                    "encoding": "",
                    "line_count": 0,
                }

            encoding_used = "utf-8"
            line_count = content.count("\n") + 1
            return {
                "content": content,
                "encoding": encoding_used,
                "line_count": line_count,
            }
        except UnicodeDecodeError:
            # Skipping fallback for brevity; you could do another read with 'latin-1'
            return {
                "error": "UnicodeDecodeError on text file",
                "content": "",
                "encoding": "",
                "line_count": 0,
            }

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        content = parsed_data.get("content", "")
        return content[:max_length] + "..." if len(content) > max_length else content


class JsonParser(BaseParser):
    """Parser for JSON files."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        json_data = await AsyncFileIO.read_json(file_path)
        return {
            "content": json_data,
            "structure": type(json_data).__name__,
            "size": len(json.dumps(json_data)),
        }

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview = json.dumps(parsed_data["content"], indent=2)
        return preview[:max_length] + "..." if len(preview) > max_length else preview


class YamlParser(BaseParser):
    """Parser for YAML files."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_text(file_path)
        data = yaml.safe_load(content)
        return {
            "content": data,
            "preview": content[:1000] if len(content) > 1000 else content,
            "metadata": {
                "keys": list(data.keys()) if isinstance(data, dict) else None,
                "size": len(data) if isinstance(data, dict | list) else None,
            },
        }

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview = yaml.dump(parsed_data["content"], default_flow_style=False)
        return preview[:max_length] + "..." if len(preview) > max_length else preview


class CsvParser(BaseParser):
    """Parser for CSV files."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        try:
            content = await AsyncFileIO.read_text(file_path)  # Async read entire CSV
            if content.startswith("# "):
                return {
                    "error": content,
                    "header": None,
                    "rows": [],
                    "row_count": 0,
                    "column_count": 0,
                }

            rows = []
            reader = csv.reader(content.splitlines())
            header = next(reader, None)
            for row in reader:
                rows.append(row)

            return {
                "header": header,
                "rows": rows,
                "row_count": len(rows),
                "column_count": len(header) if header else 0,
            }
        except Exception as e:
            logger.error(f"Error parsing CSV: {e}")
            return {
                "error": str(e),
                "header": None,
                "rows": [],
                "row_count": 0,
                "column_count": 0,
            }

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview_lines = []
        if parsed_data["header"]:
            preview_lines.append(",".join(parsed_data["header"]))

        for row in parsed_data["rows"][:5]:
            preview_lines.append(",".join(row))

        preview = "\n".join(preview_lines)
        return preview[:max_length] + "..." if len(preview) > max_length else preview


class XmlParser(BaseParser):
    """Parser for XML files."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_text(file_path)
        if content.startswith("# "):
            return {
                "error": content,
                "content": "",
                "metadata": {},
                "preview": "",
            }
        root = ET.fromstring(content)
        return {
            "content": content,
            "preview": content[:1000] if len(content) > 1000 else content,
            "metadata": {
                "root_tag": root.tag,
                "children_count": len(list(root)),
                "attributes": dict(root.attrib),
            },
        }

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview = f"Root: {parsed_data['metadata']['root_tag']}\n"
        content_preview = parsed_data["content"]
        if len(preview) + len(content_preview) > max_length:
            return preview + content_preview[: max_length - len(preview)] + "..."
        return preview + content_preview


class DocumentParser(BaseParser):
    """Parser for document files (PDF, DOCX, etc.)."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        suffix = file_path.suffix.lower()
        try:
            if suffix == ".pdf":
                return await self._parse_pdf(file_path)
            elif suffix == ".docx":
                return await self._parse_docx(file_path)
            else:
                raise ValueError(f"Unsupported document format: {suffix}")
        except Exception as e:
            logger.error(f"Error parsing document {file_path}: {e}")
            raise

    async def _parse_pdf(self, file_path: Path) -> dict[str, Any]:
        loop = asyncio.get_running_loop()

        def parse_pdf_sync(p):
            reader = PdfReader(str(p))
            text_content = []
            for page in reader.pages:
                text_content.append(page.extract_text())
            return {
                "content": "\n".join(text_content),
                "metadata": reader.metadata,
                "pages": len(reader.pages),
            }

        return await loop.run_in_executor(None, parse_pdf_sync, file_path)

    async def _parse_docx(self, file_path: Path) -> dict[str, Any]:
        loop = asyncio.get_running_loop()

        def parse_docx_sync(p):
            doc = Document(str(p))
            content = "\n".join(para.text for para in doc.paragraphs)
            return {
                "content": content,
                "metadata": {
                    "sections": len(doc.sections),
                    "paragraphs": len(doc.paragraphs),
                },
            }

        return await loop.run_in_executor(None, parse_docx_sync, file_path)

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        content = parsed_data.get("content", "")
        return content[:max_length] + "..." if len(content) > max_length else content


class SpreadsheetParser(BaseParser):
    """Parser for spreadsheet files (CSV, XLSX, ODS)."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        suffix = file_path.suffix.lower()
        try:
            if suffix == ".csv":
                return await self._parse_csv(file_path)
            elif suffix == ".xlsx":
                return await self._parse_xlsx(file_path)
            elif suffix == ".ods":
                return await self._parse_ods(file_path)
            else:
                raise ValueError(f"Unsupported spreadsheet format: {suffix}")
        except Exception as e:
            logger.error(f"Error parsing spreadsheet {file_path}: {str(e)}")
            raise

    async def _parse_csv(self, file_path: Path) -> dict[str, Any]:
        loop = asyncio.get_running_loop()

        def parse_csv_sync(p):
            df = pd.read_csv(p)
            preview = df.head().to_string()
            return {
                "content": df.to_dict(),
                "preview": preview,
                "metadata": {
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": df.columns.tolist(),
                },
            }

        return await loop.run_in_executor(None, parse_csv_sync, file_path)

    async def _parse_xlsx(self, file_path: Path) -> dict[str, Any]:
        loop = asyncio.get_running_loop()

        def parse_xlsx_sync(p):
            df = pd.read_excel(p)
            preview = df.head().to_string()
            return {
                "content": df.to_dict(),
                "preview": preview,
                "metadata": {
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": df.columns.tolist(),
                    "sheet_names": pd.ExcelFile(str(p)).sheet_names,
                },
            }

        return await loop.run_in_executor(None, parse_xlsx_sync, file_path)

    async def _parse_ods(self, file_path: Path) -> dict[str, Any]:
        loop = asyncio.get_running_loop()

        def parse_ods_sync(p):
            df = pd.read_excel(p, engine="odf")
            preview = df.head().to_string()
            return {
                "content": df.to_dict(),
                "preview": preview,
                "metadata": {
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": df.columns.tolist(),
                },
            }

        return await loop.run_in_executor(None, parse_ods_sync, file_path)

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview = parsed_data.get("preview", "")
        return preview[:max_length] + "..." if len(preview) > max_length else preview


class CodeParser(BaseParser):
    """Parser for code and configuration files (Python, JS, JSON, YAML, XML, TOML)."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        suffix = file_path.suffix.lower()
        try:
            if suffix == ".py":
                return await self._parse_python(file_path)
            elif suffix == ".js":
                return await self._parse_javascript(file_path)
            elif suffix == ".json":
                return await self._parse_json(file_path)
            elif suffix in [".yaml", ".yml"]:
                return await self._parse_yaml(file_path)
            elif suffix == ".xml":
                return await self._parse_xml(file_path)
            elif suffix == ".toml":
                return await self._parse_toml(file_path)
            else:
                raise ValueError(f"Unsupported code/config format: {suffix}")
        except Exception as e:
            logger.error(f"Error parsing code/config file {file_path}: {str(e)}")
            raise

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview = parsed_data.get("preview", "")
        return preview[:max_length] + "..." if len(preview) > max_length else preview

    async def _parse_python(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_text(file_path)
        if content.startswith("# "):
            return {"error": content, "preview": content, "metadata": {}}

        try:
            tree = ast.parse(content)
            classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
            functions = [
                n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)
            ]
            imports = [
                n for n in ast.walk(tree) if isinstance(n, ast.Import | ast.ImportFrom)
            ]
            return {
                "content": content,
                "preview": content[:1000] if len(content) > 1000 else content,
                "metadata": {
                    "classes": classes,
                    "functions": functions,
                    "import_count": len(imports),
                    "loc": len(content.splitlines()),
                },
            }
        except Exception as e:
            logger.error(f"Python parsing error: {str(e)}")
            raise

    async def _parse_javascript(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_text(file_path)
        if content.startswith("// "):
            return {"error": content, "preview": content, "metadata": {}}

        return {
            "content": content,
            "preview": content[:1000] if len(content) > 1000 else content,
            "metadata": {"loc": len(content.splitlines())},
        }

    async def _parse_json(self, file_path: Path) -> dict[str, Any]:
        json_data = await AsyncFileIO.read_json(file_path)
        if not json_data:
            return {
                "error": "Failed to parse JSON file",
                "preview": "Error reading JSON",
                "metadata": {},
            }
        content = await AsyncFileIO.read_text(file_path)
        return {
            "content": json_data,
            "preview": content[:1000] if len(content) > 1000 else content,
            "metadata": {
                "keys": list(json_data.keys()) if isinstance(json_data, dict) else None,
                "size": (
                    len(json_data) if isinstance(json_data, dict | list) else None
                ),
            },
        }

    async def _parse_yaml(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_text(file_path)
        data = yaml.safe_load(content)
        return {
            "content": data,
            "preview": content[:1000] if len(content) > 1000 else content,
            "metadata": {
                "keys": list(data.keys()) if isinstance(data, dict) else None,
                "size": len(data) if isinstance(data, dict | list) else None,
            },
        }

    async def _parse_xml(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_text(file_path)
        root = ET.fromstring(content)
        return {
            "content": content,
            "preview": content[:1000] if len(content) > 1000 else content,
            "metadata": {
                "root_tag": root.tag,
                "children_count": len(list(root)),
                "attributes": dict(root.attrib),
            },
        }

    async def _parse_toml(self, file_path: Path) -> dict[str, Any]:
        content = await AsyncFileIO.read_binary(file_path)
        try:
            data = tomli.loads(content.decode("utf-8"))
            return {
                "content": data,
                "preview": str(data)[:1000],
                "metadata": {
                    "keys": list(data.keys()) if isinstance(data, dict) else None,
                    "size": len(data) if isinstance(data, dict | list) else None,
                },
            }
        except Exception as e:
            logger.error(f"TOML parsing error: {str(e)}")
            raise


class VectorParser(BaseParser):
    """Parser for CAD and vector files (DWG, DXF, SVG)."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        suffix = file_path.suffix.lower()
        try:
            if suffix in [".dwg", ".dxf"]:
                return await self._parse_cad(file_path)
            elif suffix == ".svg":
                return await self._parse_svg(file_path)
            else:
                raise ValueError(f"Unsupported vector format: {suffix}")
        except Exception as e:
            logger.error(f"Error parsing vector file {file_path}: {str(e)}")
            raise

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        preview = parsed_data.get("preview", "")
        return preview[:max_length] + "..." if len(preview) > max_length else preview

    async def _parse_cad(self, file_path: Path) -> dict[str, Any]:
        try:
            # Import the readfile function from the filemanagement module
            from ezdxf.filemanagement import readfile

            doc = readfile(str(file_path))
            modelspace = doc.modelspace()
            entities = {
                "lines": len(modelspace.query("LINE")),
                "circles": len(modelspace.query("CIRCLE")),
                "arcs": len(modelspace.query("ARC")),
                "polylines": len(modelspace.query("LWPOLYLINE")),
                "text": len(modelspace.query("TEXT")),
            }
            layers = [layer.dxf.name for layer in doc.layers]
            metadata = {
                "filename": file_path.name,
                "created_by": doc.header["$TDCREATE"],
                "last_modified": doc.header["$TDUPDATE"],
                "drawing_units": doc.header["$INSUNITS"],
                "layers": layers,
                "entity_counts": entities,
            }
            return {
                "content": str(doc.entitydb),
                "preview": f"CAD drawing with {sum(entities.values())} entities across {len(layers)} layers",
                "metadata": metadata,
            }
        except Exception as e:
            logger.error(f"CAD parsing error: {str(e)}")
            raise

    async def _parse_svg(self, file_path: Path) -> dict[str, Any]:
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            width = root.get("width", "unknown")
            height = root.get("height", "unknown")
            view_box = root.get("viewBox", "unknown")
            elements = {
                "path": len(root.findall(".//{*}path")),
                "rect": len(root.findall(".//{*}rect")),
                "circle": len(root.findall(".//{*}circle")),
                "text": len(root.findall(".//{*}text")),
                "group": len(root.findall(".//{*}g")),
            }
            content = await AsyncFileIO.read_text(file_path)
            if content.startswith("# "):
                return {"error": content, "preview": content, "metadata": {}}
            return {
                "content": content,
                "preview": f"SVG image ({width}x{height}) with {sum(elements.values())} elements",
                "metadata": {
                    "dimensions": {
                        "width": width,
                        "height": height,
                        "view_box": view_box,
                    },
                    "element_counts": elements,
                    "xmlns": root.get("xmlns", "unknown"),
                    "version": root.get("version", "unknown"),
                },
            }
        except Exception as e:
            logger.error(f"SVG parsing error: {str(e)}")
            raise


class ArchiveParser(BaseParser):
    """Parser for archive files (ZIP, TAR, RAR, 7Z)."""

    async def parse(self, file_path: Path) -> dict[str, Any]:
        try:
            if not await AsyncFileIO.exists(file_path):
                raise FileNotFoundError(f"Archive file not found: {file_path}")

            suffix = file_path.suffix.lower()
            files_list = []

            if suffix == ".zip":
                with zipfile.ZipFile(file_path, "r") as archive:
                    files_list = archive.namelist()
            elif suffix in [".tar", ".tgz", ".gz", ".bz2"]:
                with tarfile.open(file_path, "r") as archive:
                    files_list = archive.getnames()
            elif suffix == ".rar":
                try:
                    with rarfile.RarFile(file_path, "r") as archive:
                        files_list = archive.namelist()
                except Exception as e:
                    if isinstance(e, rarfile.NotRarFile):
                        raise ValueError(f"Unsupported archive format: {suffix}") from e
                    else:
                        raise
            elif suffix == ".7z":
                with py7zr.SevenZipFile(file_path, "r") as archive:
                    files_list = archive.getnames()
            else:
                raise ValueError(f"Unsupported archive format: {suffix}")

            return {"files": files_list, "count": len(files_list)}
        except Exception as e:
            logger.error(f"Archive parsing failed for {file_path}: {e}", exc_info=True)
            raise

    def get_preview(self, parsed_data: dict[str, Any], max_length: int = 1000) -> str:
        files = parsed_data.get("files", [])
        preview = "\n".join(files)
        return preview[:max_length] + "..." if len(preview) > max_length else preview


def get_parser_for_mime_type(mime_type: str) -> BaseParser | None:
    """
    Factory function to get the appropriate parser for a MIME type.
    """
    parsers = {
        "text/plain": TextParser(),
        "text/markdown": TextParser(),
        "text/html": TextParser(),
        "application/json": JsonParser(),
        "application/yaml": YamlParser(),
        "text/yaml": YamlParser(),
        "text/csv": CsvParser(),
        "application/xml": XmlParser(),
        "text/xml": XmlParser(),
        "application/pdf": DocumentParser(),
        # Office document formats
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": DocumentParser(),
        "application/vnd.oasis.opendocument.text": DocumentParser(),
        "application/epub+zip": DocumentParser(),
        "application/zip": ArchiveParser(),
        "application/x-tar": ArchiveParser(),
        "application/x-rar-compressed": ArchiveParser(),
        "application/x-7z-compressed": ArchiveParser(),
        "application/gzip": ArchiveParser(),
        "application/x-bzip2": ArchiveParser(),
        "image/vnd.dxf": VectorParser(),
        "image/x-dwg": VectorParser(),
        "image/svg+xml": VectorParser(),
        "text/x-python": CodeParser(),
        "application/javascript": CodeParser(),
        "application/toml": CodeParser(),
    }

    if mime_type in parsers:
        return parsers[mime_type]

    main_type = mime_type.split("/")[0]
    if main_type == "text":
        return TextParser()

    return None


