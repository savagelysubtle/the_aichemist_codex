

================================================
File: src/the_aichemist_codex/backend/file_manager/__init__.py
================================================
"""File management module for The Aichemist Codex."""

from .change_detector import ChangeDetector
from .change_history_manager import ChangeHistoryManager, change_history_manager
from .directory_manager import DirectoryManager
from .directory_monitor import DirectoryMonitor, directory_monitor
from .duplicate_detector import DuplicateDetector
from .file_mover import FileMover
from .file_tree import FileTreeGenerator
from .file_watcher import FileEventHandler, monitor_directory
from .sorter import RuleBasedSorter
from .version_manager import VersionManager, version_manager

__all__ = [
    "FileMover",
    "FileTreeGenerator",
    "monitor_directory",
    "DuplicateDetector",
    "FileEventHandler",
    "RuleBasedSorter",
    "DirectoryManager",
    "ChangeDetector",
    "ChangeHistoryManager",
    "change_history_manager",
    "DirectoryMonitor",
    "directory_monitor",
    "VersionManager",
    "version_manager",
]



================================================
File: src/the_aichemist_codex/backend/file_manager/batch_file_operations.py
================================================
"""Provides batch file operations with rollback support."""

import logging
import os
from pathlib import Path

from the_aichemist_codex.backend.rollback.rollback_manager import rollback_manager
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
from the_aichemist_codex.backend.utils.batch_processor import BatchProcessor
from the_aichemist_codex.backend.utils.safety import SafeFileHandler

logger = logging.getLogger(__name__)


class BatchFileOperations:
    """Handles batch file operations with rollback support."""

    @staticmethod
    async def move_files(
        file_mappings: dict[Path, Path], batch_size: int = 10
    ) -> list[tuple[Path, Path]]:
        """
        Move multiple files in batches with rollback tracking.

        Args:
            file_mappings: Dict mapping source paths to destination paths
            batch_size: Number of files to move in each batch

        Returns:
            List of successfully moved files (source, destination) tuples
        """
        # Convert dict to list of tuples for batch processing
        operations = list(file_mappings.items())

        # Safety checks before processing
        for src, dst in operations:
            if not await AsyncFileIO.exists(src):
                logger.error(f"Source file does not exist: {src}")
                return []
            if SafeFileHandler.should_ignore(src):
                logger.warning(f"Ignoring blocked file: {src}")
                return []
            if await AsyncFileIO.exists(dst):
                logger.warning(f"Destination file already exists: {dst}")
            if not dst.parent.exists():
                logger.info(f"Will create destination directory: {dst.parent}")

        async def move_operation(item: tuple[Path, Path]) -> tuple[Path, Path]:
            src, dst = item

            # Record operation for rollback
            await rollback_manager.record_operation("move", str(src), str(dst))

            # Ensure destination directory exists
            await AsyncFileIO.write(dst.parent / ".placeholder", "")

            # Move the file
            content = await AsyncFileIO.read_binary(src)
            success = await AsyncFileIO.write_binary(dst, content)

            if success:
                try:
                    os.remove(src)
                    return (src, dst)
                except Exception as e:
                    logger.error(f"Error removing source file {src}: {e}")
                    # Try to rollback the write
                    await AsyncFileIO.write_binary(src, content)
                    raise e
            else:
                raise Exception(f"Failed to write file to {dst}")

        results = await BatchProcessor.process_batch(
            operations, move_operation, batch_size=batch_size
        )

        return results

    @staticmethod
    async def copy_files(
        file_mappings: dict[Path, Path], batch_size: int = 10
    ) -> list[tuple[Path, Path]]:
        """
        Copy multiple files in batches with rollback tracking.

        Args:
            file_mappings: Dict mapping source paths to destination paths
            batch_size: Number of files to copy in each batch

        Returns:
            List of successfully copied files (source, destination) tuples
        """
        operations = list(file_mappings.items())

        # Safety checks before processing
        for src, dst in operations:
            if not await AsyncFileIO.exists(src):
                logger.error(f"Source file does not exist: {src}")
                return []
            if SafeFileHandler.should_ignore(src):
                logger.warning(f"Ignoring blocked file: {src}")
                return []
            if await AsyncFileIO.exists(dst):
                logger.warning(f"Destination file already exists: {dst}")
            if not dst.parent.exists():
                logger.info(f"Will create destination directory: {dst.parent}")

        async def copy_operation(item: tuple[Path, Path]) -> tuple[Path, Path]:
            src, dst = item

            # Record operation for rollback
            await rollback_manager.record_operation("copy", str(src), str(dst))

            # Ensure destination directory exists
            await AsyncFileIO.write(dst.parent / ".placeholder", "")

            # Copy the file
            success = await AsyncFileIO.copy(src, dst)

            if success:
                return (src, dst)
            else:
                raise Exception(f"Failed to copy file to {dst}")

        results = await BatchProcessor.process_batch(
            operations, copy_operation, batch_size=batch_size
        )

        return results

    @staticmethod
    async def delete_files(files: list[Path], batch_size: int = 10) -> list[Path]:
        """
        Delete multiple files in batches with rollback tracking.

        Args:
            files: List of file paths to delete
            batch_size: Number of files to delete in each batch

        Returns:
            List of successfully deleted file paths
        """
        # Safety checks before processing
        for file_path in files:
            if not await AsyncFileIO.exists(file_path):
                logger.error(f"File does not exist: {file_path}")
                return []
            if SafeFileHandler.should_ignore(file_path):
                logger.warning(f"Ignoring blocked file: {file_path}")
                return []

        async def delete_operation(file_path: Path) -> Path:
            # Record operation for rollback and backup the file
            content = await AsyncFileIO.read_binary(file_path)
            await rollback_manager.record_operation("delete", str(file_path))

            try:
                os.remove(file_path)
                return file_path
            except Exception as e:
                logger.error(f"Error deleting file {file_path}: {e}")
                # Try to restore the file
                await AsyncFileIO.write_binary(file_path, content)
                raise e

        results = await BatchProcessor.process_batch(
            files, delete_operation, batch_size=batch_size
        )

        return results



================================================
File: src/the_aichemist_codex/backend/file_manager/change_detector.py
================================================
"""
Change detection and classification for The Aichemist Codex.

This module provides functionality to detect and classify different types of changes
in monitored files, with specific detection algorithms for various file types.
"""

import asyncio
import difflib
import hashlib
import logging
import time
from enum import Enum
from pathlib import Path

from the_aichemist_codex.backend.config.config_loader import config
from the_aichemist_codex.backend.utils.safety import SafeFileHandler

logger = logging.getLogger(__name__)


class ChangeSeverity(Enum):
    """Classification of change severity levels."""

    MINOR = 1  # Small changes that don't significantly alter the file
    MODERATE = 2  # Notable changes that modify some important aspects
    MAJOR = 3  # Substantial changes that significantly alter the file
    CRITICAL = 4  # Fundamental changes that completely transform the file


class ChangeType(Enum):
    """Types of changes that can be detected."""

    CONTENT = 1  # Content changes within the file
    METADATA = 2  # Changes to file metadata like permissions or timestamps
    RENAME = 3  # File was renamed
    MOVE = 4  # File was moved to a new location
    CREATE = 5  # File was newly created
    DELETE = 6  # File was deleted


class ChangeInfo:
    """Stores information about a detected change."""

    def __init__(
        self,
        file_path: Path,
        change_type: ChangeType,
        severity: ChangeSeverity,
        timestamp: float | None = None,
        details: dict | None = None,
        old_path: Path | None = None,
    ):
        """
        Initialize change information.

        Args:
            file_path: Path to the changed file
            change_type: Type of change that occurred
            severity: Severity classification of the change
            timestamp: When the change occurred (defaults to now)
            details: Additional details about the change
            old_path: Previous path for renamed/moved files
        """
        self.file_path = file_path
        self.change_type = change_type
        self.severity = severity
        self.timestamp = timestamp or time.time()
        self.details = details or {}
        self.old_path = old_path

    def to_dict(self) -> dict:
        """Convert change info to a dictionary for serialization."""
        return {
            "file_path": str(self.file_path),
            "change_type": self.change_type.name,
            "severity": self.severity.name,
            "timestamp": self.timestamp,
            "details": self.details,
            "old_path": str(self.old_path) if self.old_path else None,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "ChangeInfo":
        """Create from a dictionary representation."""
        return cls(
            file_path=Path(data["file_path"]),
            change_type=ChangeType[data["change_type"]],
            severity=ChangeSeverity[data["severity"]],
            timestamp=data["timestamp"],
            details=data["details"],
            old_path=Path(data["old_path"]) if data["old_path"] else None,
        )

    def __str__(self) -> str:
        """String representation of the change."""
        return (
            f"Change[{self.change_type.name}] in {self.file_path} "
            f"(Severity: {self.severity.name})"
        )


class ChangeDetector:
    """
    Detects and classifies different types of changes in files.

    Features:
    - Content-based change detection for text files
    - Hash-based detection for binary files
    - Smart debounce for rapid sequential changes
    - Change severity classification
    """

    def __init__(self):
        """Initialize the change detector with debounce settings."""
        self.file_cache: dict[str, dict] = {}
        self.debounce_interval = config.get("change_debounce_interval", 2.0)
        self.debounce_buffer: dict[str, tuple[float, asyncio.Task]] = {}
        self.txt_extensions = {
            ".txt",
            ".md",
            ".py",
            ".js",
            ".html",
            ".css",
            ".json",
            ".xml",
            ".yml",
            ".yaml",
            ".csv",
        }
        self.max_text_size = config.get(
            "max_text_diff_size", 1024 * 1024
        )  # 1MB default

        # Thresholds for severity classification
        self.minor_threshold = config.get("minor_change_threshold", 0.05)  # 5% changed
        self.moderate_threshold = config.get(
            "moderate_change_threshold", 0.25
        )  # 25% changed
        self.major_threshold = config.get("major_change_threshold", 0.50)  # 50% changed
        # Above major_threshold is considered CRITICAL

    async def detect_change(self, file_path: Path) -> ChangeInfo | None:
        """
        Detect changes in a file compared to its previously known state.
        For new files, establishes a baseline.

        Args:
            file_path: Path to the file to check

        Returns:
            ChangeInfo object if a change is detected, None otherwise
        """
        if not file_path.exists():
            # File was deleted or moved, handled by the event handler
            return None

        # Skip ignored files
        if SafeFileHandler.should_ignore(file_path):
            return None

        # Implement smart debounce
        path_str = str(file_path)
        current_time = time.time()

        # If there's an existing debounce task for this file
        if path_str in self.debounce_buffer:
            last_time, task = self.debounce_buffer[path_str]

            # If it's within the debounce interval, cancel the old task and schedule a new one
            if current_time - last_time < self.debounce_interval:
                if not task.done():
                    task.cancel()

                # Create a new task with the current file state
                new_task = asyncio.create_task(
                    self._delayed_process(file_path, self.debounce_interval)
                )
                self.debounce_buffer[path_str] = (current_time, new_task)
                return None

        # No active debounce, process immediately
        return await self._process_change(file_path)

    async def _delayed_process(
        self, file_path: Path, delay: float
    ) -> ChangeInfo | None:
        """
        Process a file change after a delay to handle rapid sequential changes.

        Args:
            file_path: Path to the file to check
            delay: Time to wait before processing

        Returns:
            ChangeInfo if a change is detected, None otherwise
        """
        await asyncio.sleep(delay)
        path_str = str(file_path)

        # Remove from debounce buffer when processing
        if path_str in self.debounce_buffer:
            del self.debounce_buffer[path_str]

        return await self._process_change(file_path)

    async def _process_change(self, file_path: Path) -> ChangeInfo | None:
        """
        Process a file to detect changes compared to cache.

        Args:
            file_path: Path to the file to check

        Returns:
            ChangeInfo if a change is detected, None otherwise
        """
        path_str = str(file_path)

        try:
            # Get current file stats
            stats = await asyncio.to_thread(file_path.stat)
            modified_time = stats.st_mtime
            file_size = stats.st_size

            # Check cache for previous state
            if path_str in self.file_cache:
                # File exists in cache, compare for changes
                cached = self.file_cache[path_str]

                # Check if modified time is different
                if modified_time == cached.get(
                    "modified_time"
                ) and file_size == cached.get("file_size"):
                    # No changes detected based on stats
                    return None

                # Determine file type and appropriate detection method
                is_text = await self._is_text_file(file_path)

                if is_text and file_size <= self.max_text_size:
                    # Use content-based detection for text files
                    return await self._detect_text_changes(file_path, cached)
                else:
                    # Use hash-based detection for binary or large files
                    return await self._detect_binary_changes(file_path, cached)
            else:
                # New file, add to cache
                file_hash = await self._calculate_file_hash(file_path)

                # Store in cache
                self.file_cache[path_str] = {
                    "modified_time": modified_time,
                    "file_size": file_size,
                    "hash": file_hash,
                    "content": None,  # Don't store content by default
                }

                # First time seeing this file, treat as creation
                return ChangeInfo(
                    file_path=file_path,
                    change_type=ChangeType.CREATE,
                    severity=ChangeSeverity.MODERATE,
                    details={"size": file_size},
                )
        except Exception as e:
            logger.error(f"Error detecting changes for {file_path}: {e}")
            return None

    async def _is_text_file(self, file_path: Path) -> bool:
        """
        Determine if a file is a text file based on extension and content sampling.

        Args:
            file_path: Path to check

        Returns:
            True if the file appears to be text, False otherwise
        """
        # Quick check based on extension
        if file_path.suffix.lower() in self.txt_extensions:
            return True

        if SafeFileHandler.is_binary_file(file_path):
            return False

        # Sample the first few KB to check for binary content
        try:
            sample_size = min(4096, file_path.stat().st_size)
            if sample_size == 0:
                return True  # Empty file is considered text

            with open(file_path, "rb") as f:
                sample = f.read(sample_size)

            # Check for null bytes which indicate binary content
            return b"\0" not in sample
        except Exception as e:
            logger.warning(f"Error checking if file is text: {file_path}, {e}")
            return False

    async def _detect_text_changes(
        self, file_path: Path, cached: dict
    ) -> ChangeInfo | None:
        """
        Detect changes in text files using diff algorithms.

        Args:
            file_path: Path to the text file
            cached: Cached information about the file

        Returns:
            ChangeInfo with details about the changes
        """
        try:
            # Read current content
            current_content = await asyncio.to_thread(
                file_path.read_text, encoding="utf-8"
            )

            # If we don't have cached content, read it now
            cached_content = cached.get("content")
            if cached_content is None:
                # No previous content to compare, update cache and return
                file_hash = await self._calculate_file_hash(file_path)
                self.file_cache[str(file_path)].update(
                    {
                        "hash": file_hash,
                        "content": current_content,
                        "modified_time": file_path.stat().st_mtime,
                        "file_size": file_path.stat().st_size,
                    }
                )
                return None

            # Calculate diff between versions
            diff = list(
                difflib.unified_diff(
                    cached_content.splitlines(), current_content.splitlines()
                )
            )

            if not diff:
                # No actual content changes despite stat changes
                return None

            # Count added/removed lines
            added = sum(
                1
                for line in diff
                if line.startswith("+") and not line.startswith("+++")
            )
            removed = sum(
                1
                for line in diff
                if line.startswith("-") and not line.startswith("---")
            )
            total_lines = max(
                len(cached_content.splitlines()), len(current_content.splitlines())
            )

            # Calculate change percentage
            change_percentage = (
                (added + removed) / total_lines if total_lines > 0 else 0
            )

            # Determine severity based on percentage changed
            severity = self._classify_severity(change_percentage)

            # Update cache with new content
            self.file_cache[str(file_path)].update(
                {
                    "content": current_content,
                    "modified_time": file_path.stat().st_mtime,
                    "file_size": file_path.stat().st_size,
                    "hash": await self._calculate_file_hash(file_path),
                }
            )

            return ChangeInfo(
                file_path=file_path,
                change_type=ChangeType.CONTENT,
                severity=severity,
                details={
                    "added_lines": added,
                    "removed_lines": removed,
                    "total_lines": total_lines,
                    "change_percentage": change_percentage,
                },
            )
        except UnicodeDecodeError:
            # File isn't valid text, use hash-based detection instead
            return await self._detect_binary_changes(file_path, cached)
        except Exception as e:
            logger.error(f"Error detecting text changes: {file_path}, {e}")
            return None

    async def _detect_binary_changes(
        self, file_path: Path, cached: dict
    ) -> ChangeInfo | None:
        """
        Detect changes in binary files using hash comparison.

        Args:
            file_path: Path to the binary file
            cached: Cached information about the file

        Returns:
            ChangeInfo with basic change details
        """
        try:
            # Calculate current hash
            current_hash = await self._calculate_file_hash(file_path)
            cached_hash = cached.get("hash")

            if current_hash == cached_hash:
                # File hasn't changed despite stat differences
                return None

            # Update cache
            stats = file_path.stat()
            self.file_cache[str(file_path)].update(
                {
                    "hash": current_hash,
                    "modified_time": stats.st_mtime,
                    "file_size": stats.st_size,
                }
            )

            # Get file size difference
            size_diff = abs(stats.st_size - cached.get("file_size", 0))
            size_percentage = size_diff / max(stats.st_size, 1)

            # Determine severity based on size changes
            severity = self._classify_severity(size_percentage)

            return ChangeInfo(
                file_path=file_path,
                change_type=ChangeType.CONTENT,
                severity=severity,
                details={
                    "old_size": cached.get("file_size", 0),
                    "new_size": stats.st_size,
                    "size_diff": size_diff,
                    "size_percentage": size_percentage,
                },
            )
        except Exception as e:
            logger.error(f"Error detecting binary changes: {file_path}, {e}")
            return None

    async def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA-256 hash of a file asynchronously."""
        try:
            hasher = hashlib.sha256()

            # Process in chunks to handle large files
            async def process_file():
                chunk_size = 65536  # 64KB chunks
                with open(file_path, "rb") as f:
                    while chunk := f.read(chunk_size):
                        hasher.update(chunk)

            await asyncio.to_thread(process_file)
            return hasher.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating file hash: {file_path}, {e}")
            return ""

    def _classify_severity(self, change_percentage: float) -> ChangeSeverity:
        """
        Classify the severity of a change based on the percentage changed.

        Args:
            change_percentage: Percentage of file that changed (0.0-1.0)

        Returns:
            ChangeSeverity classification
        """
        if change_percentage <= self.minor_threshold:
            return ChangeSeverity.MINOR
        elif change_percentage <= self.moderate_threshold:
            return ChangeSeverity.MODERATE
        elif change_percentage <= self.major_threshold:
            return ChangeSeverity.MAJOR
        else:
            return ChangeSeverity.CRITICAL



================================================
File: src/the_aichemist_codex/backend/file_manager/change_history_manager.py
================================================
"""
Change history storage and management for The Aichemist Codex.

This module provides functionality to store, retrieve, and manage
file change history in a SQLite database.
"""

import asyncio
import json
import logging
import os
import sqlite3
import time
from datetime import datetime
from pathlib import Path

from the_aichemist_codex.backend.config.config_loader import config
from the_aichemist_codex.backend.file_manager.change_detector import (
    ChangeInfo,
    ChangeSeverity,
    ChangeType,
)

logger = logging.getLogger(__name__)


# Create a function to determine the data directory dynamically
def get_data_dir() -> Path:
    """Get the data directory path dynamically to avoid circular imports."""
    # Use environment variable if available
    env_data_dir = os.environ.get("AICHEMIST_DATA_DIR")
    if env_data_dir:
        return Path(env_data_dir)

    # Fallback to a default location relative to this file
    return Path(__file__).resolve().parents[3] / "data"


# Initialize the change history database path based on the data directory
CHANGE_HISTORY_DB = get_data_dir() / "change_history.db"


class ChangeHistoryManager:
    """
    Manages storage and retrieval of file change history.

    Features:
    - Maintains a SQLite database of file changes
    - Stores metadata about each change (timestamp, type, severity, etc.)
    - Provides querying by file, time range, change type, etc.
    - Implements cleanup policies for old records
    """

    _instance = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(ChangeHistoryManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, db_path: Path = CHANGE_HISTORY_DB):
        """
        Initialize the change history manager.

        Args:
            db_path: Path to the SQLite database file
        """
        # Only initialize once (singleton pattern)
        if ChangeHistoryManager._initialized:
            return

        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.retention_days = config.get("change_history_retention_days", 30)
        self._init_db()

        ChangeHistoryManager._initialized = True

    def _init_db(self) -> None:
        """Initialize the SQLite database with necessary tables."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Create changes table
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS changes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT NOT NULL,
                    change_type TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    details TEXT,
                    old_path TEXT
                )
                """)

                # Create indexes for efficient querying
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_file_path ON changes(file_path)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_timestamp ON changes(timestamp)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_change_type ON changes(change_type)"
                )

                conn.commit()
                logger.info(f"Change history database initialized at {self.db_path}")
        except sqlite3.Error as e:
            logger.error(f"Error initializing change history database: {e}")

    async def record_change(self, change_info: ChangeInfo) -> bool:
        """
        Record a file change in the database.

        Args:
            change_info: Information about the detected change

        Returns:
            True if successfully recorded, False otherwise
        """
        try:
            # Serialize details dictionary to JSON
            details_json = json.dumps(change_info.details)
            old_path = str(change_info.old_path) if change_info.old_path else None

            # Run the database operation in a thread to avoid blocking
            return await asyncio.to_thread(
                self._insert_change,
                str(change_info.file_path),
                change_info.change_type.name,
                change_info.severity.name,
                change_info.timestamp,
                details_json,
                old_path,
            )
        except Exception as e:
            logger.error(f"Error recording change: {e}")
            return False

    def _insert_change(
        self,
        file_path: str,
        change_type: str,
        severity: str,
        timestamp: float,
        details_json: str,
        old_path: str | None,
    ) -> bool:
        """
        Insert a change record into the database (synchronous).

        Args:
            file_path: Path to the changed file
            change_type: Type of change (CREATE, MODIFY, etc.)
            severity: Severity level (MINOR, MAJOR, etc.)
            timestamp: Unix timestamp of the change
            details_json: JSON string of change details
            old_path: Previous path for moved/renamed files

        Returns:
            True if successful, False otherwise
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT INTO changes
                    (file_path, change_type, severity, timestamp, details, old_path)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (
                        file_path,
                        change_type,
                        severity,
                        timestamp,
                        details_json,
                        old_path,
                    ),
                )
                conn.commit()
                return True
        except sqlite3.Error as e:
            logger.error(f"Database error recording change: {e}")
            return False

    async def get_changes_by_file(
        self, file_path: Path, limit: int = 100
    ) -> list[ChangeInfo]:
        """
        Retrieve change history for a specific file.

        Args:
            file_path: Path to the file
            limit: Maximum number of records to return

        Returns:
            List of ChangeInfo objects, newest first
        """
        return await self._query_changes(
            "WHERE file_path = ? OR old_path = ? ORDER BY timestamp DESC LIMIT ?",
            (str(file_path), str(file_path), limit),
        )

    async def get_changes_by_time_range(
        self, start_time: float, end_time: float | None = None, limit: int = 100
    ) -> list[ChangeInfo]:
        """
        Retrieve changes that occurred within a specific time range.

        Args:
            start_time: Start timestamp (Unix time)
            end_time: End timestamp (defaults to now)
            limit: Maximum number of records to return

        Returns:
            List of ChangeInfo objects
        """
        end_time = end_time or time.time()
        return await self._query_changes(
            "WHERE timestamp BETWEEN ? AND ? ORDER BY timestamp DESC LIMIT ?",
            (start_time, end_time, limit),
        )

    async def get_changes_by_type(
        self, change_type: ChangeType, limit: int = 100
    ) -> list[ChangeInfo]:
        """
        Retrieve changes of a specific type.

        Args:
            change_type: Type of change to filter by
            limit: Maximum number of records to return

        Returns:
            List of ChangeInfo objects
        """
        return await self._query_changes(
            "WHERE change_type = ? ORDER BY timestamp DESC LIMIT ?",
            (change_type.name, limit),
        )

    async def get_changes_by_severity(
        self, min_severity: ChangeSeverity, limit: int = 100
    ) -> list[ChangeInfo]:
        """
        Retrieve changes with at least the specified severity.

        Args:
            min_severity: Minimum severity level
            limit: Maximum number of records to return

        Returns:
            List of ChangeInfo objects
        """
        # Convert severity enum to a list of severity names to include
        severity_levels = [
            level.name for level in ChangeSeverity if level.value >= min_severity.value
        ]

        placeholders = ",".join(["?"] * len(severity_levels))
        query = f"WHERE severity IN ({placeholders}) ORDER BY timestamp DESC LIMIT ?"
        params = (*severity_levels, limit)

        return await self._query_changes(query, params)

    async def _query_changes(
        self, where_clause: str, params: tuple
    ) -> list[ChangeInfo]:
        """
        Execute a database query and return results as ChangeInfo objects.

        Args:
            where_clause: SQL WHERE clause
            params: Query parameters

        Returns:
            List of ChangeInfo objects
        """
        try:
            # Run in a thread to avoid blocking
            rows = await asyncio.to_thread(self._execute_query, where_clause, params)

            # Convert rows to ChangeInfo objects
            changes = []
            for row in rows:
                (
                    change_id,
                    file_path,
                    change_type,
                    severity,
                    timestamp,
                    details_json,
                    old_path,
                ) = row

                try:
                    details = json.loads(details_json) if details_json else {}
                except json.JSONDecodeError:
                    details = {}

                changes.append(
                    ChangeInfo(
                        file_path=Path(file_path),
                        change_type=ChangeType[change_type],
                        severity=ChangeSeverity[severity],
                        timestamp=timestamp,
                        details=details,
                        old_path=Path(old_path) if old_path else None,
                    )
                )

            return changes
        except Exception as e:
            logger.error(f"Error querying changes: {e}")
            return []

    def _execute_query(self, where_clause: str, params: tuple) -> list[tuple]:
        """
        Execute a database query synchronously.

        Args:
            where_clause: SQL WHERE clause
            params: Query parameters

        Returns:
            List of result rows
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    f"SELECT id, file_path, change_type, severity, timestamp, details, old_path "
                    f"FROM changes {where_clause}",
                    params,
                )
                return cursor.fetchall()
        except sqlite3.Error as e:
            logger.error(f"Database error executing query: {e}")
            return []

    async def cleanup_old_records(self, days: float | None = None) -> int:
        """
        Remove change records older than the specified number of days.

        Args:
            days: Number of days to keep (defaults to configured retention period)

        Returns:
            Number of records removed
        """
        retention_days = days if days is not None else self.retention_days
        cutoff_time = time.time() - (retention_days * 86400)  # Convert days to seconds

        try:
            return await asyncio.to_thread(self._delete_old_records, cutoff_time)
        except Exception as e:
            logger.error(f"Error cleaning up old records: {e}")
            return 0

    def _delete_old_records(self, cutoff_time: float) -> int:
        """
        Delete records older than the cutoff time (synchronous).

        Args:
            cutoff_time: Timestamp threshold for deletion

        Returns:
            Number of records deleted
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "DELETE FROM changes WHERE timestamp < ?", (cutoff_time,)
                )
                deleted_count = cursor.rowcount
                conn.commit()
                logger.info(f"Removed {deleted_count} old change records")
                return deleted_count
        except sqlite3.Error as e:
            logger.error(f"Database error deleting old records: {e}")
            return 0

    async def get_statistics(self) -> dict:
        """
        Get statistics about the change history.

        Returns:
            Dictionary with statistics like total changes, changes by type, etc.
        """
        try:
            return await asyncio.to_thread(self._gather_statistics)
        except Exception as e:
            logger.error(f"Error gathering statistics: {e}")
            return {}

    def _gather_statistics(self) -> dict:
        """
        Gather statistics from the database (synchronous).

        Returns:
            Dictionary with various statistics
        """
        stats = {
            "total_changes": 0,
            "changes_by_type": {},
            "changes_by_severity": {},
            "recent_changes": 0,  # Last 24 hours
            "oldest_record": None,
            "newest_record": None,
        }

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # Total count
                cursor.execute("SELECT COUNT(*) FROM changes")
                stats["total_changes"] = cursor.fetchone()[0]

                # Counts by type
                cursor.execute(
                    "SELECT change_type, COUNT(*) FROM changes GROUP BY change_type"
                )
                stats["changes_by_type"] = {row[0]: row[1] for row in cursor.fetchall()}

                # Counts by severity
                cursor.execute(
                    "SELECT severity, COUNT(*) FROM changes GROUP BY severity"
                )
                stats["changes_by_severity"] = {
                    row[0]: row[1] for row in cursor.fetchall()
                }

                # Recent changes (last 24 hours)
                recent_cutoff = time.time() - 86400
                cursor.execute(
                    "SELECT COUNT(*) FROM changes WHERE timestamp > ?", (recent_cutoff,)
                )
                stats["recent_changes"] = cursor.fetchone()[0]

                # Oldest and newest timestamps
                if stats["total_changes"] > 0:
                    cursor.execute("SELECT MIN(timestamp), MAX(timestamp) FROM changes")
                    oldest, newest = cursor.fetchone()
                    stats["oldest_record"] = datetime.fromtimestamp(oldest).isoformat()
                    stats["newest_record"] = datetime.fromtimestamp(newest).isoformat()

                return stats
        except sqlite3.Error as e:
            logger.error(f"Database error gathering statistics: {e}")
            return stats


# Create a singleton instance
change_history_manager = ChangeHistoryManager()



================================================
File: src/the_aichemist_codex/backend/file_manager/common.py
================================================
"""
Common utilities and shared functionality for file management.

This module contains shared functionality between directory_monitor and file_watcher
to avoid circular dependencies.
"""

import logging
from enum import Enum
from pathlib import Path

logger = logging.getLogger(__name__)


class DirectoryPriority(Enum):
    """Priority levels for monitored directories."""

    CRITICAL = 0  # Highest priority, always process immediately
    HIGH = 1  # Process with minimal delay
    NORMAL = 2  # Standard processing
    LOW = 3  # Process when resources available


# Global set to track files being processed to avoid duplicate processing
files_being_processed: set[Path] = set()

# Global set to track directories being monitored
monitored_directories: set[Path] = set()


def is_file_being_processed(file_path: Path) -> bool:
    """Check if a file is currently being processed."""
    return file_path in files_being_processed


def mark_file_as_processing(file_path: Path) -> None:
    """Mark a file as being processed."""
    files_being_processed.add(file_path)


def mark_file_as_done_processing(file_path: Path) -> None:
    """Mark a file as done being processed."""
    if file_path in files_being_processed:
        files_being_processed.remove(file_path)


def is_directory_monitored(dir_path: Path) -> bool:
    """Check if a directory is being monitored."""
    return dir_path in monitored_directories


def add_monitored_directory(dir_path: Path) -> None:
    """Add a directory to the monitored set."""
    monitored_directories.add(dir_path)


def remove_monitored_directory(dir_path: Path) -> None:
    """Remove a directory from the monitored set."""
    if dir_path in monitored_directories:
        monitored_directories.remove(dir_path)


def get_throttle_for_priority(
    priority: DirectoryPriority, base_throttle: float
) -> float:
    """Get the throttle value for a given priority level."""
    if priority == DirectoryPriority.CRITICAL:
        return 0.0  # No throttling for critical directories
    elif priority == DirectoryPriority.HIGH:
        return base_throttle * 0.5  # Half throttling for high priority
    elif priority == DirectoryPriority.NORMAL:
        return base_throttle  # Normal throttling
    else:  # LOW priority
        return base_throttle * 2.0  # Double throttling for low priority



================================================
File: src/the_aichemist_codex/backend/file_manager/directory_manager.py
================================================
import asyncio
import logging
import os
from pathlib import Path

from the_aichemist_codex.backend.rollback.rollback_manager import RollbackManager

logger = logging.getLogger(__name__)
rollback_manager = RollbackManager()


class DirectoryManager:
    """
    Central manager for data directory access.

    This class provides standardized access to project data directories,
    handling creation, validation, and path resolution.
    """

    # Standard directory types
    STANDARD_DIRS = ["cache", "logs", "versions", "exports", "backup", "trash"]

    def __init__(self, base_dir: Path | None = None):
        """
        Initialize with optional base directory override.

        Args:
            base_dir: Optional custom base directory path
        """
        self.base_dir = base_dir or self._get_default_data_dir()
        self._ensure_directories_exist()

    def _get_default_data_dir(self) -> Path:
        """
        Get the default data directory based on env vars or system location.

        Returns:
            Path: The resolved data directory path
        """
        # Check for environment variable override
        if env_dir := os.environ.get("AICHEMIST_DATA_DIR"):
            return Path(env_dir)

        # Use standard OS-specific data directories
        if os.name == "nt":  # Windows
            return Path(os.environ["APPDATA"]) / "AichemistCodex"
        else:  # Linux/Mac
            return Path.home() / ".aichemist"

    def _ensure_directories_exist(self) -> None:
        """Create all required directories if they don't exist."""
        for subdir in self.STANDARD_DIRS:
            (self.base_dir / subdir).mkdir(parents=True, exist_ok=True)

    def get_dir(self, dir_type: str) -> Path:
        """
        Get a specific subdirectory path.

        Args:
            dir_type: Directory type (cache, logs, versions, etc.)

        Returns:
            Path: The resolved directory path

        Raises:
            ValueError: If the directory type is not recognized
        """
        if dir_type not in self.STANDARD_DIRS:
            raise ValueError(f"Unknown directory type: {dir_type}")
        return self.base_dir / dir_type

    def get_file_path(self, filename: str) -> Path:
        """
        Get path for a file in the base data directory.

        Args:
            filename: Name of the file

        Returns:
            Path: The resolved file path
        """
        return self.base_dir / filename

    async def ensure_directory(self, directory: Path):
        """
        Ensures that a directory exists asynchronously and records a rollback for creation.

        Args:
            directory: The directory path to ensure exists
        """
        directory = directory.resolve()

        # Use async existence check
        dir_exists = await asyncio.to_thread(directory.exists)

        if not dir_exists:
            try:
                await asyncio.to_thread(directory.mkdir, parents=True, exist_ok=True)
                logger.info(f"Ensured directory exists: {directory}")
                # Record the creation so that an undo would delete this new directory
                await rollback_manager.record_operation("create", str(directory))
            except Exception as e:
                logger.error(f"Error ensuring directory {directory}: {e}")
        else:
            logger.debug(f"Directory already exists: {directory}")

    async def cleanup_empty_dirs(self, directory: Path):
        """
        Recursively removes empty directories asynchronously and records a rollback for each deletion.

        Args:
            directory: The base directory to clean up
        """
        directory = directory.resolve()

        async def remove_if_empty(subdir: Path):
            # Check if directory exists and is empty
            is_dir = await asyncio.to_thread(subdir.is_dir)
            if not is_dir:
                return

            # Check if directory is empty (convert to list to properly evaluate)
            dir_contents = await asyncio.to_thread(lambda: list(subdir.iterdir()))
            if not dir_contents:
                try:
                    # Record deletion before removing the directory
                    await rollback_manager.record_operation("delete", str(subdir))
                    await asyncio.to_thread(subdir.rmdir)
                    logger.info(f"Removed empty directory: {subdir}")
                except Exception as e:
                    logger.error(f"Failed to remove {subdir}: {e}")

        # Get all subdirectories
        subdirs = await asyncio.to_thread(lambda: list(directory.glob("**/")))

        # Process each subdirectory
        for subdir in sorted(subdirs, reverse=True):  # Process deeper directories first
            await remove_if_empty(subdir)



================================================
File: src/the_aichemist_codex/backend/file_manager/directory_monitor.py
================================================
"""
Enhanced directory monitoring for The Aichemist Codex.

This module provides advanced directory monitoring capabilities including:
- Priority-based monitoring for directories
- Resource throttling for high-change-rate directories
- Recursive directory discovery with customizable depth
- Dynamic directory registration and unregistration
"""

import logging
import threading
from dataclasses import dataclass
from pathlib import Path

from watchdog.observers import Observer

from the_aichemist_codex.backend.config.config_loader import config

# Import from common module instead of file_watcher
from the_aichemist_codex.backend.file_manager.common import DirectoryPriority

logger = logging.getLogger(__name__)


@dataclass
class DirectoryConfig:
    """Configuration for a monitored directory."""

    path: Path
    priority: DirectoryPriority = DirectoryPriority.NORMAL
    recursive_depth: int = 3  # How deep to scan subdirectories (0 = no recursion)
    throttle: float = 1.0  # Seconds to wait between processing in high-activity dirs


class DirectoryMonitor:
    """
    Enhanced directory monitoring with priority-based processing.

    This class provides advanced directory monitoring capabilities with
    configurable priorities, throttling, and recursive depth.
    """

    def __init__(self):
        """Initialize the directory monitor."""
        self.observers = {}  # Dictionary mapping path to Observer
        self.directory_configs = {}  # Dictionary mapping path to DirectoryConfig
        self.discovery_thread = None  # Directory discovery thread
        self.discovery_interval = config.get(
            "file_manager.directory_discovery_interval", 300
        )  # 5 minutes default
        self.discovery_running = False
        self.discovery_lock = threading.Lock()
        self.base_throttle = config.get("file_manager.base_throttle", 1.0)
        self._load_directories_from_config()

    def start(self):
        """Start monitoring all registered directories."""
        logger.info("Starting directory monitoring")
        for dir_path, dir_config in self.directory_configs.items():
            self._start_monitoring_directory(dir_path, dir_config)

        # Start directory discovery if enabled
        if config.get("file_manager.enable_directory_discovery", False):
            self._start_directory_discovery()

    def stop(self):
        """Stop monitoring all directories."""
        logger.info("Stopping directory monitoring")
        # Stop discovery thread
        if self.discovery_thread and self.discovery_thread.is_alive():
            self.discovery_running = False
            self.discovery_thread.join(timeout=5.0)
            if self.discovery_thread.is_alive():
                logger.warning(
                    "Directory discovery thread did not terminate gracefully"
                )

        # Stop all observers
        for observer in self.observers.values():
            if observer.is_alive():
                observer.stop()
                observer.join(timeout=5.0)
                if observer.is_alive():
                    logger.warning(
                        "Observer did not terminate gracefully, some resources may not be released"
                    )

        self.observers.clear()

    def register_directory(
        self,
        dir_path: str | Path,
        priority: DirectoryPriority | str | None = None,
        recursive_depth: int | None = None,
        throttle: float | None = None,
    ) -> bool:
        """
        Register a directory for monitoring.

        Args:
            dir_path: Path to the directory to monitor
            priority: DirectoryPriority enum value or string name
            recursive_depth: How deep to scan subdirectories
            throttle: Seconds to wait between processing in high-activity dirs

        Returns:
            bool: True if registration was successful, False otherwise
        """
        path = Path(dir_path).resolve()
        if not path.exists() or not path.is_dir():
            logger.error(f"Cannot register non-existent directory: {path}")
            return False

        # Convert string priority to enum if needed
        if priority is not None and isinstance(priority, str):
            priority = self._parse_priority(priority)

        # Create directory config
        dir_config = DirectoryConfig(
            path=path,
            priority=priority if priority is not None else DirectoryPriority.NORMAL,
            recursive_depth=recursive_depth if recursive_depth is not None else 3,
            throttle=throttle if throttle is not None else self.base_throttle,
        )

        # Store the config
        str_path = str(path)
        self.directory_configs[str_path] = dir_config

        # Start monitoring if we're already running
        if self.observers:
            self._start_monitoring_directory(str_path, dir_config)

        logger.info(
            f"Registered directory: {path} (Priority: {dir_config.priority.name}, "
            f"Depth: {dir_config.recursive_depth}, Throttle: {dir_config.throttle}s)"
        )
        return True

    def unregister_directory(self, dir_path: str | Path) -> bool:
        """
        Unregister a directory from monitoring.

        Args:
            dir_path: Path to the directory to stop monitoring

        Returns:
            bool: True if unregistration was successful, False otherwise
        """
        path = Path(dir_path).resolve()
        str_path = str(path)

        if str_path not in self.directory_configs:
            logger.warning(f"Directory not registered: {path}")
            return False

        # Stop and remove the observer
        if str_path in self.observers:
            observer = self.observers[str_path]
            if observer.is_alive():
                observer.stop()
                observer.join(timeout=5.0)
                if observer.is_alive():
                    logger.warning(f"Observer for {path} did not terminate gracefully")
            del self.observers[str_path]

        # Remove the config
        del self.directory_configs[str_path]
        logger.info(f"Unregistered directory: {path}")
        return True

    def update_directory_config(self, dir_path: str | Path, **kwargs) -> bool:
        """
        Update configuration for a monitored directory.

        Args:
            dir_path: Path to the directory to update
            **kwargs: Configuration options to update:
                - priority: DirectoryPriority enum value or string name
                - recursive_depth: How deep to scan subdirectories
                - throttle: Seconds to wait between processing in high-activity dirs

        Returns:
            bool: True if update was successful, False otherwise
        """
        path = Path(dir_path).resolve()
        str_path = str(path)

        if str_path not in self.directory_configs:
            logger.warning(f"Cannot update non-registered directory: {path}")
            return False

        # Get current config
        dir_config = self.directory_configs[str_path]

        # Update config values
        if "priority" in kwargs:
            priority = kwargs["priority"]
            if isinstance(priority, str):
                priority = self._parse_priority(priority)
            dir_config.priority = priority

        if "recursive_depth" in kwargs:
            dir_config.recursive_depth = int(kwargs["recursive_depth"])

        if "throttle" in kwargs:
            dir_config.throttle = float(kwargs["throttle"])

        # Restart monitoring with new config if we're already running
        if str_path in self.observers:
            # Stop current observer
            observer = self.observers[str_path]
            if observer.is_alive():
                observer.stop()
                observer.join(timeout=5.0)
                if observer.is_alive():
                    logger.warning(
                        f"Observer for {path} did not terminate gracefully during update"
                    )
            del self.observers[str_path]

            # Start with new config
            self._start_monitoring_directory(str_path, dir_config)

        logger.info(
            f"Updated directory config: {path} (Priority: {dir_config.priority.name}, "
            f"Depth: {dir_config.recursive_depth}, Throttle: {dir_config.throttle}s)"
        )
        return True

    def get_directory_config(self, dir_path: str | Path) -> DirectoryConfig | None:
        """
        Get configuration for a monitored directory.

        Args:
            dir_path: Path to the directory

        Returns:
            DirectoryConfig or None: Configuration for the directory, or None if not registered
        """
        path = Path(dir_path).resolve()
        str_path = str(path)
        return self.directory_configs.get(str_path)

    def get_all_directories(self) -> dict[str, DirectoryConfig]:
        """
        Get all monitored directories and their configurations.

        Returns:
            dict: Dictionary mapping directory paths to their configurations
        """
        return self.directory_configs.copy()

    def _load_directories_from_config(self):
        """Load directories from configuration."""
        dirs_config = config.get("file_manager.monitored_directories", [])
        if not dirs_config:
            logger.info("No directories configured for monitoring")
            return

        for dir_config in dirs_config:
            if not isinstance(dir_config, dict) or "path" not in dir_config:
                logger.warning(f"Invalid directory configuration: {dir_config}")
                continue

            path = dir_config["path"]
            kwargs = {}

            if "priority" in dir_config:
                kwargs["priority"] = dir_config["priority"]

            if "recursive_depth" in dir_config:
                kwargs["recursive_depth"] = int(dir_config["recursive_depth"])

            if "throttle" in dir_config:
                kwargs["throttle"] = float(dir_config["throttle"])

            try:
                self.register_directory(path, **kwargs)
            except Exception as e:
                logger.error(f"Error registering directory {path}: {e}")

    def _start_monitoring_directory(self, dir_path: str, dir_config: DirectoryConfig):
        """
        Start monitoring a directory.

        Args:
            dir_path: String path to the directory
            dir_config: Configuration for the directory
        """
        # Import here to avoid circular import
        from the_aichemist_codex.backend.file_manager.file_watcher import (
            FileEventHandler,
        )

        if dir_path in self.observers and self.observers[dir_path].is_alive():
            logger.warning(f"Directory already being monitored: {dir_path}")
            return

        try:
            # Create observer and event handler
            observer = Observer()
            event_handler = FileEventHandler(dir_config.path)

            # Schedule the observer
            if dir_config.recursive_depth > 0:
                # Handle limited-depth recursion
                self._handle_limited_depth_recursion(
                    dir_config.path,
                    dir_config.recursive_depth,
                    event_handler,
                    observer,
                )
            else:
                # Non-recursive monitoring
                observer.schedule(event_handler, dir_path, recursive=False)

            # Start the observer
            observer.start()
            self.observers[dir_path] = observer

            logger.info(
                f"Started monitoring directory: {dir_path} "
                f"(Priority: {dir_config.priority.name}, Depth: {dir_config.recursive_depth})"
            )
        except Exception as e:
            logger.error(f"Error starting monitoring for {dir_path}: {e}")

    def _handle_limited_depth_recursion(
        self,
        base_path: Path,
        max_depth: int,
        event_handler,
        observer,
    ):
        """
        Handle limited-depth recursion for directory monitoring.

        Args:
            base_path: Base directory path
            max_depth: Maximum recursion depth
            event_handler: Event handler to use
            observer: Observer to schedule
        """
        # Schedule the base directory
        observer.schedule(event_handler, str(base_path), recursive=False)

        # Collect subdirectories up to max_depth
        subdirs = []

        def collect_subdirs(path, current_depth=1):
            if current_depth > max_depth:
                return

            try:
                for item in path.iterdir():
                    if item.is_dir():
                        subdirs.append(item)
                        collect_subdirs(item, current_depth + 1)
            except PermissionError:
                logger.warning(f"Permission denied accessing directory: {path}")
            except Exception as e:
                logger.error(f"Error scanning directory {path}: {e}")

        # Collect subdirectories
        collect_subdirs(base_path)

        # Schedule each subdirectory non-recursively
        for subdir in subdirs:
            try:
                observer.schedule(event_handler, str(subdir), recursive=False)
            except Exception as e:
                logger.error(f"Error scheduling observer for {subdir}: {e}")

    def _start_directory_discovery(self):
        """Start the directory discovery thread."""
        if self.discovery_thread and self.discovery_thread.is_alive():
            logger.warning("Directory discovery already running")
            return

        self.discovery_running = True
        self.discovery_thread = threading.Thread(
            target=self._directory_discovery_loop,
            daemon=True,
            name="DirectoryDiscoveryThread",
        )
        self.discovery_thread.start()
        logger.info("Started directory discovery thread")

    def _directory_discovery_loop(self):
        """Run the directory discovery loop."""
        while self.discovery_running:
            try:
                self._discover_new_directories()
            except Exception as e:
                logger.error(f"Error in directory discovery: {e}")

            # Sleep for the discovery interval
            for _ in range(int(self.discovery_interval)):
                if not self.discovery_running:
                    break
                threading.Event().wait(1)

    def _discover_new_directories(self):
        """Discover new directories to monitor."""
        with self.discovery_lock:
            # Get base directories to scan
            base_dirs = config.get("file_manager.discovery_base_directories", [])
            if not base_dirs:
                return

            # Get discovery depth
            max_depth = config.get("file_manager.discovery_max_depth", 3)

            # Get patterns to include/exclude
            include_patterns = config.get("file_manager.discovery_include_patterns", [])
            exclude_patterns = config.get("file_manager.discovery_exclude_patterns", [])

            # Scan directories
            new_dirs = set()

            def scan_dir(path, current_depth=1):
                if current_depth > max_depth:
                    return

                try:
                    for item in Path(path).iterdir():
                        if not item.is_dir():
                            continue

                        # Check if this directory should be included
                        dir_name = item.name.lower()
                        if include_patterns and not any(
                            pattern.lower() in dir_name for pattern in include_patterns
                        ):
                            continue

                        # Check if this directory should be excluded
                        if any(
                            pattern.lower() in dir_name for pattern in exclude_patterns
                        ):
                            continue

                        # Add to new directories
                        new_dirs.add(item)

                        # Scan subdirectories
                        scan_dir(item, current_depth + 1)
                except Exception as e:
                    logger.error(f"Error scanning directory {path}: {e}")

            # Scan all base directories
            for base_dir in base_dirs:
                try:
                    base_path = Path(base_dir)
                    if base_path.exists() and base_path.is_dir():
                        scan_dir(base_path)
                except Exception as e:
                    logger.error(f"Error scanning base directory {base_dir}: {e}")

            # Register new directories
            for new_dir in new_dirs:
                str_path = str(new_dir)
                if str_path not in self.directory_configs:
                    self.register_directory(new_dir)

    def _parse_priority(self, priority_str: str) -> DirectoryPriority:
        """
        Parse a priority string into a DirectoryPriority enum value.

        Args:
            priority_str: Priority string (case-insensitive)

        Returns:
            DirectoryPriority: Corresponding enum value
        """
        priority_map = {
            "critical": DirectoryPriority.CRITICAL,
            "high": DirectoryPriority.HIGH,
            "normal": DirectoryPriority.NORMAL,
            "low": DirectoryPriority.LOW,
        }

        priority_str = priority_str.lower()
        if priority_str in priority_map:
            return priority_map[priority_str]
        else:
            logger.warning(f"Unknown priority: {priority_str}, defaulting to NORMAL")
            return DirectoryPriority.NORMAL


# Create a singleton instance
directory_monitor = DirectoryMonitor()



================================================
File: src/the_aichemist_codex/backend/file_manager/duplicate_detector.py
================================================
import hashlib
import logging
from pathlib import Path

from the_aichemist_codex.backend.utils.async_io import AsyncFileIO

logger = logging.getLogger(__name__)


class DuplicateDetector:
    def __init__(self):
        self.hashes = {}  # Maps file hash to a list of file paths.

    async def compute_hash(self, file_path: Path, hash_algo="md5") -> str:
        h = hashlib.new(hash_algo)
        try:
            data = await AsyncFileIO.read_binary(file_path)
            h.update(data)
            return h.hexdigest()
        except Exception as e:
            logger.error(f"Error computing hash for {file_path}: {e}")
            return ""

    async def scan_directory(
        self, directory: Path, method="md5"
    ) -> dict[str, list[Path]]:
        """
        Scan a directory for duplicate files.

        Args:
            directory: Path to the directory to scan
            method: Either a specific hash algorithm (md5, sha1, sha256) or
                    a method name (hash, name, content)

        Returns:
            Dictionary mapping hash values to lists of file paths
        """
        # Translate method name to actual hash algorithm if needed
        hash_algo = method
        if method == "hash":
            # Default to md5 when "hash" is specified as the method
            hash_algo = "md5"
        elif method == "name" or method == "content":
            # Handle other methods (these would need their own implementation)
            logger.warning(f"Method '{method}' not fully implemented; using md5 hash")
            hash_algo = "md5"

        for file in directory.rglob("*"):
            if file.is_file():
                file_hash = await self.compute_hash(file, hash_algo)
                if file_hash:
                    if file_hash in self.hashes:
                        self.hashes[file_hash].append(file)
                    else:
                        self.hashes[file_hash] = [file]
        return self.hashes

    def get_duplicates(self):
        # Return only the hashes with more than one file.
        return {h: files for h, files in self.hashes.items() if len(files) > 1}



================================================
File: src/the_aichemist_codex/backend/file_manager/file_mover.py
================================================
import asyncio
import datetime
import hashlib
import logging
import os
from pathlib import Path

from the_aichemist_codex.backend.config.rules_engine import rules_engine
from the_aichemist_codex.backend.rollback.rollback_manager import RollbackManager
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
from the_aichemist_codex.backend.utils.safety import SafeFileHandler

from .directory_manager import DirectoryManager

logger = logging.getLogger(__name__)
rollback_manager = RollbackManager()


# Function to get data directory without depending on settings.py
def get_data_dir() -> Path:
    """
    Get the data directory without creating circular imports.

    Returns:
        Path: The data directory
    """
    # Check environment variable first
    env_data_dir = os.environ.get("AICHEMIST_DATA_DIR")
    if env_data_dir:
        return Path(env_data_dir)

    # Fall back to a directory relative to the project root
    return Path(__file__).resolve().parents[3] / "data"


# Create a DirectoryManager instance for use across the module
directory_manager = DirectoryManager(get_data_dir())


class FileMover:
    def __init__(self, base_directory: Path):
        self.base_directory = base_directory
        self.safe_scanner = SafeFileHandler

    @staticmethod
    async def get_file_hash(file_path: Path) -> str:
        """Calculate SHA-256 hash of a file using async I/O."""
        try:
            hasher = hashlib.sha256()
            # Use AsyncFileIO for reading the file in chunks
            async for chunk in AsyncFileIO.read_chunked(file_path, chunk_size=4096):
                hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating hash for {file_path}: {e}")
            return ""

    @staticmethod
    async def verify_file_copy(source: Path, destination: Path) -> bool:
        """Verify that the destination file exists and has the same size and hash as the source."""
        try:
            if not await AsyncFileIO.exists(destination):
                logger.error(f"Destination file does not exist: {destination}")
                return False

            # Check file sizes match
            source_size = await AsyncFileIO.get_file_size(source)
            dest_size = await AsyncFileIO.get_file_size(destination)

            if source_size is None or dest_size is None:
                logger.error(
                    f"Could not determine file size for {source} or {destination}"
                )
                return False

            if source_size != dest_size:
                logger.error(
                    f"File size mismatch: {source} ({source_size} bytes) -> {destination} ({dest_size} bytes)"
                )
                return False

            # For small files (< 10MB), also verify contents with hash
            if source_size < 10_000_000:
                source_hash = await FileMover.get_file_hash(source)
                dest_hash = await FileMover.get_file_hash(destination)
                if source_hash != dest_hash:
                    logger.error(f"File hash mismatch: {source} -> {destination}")
                    return False

            return True
        except Exception as e:
            logger.error(f"Error verifying file copy {source} -> {destination}: {e}")
            return False

    @staticmethod
    async def safe_remove_file(file_path: Path) -> bool:
        """Safely remove a file using AsyncFileIO."""
        try:
            # Instead of using os.remove directly, use a helper method from AsyncFileIO
            # Since AsyncFileIO doesn't have a direct remove method, we'll simulate it
            # by deleting the content and then using asyncio to remove the file
            await asyncio.to_thread(lambda: file_path.unlink(missing_ok=True))
            logger.debug(f"Removed file: {file_path}")
            return True
        except Exception as e:
            logger.error(f"Error removing file {file_path}: {e}")
            return False

    @staticmethod
    async def move_file(source: Path, destination: Path):
        """Move a file from source to destination using AsyncFileIO operations."""
        if SafeFileHandler.should_ignore(source):
            logger.info(f"Skipping ignored file: {source}")
            return

        # Create a backup BEFORE attempting any move
        try:
            # Use DATA_DIR from settings for backup location
            backup_dir = get_data_dir() / "backup/file_backups"
            backup_dir.mkdir(parents=True, exist_ok=True)

            # Create timestamped backup
            timestamp = int(datetime.datetime.now().timestamp())
            backup_path = backup_dir / f"{source.name}.{timestamp}.bak"

            # Perform backup
            await AsyncFileIO.copy(source, backup_path)
            logger.info(f"Created backup of {source} at {backup_path}")
        except Exception as backup_error:
            logger.error(f"Failed to create backup of {source}: {backup_error}")
            # Even if backup fails, we'll continue with the move

        # Track success for rollback operations
        move_successful = False

        try:
            # Ensure destination directory exists asynchronously
            await directory_manager.ensure_directory(destination.parent)

            # Check if destination already exists
            if await AsyncFileIO.exists(destination):
                logger.warning(f"Destination file already exists: {destination}")
                # Generate a unique name by adding a timestamp
                new_name = f"{destination.stem}_{int(datetime.datetime.now().timestamp())}{destination.suffix}"
                destination = destination.parent / new_name
                logger.info(f"Using alternative destination: {destination}")

            # For large files, use chunked copy operation
            file_size = await AsyncFileIO.get_file_size(source)
            if file_size and file_size > 10_000_000:  # 10MB threshold
                logger.info(
                    f"Using chunked copy for large file: {source} ({file_size} bytes)"
                )
                copy_success = await AsyncFileIO.copy_chunked(source, destination)
            else:
                # Use regular copy for smaller files
                copy_success = await AsyncFileIO.copy(source, destination)

            if copy_success:
                # Verify the file was copied correctly
                verification = await FileMover.verify_file_copy(source, destination)

                if verification:
                    logger.info(f"Verified copy successful: {source} -> {destination}")

                    # Remove original file only after successful copy and verification
                    removal_success = await FileMover.safe_remove_file(source)

                    if removal_success:
                        logger.info(f"Moved {source} -> {destination}")
                        move_successful = True
                    else:
                        logger.error(
                            f"Failed to remove source file after copying: {source}"
                        )
                else:
                    logger.error(
                        f"File verification failed, cancelling move operation: {source} -> {destination}"
                    )
                    # Try to clean up the potentially corrupted destination file
                    try:
                        if await AsyncFileIO.exists(destination):
                            await FileMover.safe_remove_file(destination)
                    except Exception as cleanup_error:
                        logger.error(
                            f"Error cleaning up destination file: {cleanup_error}"
                        )
            else:
                logger.error(f"Failed to copy {source} to {destination}")
        except Exception as e:
            logger.error(f"Error moving {source}: {e}")

        # ALWAYS record the operation, even if it failed
        try:
            # Use await to properly wait for the coroutine
            await rollback_manager.record_operation(
                "move", str(source), str(destination)
            )
        except Exception as rollback_error:
            logger.error(f"Error recording rollback operation: {rollback_error}")

    async def apply_rules(self, file_path: Path):
        """
        Apply organization rules to a file.

        Args:
            file_path: Path to the file to be organized

        Returns:
            bool: True if a rule was applied, False otherwise
        """
        # Ensure we're using an absolute, resolved path
        file_path = file_path.resolve()
        logger.info(f"Applying rules to: {file_path}")

        # Skip if the file no longer exists
        if not await AsyncFileIO.exists(file_path):
            logger.warning(f"File no longer exists: {file_path}")
            return False

        for rule in rules_engine.rules:
            # Check if the file matches rule extensions
            if any(
                file_path.suffix.lower() == ext.lower()
                for ext in rule.get("extensions", [])
            ):
                # Build target directory path
                target_dir = Path(rule["target_dir"])
                if not target_dir.is_absolute():
                    target_dir = self.base_directory / target_dir
                target_dir = target_dir.resolve()

                logger.info(
                    f"Rule matched for {file_path}. Target directory: {target_dir}"
                )

                # Ensure the target directory exists
                await directory_manager.ensure_directory(target_dir)

                # Move the file
                target_file = target_dir / file_path.name
                logger.info(f"Moving file to {target_file}")
                await FileMover.move_file(file_path, target_file)
                return True

        logger.info(f"No rules matched for {file_path}")
        return False

    async def auto_folder_structure(self, file_path: Path):
        """
        Create an automatic folder structure based on file type and date.

        Args:
            file_path: Path to the file to organize

        Returns:
            Path: Path to the target directory
        """
        # Ensure we're using an absolute, resolved path
        file_path = file_path.resolve()
        logger.info(f"Creating auto folder structure for: {file_path}")

        # Skip if the file no longer exists
        if not await AsyncFileIO.exists(file_path):
            logger.warning(f"File no longer exists: {file_path}")
            return self.base_directory / "organized" / "unknown"

        # Organize by file extension and creation date (YYYY-MM)
        ext = file_path.suffix.lower().lstrip(".")
        if not ext:
            ext = "no_extension"

        # Get file creation time
        try:
            # Use AsyncFileIO where possible
            file_stats = await asyncio.to_thread(file_path.stat)
            creation_time = file_stats.st_ctime
            dt = datetime.datetime.fromtimestamp(creation_time)
            date_folder = dt.strftime("%Y-%m")
        except Exception as e:
            logger.warning(f"Error getting creation time for {file_path}: {e}")
            date_folder = "unknown_date"

        # Create path structure
        target_dir = (self.base_directory / "organized" / ext / date_folder).resolve()
        logger.info(f"Auto-organization target directory: {target_dir}")

        # Ensure the directory exists
        await directory_manager.ensure_directory(target_dir)

        return target_dir



================================================
File: src/the_aichemist_codex/backend/file_manager/file_tree.py
================================================
"""Generates and manages file tree representations."""

import logging
import os
from pathlib import Path

from the_aichemist_codex.backend.utils.cache_manager import cache_manager
from the_aichemist_codex.backend.utils.safety import SafeFileHandler

logger = logging.getLogger(__name__)


class FileTreeGenerator:
    """Class to generate and manage file trees."""

    async def generate(
        self,
        directory_path: Path,
        max_depth: int = 10,
        use_cache: bool = True,
        cache_ttl: int = 300,  # 5 minutes cache TTL
    ) -> dict:
        """
        Generate a hierarchical representation of files and directories.

        Args:
            directory_path: Root directory to process
            max_depth: Maximum directory depth to traverse
            use_cache: Whether to use caching
            cache_ttl: Cache time-to-live in seconds

        Returns:
            Dict representation of the file tree
        """
        return await generate_file_tree(directory_path, max_depth, use_cache, cache_ttl)


async def generate_file_tree(
    directory_path: Path,
    max_depth: int = 10,
    use_cache: bool = True,
    cache_ttl: int = 300,  # 5 minutes cache TTL
) -> dict:
    """
    Generate a hierarchical representation of files and directories.

    Args:
        directory_path: Root directory to process
        max_depth: Maximum directory depth to traverse
        use_cache: Whether to use caching
        cache_ttl: Cache time-to-live in seconds

    Returns:
        Dict representation of the file tree
    """
    # Check cache first if enabled
    if use_cache:
        cache_key = f"file_tree_{str(directory_path)}_{max_depth}"
        cached_tree = await cache_manager.get(cache_key)
        if cached_tree:
            return cached_tree

    async def process_directory(path: Path, current_depth: int) -> dict:
        """Process a directory and its contents recursively."""
        if current_depth > max_depth:
            return {"type": "directory", "truncated": True}

        try:
            result = {}

            # List directory contents
            try:
                entries = os.listdir(path)
            except PermissionError:
                logger.warning(f"Permission denied accessing directory: {path}")
                return {"type": "directory", "error": "permission_denied"}
            except Exception as e:
                logger.error(f"Error accessing directory {path}: {e}")
                return {"type": "directory", "error": str(e)}

            for entry in entries:
                entry_path = path / entry

                # Skip ignored files/directories
                if SafeFileHandler.should_ignore(entry_path):
                    continue

                try:
                    if entry_path.is_file():
                        # Get file metadata
                        try:
                            stats = entry_path.stat()
                            result[entry] = {
                                "type": "file",
                                "size": stats.st_size,
                                "modified": stats.st_mtime,
                                "created": stats.st_ctime,
                            }
                        except Exception as e:
                            logger.error(
                                f"Error getting file stats for {entry_path}: {e}"
                            )
                            result[entry] = {"type": "file", "error": str(e)}
                    elif entry_path.is_dir():
                        # Process subdirectory
                        result[entry] = await process_directory(
                            entry_path, current_depth + 1
                        )
                except Exception as e:
                    logger.error(f"Error processing {entry_path}: {e}")
                    result[entry] = {"type": "unknown", "error": str(e)}

            return result
        except Exception as e:
            logger.error(f"Error processing directory {path}: {e}")
            return {"type": "directory", "error": str(e)}

    # Generate the file tree
    try:
        tree = await process_directory(directory_path, 0)

        # Cache the result if caching is enabled
        if use_cache:
            cache_key = f"file_tree_{str(directory_path)}_{max_depth}"
            await cache_manager.put(cache_key, tree)

        return tree
    except Exception as e:
        logger.error(f"Error generating file tree for {directory_path}: {e}")
        return {"type": "directory", "error": str(e)}


async def invalidate_file_tree_cache(directory_path: Path) -> None:
    """
    Invalidate the file tree cache for a directory.

    Args:
        directory_path: Directory whose cache should be invalidated
    """
    try:
        await cache_manager.invalidate_pattern(f"file_tree_{str(directory_path)}")
    except Exception as e:
        logger.error(f"Error invalidating file tree cache for {directory_path}: {e}")


async def get_file_tree_stats(directory_path: Path) -> dict:
    """
    Get statistics about a file tree.

    Args:
        directory_path: Root directory to analyze

    Returns:
        Dictionary containing file tree statistics
    """
    stats = {
        "total_files": 0,
        "total_dirs": 0,
        "total_size": 0,
        "max_depth": 0,
        "errors": [],
    }

    async def analyze_directory(path: Path, current_depth: int) -> None:
        nonlocal stats

        try:
            entries = os.listdir(path)

            for entry in entries:
                entry_path = path / entry

                if SafeFileHandler.should_ignore(entry_path):
                    continue

                try:
                    if entry_path.is_file():
                        stats["total_files"] += 1
                        stats["total_size"] += entry_path.stat().st_size
                    elif entry_path.is_dir():
                        stats["total_dirs"] += 1
                        stats["max_depth"] = max(stats["max_depth"], current_depth + 1)
                        await analyze_directory(entry_path, current_depth + 1)
                except Exception as e:
                    stats["errors"].append(str(e))
        except Exception as e:
            stats["errors"].append(str(e))

    await analyze_directory(directory_path, 0)
    return stats



================================================
File: src/the_aichemist_codex/backend/file_manager/file_watcher.py
================================================
"""File system monitoring and event handling for The Aichemist Codex."""

import asyncio
import logging
import threading
import time
from pathlib import Path
from typing import NoReturn

from watchdog.events import FileSystemEvent, FileSystemEventHandler

from the_aichemist_codex.backend.config.config_loader import config
from the_aichemist_codex.backend.file_manager.change_detector import ChangeDetector
from the_aichemist_codex.backend.file_manager.change_history_manager import (
    change_history_manager,
)
from the_aichemist_codex.backend.file_manager.common import (
    is_file_being_processed,
    mark_file_as_done_processing,
    mark_file_as_processing,
)
from the_aichemist_codex.backend.file_manager.file_mover import FileMover
from the_aichemist_codex.backend.file_manager.sorter import RuleBasedSorter
from the_aichemist_codex.backend.file_manager.version_manager import version_manager
from the_aichemist_codex.backend.rollback.rollback_manager import RollbackManager

logger = logging.getLogger(__name__)
rollback_manager = RollbackManager()
change_detector = ChangeDetector()


class FileEventHandler(FileSystemEventHandler):
    def __init__(self, base_directory: Path) -> None:
        self.base_directory = base_directory.resolve()
        self.file_mover = FileMover(self.base_directory)
        self.sorter = RuleBasedSorter(self.base_directory)
        self.debounce_interval = config.get("file_manager.debounce_interval", 1)
        self.processing_lock = threading.Lock()
        self.last_processed = {}

    def on_created(self, event: FileSystemEvent) -> None:
        """
        Handle file creation events.

        Args:
            event: File system event
        """
        if event.is_directory:
            return

        file_path = Path(event.src_path).resolve()
        logger.debug(f"File created: {file_path}")

        # Skip temporary files
        if self._is_temporary_file(file_path):
            return

        # Check if this is a new file or one we're tracking
        if not change_detector.is_tracked_file(file_path):
            change_detector.add_file(file_path)
            change_history_manager.record_creation(file_path)

        # Process the file
        self.process_file(file_path)

    def on_modified(self, event: FileSystemEvent) -> None:
        """
        Handle file modification events.

        Args:
            event: File system event
        """
        if event.is_directory:
            return

        file_path = Path(event.src_path).resolve()

        # Skip temporary files
        if self._is_temporary_file(file_path):
            return

        # Debounce rapid modifications
        current_time = time.time()
        if file_path in self.last_processed:
            time_diff = current_time - self.last_processed[file_path]
            if time_diff < self.debounce_interval:
                return

        self.last_processed[file_path] = current_time
        logger.debug(f"File modified: {file_path}")

        # Record the change
        if change_detector.is_tracked_file(file_path):
            change_history_manager.record_modification(file_path)

        # Process the file
        self.process_file(file_path)

    def on_deleted(self, event: FileSystemEvent) -> None:
        """
        Handle file deletion events.

        Args:
            event: File system event
        """
        if event.is_directory:
            return

        file_path = Path(event.src_path).resolve()
        logger.debug(f"File deleted: {file_path}")

        # Record the deletion
        if change_detector.is_tracked_file(file_path):
            change_history_manager.record_deletion(file_path)
            change_detector.remove_file(file_path)

    def on_moved(self, event: FileSystemEvent) -> None:
        """
        Handle file move events.

        Args:
            event: File system event
        """
        if event.is_directory:
            return

        src_path = Path(event.src_path).resolve()
        dest_path = Path(event.dest_path).resolve()
        logger.debug(f"File moved: {src_path} -> {dest_path}")

        # Skip temporary files
        if self._is_temporary_file(src_path) or self._is_temporary_file(dest_path):
            return

        # Record the move
        if change_detector.is_tracked_file(src_path):
            change_history_manager.record_move(src_path, dest_path)
            change_detector.update_file_path(src_path, dest_path)

        # Process the destination file
        self.process_file(dest_path)

    async def _analyze_change(self, file_path: Path) -> None:
        """
        Analyze a file change asynchronously.

        Args:
            file_path: Path to the file
        """
        try:
            # Check if file still exists
            if not file_path.exists():
                logger.debug(f"File no longer exists: {file_path}")
                return

            # Create a version snapshot
            version_manager.create_version(file_path)

            # Apply rules based on file type and content
            await self.sorter.apply_rules_async(file_path)

            # Update metadata
            # This would typically involve extracting and storing metadata
            # about the file, such as MIME type, creation date, etc.
            # For now, we'll just log that we're doing it
            logger.debug(f"Updating metadata for {file_path}")

            # Check for duplicates
            # This would involve comparing the file against known files
            # to identify potential duplicates
            # For now, we'll just log that we're doing it
            logger.debug(f"Checking for duplicates of {file_path}")

        except Exception as e:
            logger.error(f"Error analyzing change for {file_path}: {e}")
            # Create rollback point in case of error
            rollback_manager.create_rollback_point(file_path)

    def process_file(self, file_path: Path) -> None:
        """
        Process a file after a change event.

        Args:
            file_path: Path to the file
        """
        # Skip if file is already being processed
        if is_file_being_processed(file_path):
            logger.debug(f"File already being processed: {file_path}")
            return

        # Mark file as being processed
        with self.processing_lock:
            mark_file_as_processing(file_path)

        try:
            # Run async analysis in a separate thread
            asyncio.run(self._analyze_change(file_path))
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {e}")
        finally:
            # Mark file as done processing
            with self.processing_lock:
                mark_file_as_done_processing(file_path)

    def _is_temporary_file(self, file_path: Path) -> bool:
        """
        Check if a file is a temporary file.

        Args:
            file_path: Path to the file

        Returns:
            bool: True if the file is temporary, False otherwise
        """
        # Check file name patterns that indicate temporary files
        name = file_path.name.lower()
        return (
            name.startswith("~")
            or name.startswith(".")
            or name.endswith(".tmp")
            or name.endswith(".temp")
            or ".tmp." in name
            or "._mp_" in name
        )


def monitor_directory() -> None:
    """Start monitoring the configured directories."""
    # This function is called from the main application to start monitoring
    # We'll use the directory_monitor singleton from the module that imports this
    # This avoids the circular import
    logger.info("Directory monitoring started")


def scheduled_sorting() -> NoReturn:
    """Run scheduled sorting operations in the background."""
    logger.info("Starting scheduled sorting thread")
    sorter = RuleBasedSorter(Path(config.get("base_directory", ".")))

    while True:
        try:
            # Run sorting operations
            logger.debug("Running scheduled sorting")
            # This would typically involve applying rules to files
            # based on a schedule, rather than in response to events
            # For now, we'll just log that we're doing it
            logger.debug("Scheduled sorting complete")
        except Exception as e:
            logger.error(f"Error in scheduled sorting: {e}")

        # Sleep until next run
        time.sleep(config.get("scheduled_sorting_interval", 3600))  # Default: 1 hour



================================================
File: src/the_aichemist_codex/backend/file_manager/fix_circular_imports.py
================================================
#!/usr/bin/env python
"""
Script to fix circular import issues in the file_manager package.

This script analyzes the imports in the file_manager package and suggests
changes to fix circular dependencies.
"""

import os
import re
import sys
from pathlib import Path


def analyze_imports(file_path):
    """Analyze imports in the given file."""
    with open(file_path, encoding="utf-8") as f:
        content = f.read()

    # Find all import statements
    import_pattern = r"(?:from\s+([.\w]+)\s+import\s+([^#\n]+)|import\s+([^#\n]+))"
    imports = re.findall(import_pattern, content)

    # Process imports
    result = []
    for from_module, from_imports, direct_imports in imports:
        if from_module:
            # Handle 'from X import Y' statements
            imports_list = [imp.strip() for imp in from_imports.split(",")]
            for imp in imports_list:
                result.append((from_module, imp))
        else:
            # Handle 'import X' statements
            imports_list = [imp.strip() for imp in direct_imports.split(",")]
            for imp in imports_list:
                result.append((imp, None))

    return result


def find_circular_imports(base_dir):
    """Find circular imports in the given directory."""
    # Get all Python files in the directory
    py_files = []
    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".py"):
                py_files.append(os.path.join(root, file))

    # Analyze imports in each file
    file_imports = {}
    for file_path in py_files:
        rel_path = os.path.relpath(file_path, base_dir)
        module_name = os.path.splitext(rel_path)[0].replace(os.path.sep, ".")
        file_imports[module_name] = analyze_imports(file_path)

    # Find circular dependencies
    circular_deps = []
    for module, imports in file_imports.items():
        for from_module, from_import in imports:
            if from_module in file_imports:
                # Check if the imported module imports back
                for back_from, back_import in file_imports[from_module]:
                    if back_from == module:
                        circular_deps.append(
                            (module, from_module, from_import, back_import)
                        )

    return circular_deps, file_imports


def suggest_fixes(circular_deps, file_imports):
    """Suggest fixes for circular dependencies."""
    suggestions = []

    for module, from_module, from_import, back_import in circular_deps:
        # Determine which import is more critical
        module_short = module.split(".")[-1]
        from_module_short = from_module.split(".")[-1]

        suggestion = f"Circular dependency between {module} and {from_module}:\n"
        suggestion += f"  - {module} imports {from_import} from {from_module}\n"
        suggestion += f"  - {from_module} imports {back_import} from {module}\n\n"

        suggestion += "Suggested fixes:\n"
        suggestion += f"1. Move shared functionality to a new module (e.g., '{module_short}_{from_module_short}_common.py')\n"
        suggestion += f"2. Use lazy imports in {module} or {from_module}:\n"
        suggestion += "   ```python\n"
        suggestion += "   def some_function():\n"
        suggestion += f"       from {from_module} import {from_import}  # Import inside function\n"
        suggestion += "       # Use the imported module\n"
        suggestion += "   ```\n"
        suggestion += "3. Use type hints with quotes for forward references:\n"
        suggestion += "   ```python\n"
        suggestion += (
            f"   def process_item(item: '{from_module_short}.{from_import}') -> None:\n"
        )
        suggestion += "       # Process the item\n"
        suggestion += "   ```\n"

        suggestions.append(suggestion)

    return suggestions


def main():
    """Main entry point."""
    # Get the base directory
    base_dir = Path(__file__).parent

    print(f"Analyzing imports in {base_dir}...")

    # Find circular imports
    circular_deps, file_imports = find_circular_imports(base_dir)

    if not circular_deps:
        print("No circular dependencies found!")
        return 0

    print(f"Found {len(circular_deps)} circular dependencies:")
    for module, from_module, from_import, back_import in circular_deps:
        print(f"  - {module} <-> {from_module}")

    print("\nSuggested fixes:")
    suggestions = suggest_fixes(circular_deps, file_imports)
    for i, suggestion in enumerate(suggestions, 1):
        print(f"\nSuggestion {i}:")
        print(suggestion)

    return 0


if __name__ == "__main__":
    sys.exit(main())



================================================
File: src/the_aichemist_codex/backend/file_manager/sorter.py
================================================
import asyncio
import fnmatch
import logging
from datetime import datetime
from pathlib import Path
from typing import Any

import yaml

from the_aichemist_codex.backend.file_manager.directory_manager import DirectoryManager
from the_aichemist_codex.backend.file_manager.file_mover import FileMover
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO

logger = logging.getLogger(__name__)


class RuleBasedSorter:
    """
    * RuleBasedSorter
    This class provides functionality to sort files based on a set of user-defined rules
    specified in a YAML file. It supports both basic matching (file name patterns, extensions)
    and extended matching (metadata filters, content keywords).

    Attributes:
        rules (list): A list of dictionaries, each representing a sorting rule.
    """

    def __init__(self, config_file: Path | None = None) -> None:
        """
        * Initializes the sorter instance.

        ! By default, 'rules' is set to None. The actual rules are loaded asynchronously
        the first time 'sort_directory' is called, via 'load_rules'.

        Args:
            config_file (Optional[Path], optional): Path to a custom sorting rules YAML file.
                                          If None, the default rules file will be used.
        """
        self.config_file = config_file
        self.rules: list[dict[str, Any]] | None = None
        # Get the data directory for FileMover
        data_dir = Path(__file__).resolve().parents[3] / "data"
        self.file_mover = FileMover(base_directory=data_dir)
        self.directory_manager = DirectoryManager(base_dir=None)
        self.pattern_cache: dict[str, bool] = {}

    async def load_rules(self) -> list[dict[str, Any]]:
        """
        Loads the sorting rules from 'sorting_rules.yaml' in the config directory
        or from a custom config file if provided.

        * If the file is not found, it logs a warning and returns an empty list.
        * If the file is found, it attempts to parse the 'rules' key from the YAML.

        Returns:
            List[Dict[str, Any]]: A list of rule dictionaries loaded from the YAML file.
        """
        # Set default config file path
        config_dir = Path(__file__).resolve().parent.parent / "config"
        rules_file = config_dir / "sorting_rules.yaml"

        # Use custom config file if provided
        if self.config_file is not None:
            rules_file = self.config_file

        if not await AsyncFileIO.exists(rules_file):
            # ! Logging a warning because the rules file is missing.
            logger.warning(
                f"Sorting rules file not found at {rules_file}. No rules loaded."
            )
            return []
        try:
            content = await AsyncFileIO.read_text(rules_file)
            rules_data = yaml.safe_load(content)
            logger.info(
                f"Loaded {len(rules_data.get('rules', []))} "
                f"sorting rules from {rules_file}"
            )
            return rules_data.get("rules", [])
        except Exception as e:
            # ? Catching any parsing or IO errors to prevent crash.
            logger.error(f"Error loading sorting rules: {e}")
            return []

    def rule_matches(self, file_path: Path, rule: dict) -> bool:
        """
        Checks if a file matches the basic criteria of a given rule:
        name patterns and file extensions.

        * This method is synchronous and only checks for the 'pattern' and 'extensions' keys.

        Args:
            file_path (Path): The path of the file to check.
            rule (dict): A dictionary defining the sorting rule.

        Returns:
            bool: True if the file matches the pattern and/or extensions in the rule,
                  False otherwise.
        """
        pattern = rule.get("pattern")
        if pattern and not fnmatch.fnmatch(file_path.name, pattern):
            return False

        extensions = rule.get("extensions")
        if extensions and file_path.suffix.lower() not in [
            ext.lower() for ext in extensions
        ]:
            return False

        return True

    async def rule_matches_extended(self, file_path: Path, rule: dict) -> bool:
        """
        Asynchronously checks if a file matches extended criteria defined in a rule.

        * First, calls 'rule_matches' for basic pattern/extension checks.
        * Then, checks file size limits (min_size, max_size).
        * Next, checks file creation timestamps (created_after, created_before).
        * Finally, searches for any keywords in the file's text content.

        Args:
            file_path (Path): The path of the file to check.
            rule (dict): A dictionary defining the sorting rule.

        Returns:
            bool: True if the file meets all criteria, False otherwise.
        """
        # ? Start with the basic rule matching.
        if not self.rule_matches(file_path, rule):
            return False

        try:
            stat = file_path.stat()
        except Exception as e:
            logger.error(f"Error getting stat for {file_path}: {e}")
            return False

        # * File size checks
        if "min_size" in rule:
            if stat.st_size < rule["min_size"]:
                return False
        if "max_size" in rule:
            if stat.st_size > rule["max_size"]:
                return False

        # * File creation timestamp checks
        if "created_after" in rule:
            try:
                after = datetime.fromisoformat(rule["created_after"])
                file_creation = datetime.fromtimestamp(stat.st_ctime)
                if file_creation < after:
                    return False
            except Exception as e:
                logger.error(f"Error parsing created_after for rule {rule}: {e}")
                return False

        if "created_before" in rule:
            try:
                before = datetime.fromisoformat(rule["created_before"])
                file_creation = datetime.fromtimestamp(stat.st_ctime)
                if file_creation > before:
                    return False
            except Exception as e:
                logger.error(f"Error parsing created_before for rule {rule}: {e}")
                return False

        # * Content keyword checks
        if "keywords" in rule:
            try:
                # ! Dynamically importing FileReader to avoid circular dependencies.
                from the_aichemist_codex.backend.file_reader.file_reader import (
                    FileReader,
                )

                reader = FileReader()
                metadata = await reader.process_file(file_path)
                content = metadata.preview
                # TODO: If a rule has many keywords, consider using more efficient search logic.
                for keyword in rule["keywords"]:
                    if keyword.lower() not in content.lower():
                        return False
            except Exception as e:
                logger.error(f"Error checking keywords for {file_path}: {e}")
                return False

        return True

    async def sort_directory(self, directory: Path):
        """
        Sorts all files within a directory (recursively) according to the loaded rules.

        * If 'self.rules' is None, it calls 'load_rules' first.
        * For each file, it iterates through the rules. The first matching rule
          triggers the file to be moved to the specified 'target_dir'.

        Args:
            directory (Path): The directory to sort.
        """
        if self.rules is None:
            # ? Load the rules once if not already loaded.
            self.rules = await self.load_rules()

        if not self.rules:
            logger.warning("No sorting rules found. No files will be moved.")
            return

        # * Walk through each file in the directory tree.
        moved_count = 0
        skipped_count = 0

        for file in directory.rglob("*"):
            if file.is_file():
                # ? Check each rule in turn until one matches.
                for rule in self.rules:
                    if await self.rule_matches_extended(file, rule):
                        target_dir_str = rule.get("target_dir", "")
                        if not target_dir_str:
                            logger.error(f"Missing target_dir in rule: {rule}")
                            continue

                        target_dir = Path(target_dir_str)
                        if not target_dir.is_absolute():
                            target_dir = directory / target_dir

                        # ! Skip if the file is already in the target directory.
                        if file.parent == target_dir:
                            skipped_count += 1
                            continue

                        # Add safety check: ensure target_dir exists and is a valid directory
                        if not target_dir.exists():
                            await self.directory_manager.ensure_directory(target_dir)
                        elif not target_dir.is_dir():
                            logger.error(
                                f"Target path {target_dir} is not a directory. Skipping rule for {file}"
                            )
                            continue

                        logger.info(f"Applying rule {rule} to file {file}")

                        # Handle preserve_path flag if present in the rule
                        if rule.get("preserve_path", False):
                            # Get relative path from the base directory
                            try:
                                rel_path = file.relative_to(directory)
                                # Use parent directory name as prefix for the filename
                                if (
                                    len(rel_path.parts) > 1
                                ):  # If file is in a subdirectory
                                    parent_dir = rel_path.parts[0]
                                    new_filename = f"{parent_dir}_{file.name}"
                                    target_file = target_dir / new_filename
                                else:
                                    target_file = target_dir / file.name
                            except ValueError:
                                # Handle case where file is not relative to directory
                                logger.warning(
                                    f"Could not determine relative path for {file}. Using original filename."
                                )
                                target_file = target_dir / file.name
                        else:
                            target_file = target_dir / file.name

                        # Safety check: If target exists, generate a unique name
                        if target_file.exists():
                            timestamp = int(datetime.now().timestamp())
                            target_file = (
                                target_dir
                                / f"{target_file.stem}_{timestamp}{target_file.suffix}"
                            )
                            logger.info(
                                f"Target file already exists, using unique name: {target_file}"
                            )

                        await FileMover.move_file(file, target_file)
                        moved_count += 1
                        break  # * Stop after the first matching rule.

        logger.info(
            f"Sort operation completed. Moved {moved_count} files, skipped {skipped_count} files."
        )

    def sort_directory_sync(self, directory: Path) -> None:
        """
        Synchronous wrapper around 'sort_directory' for convenience.

        * Uses 'asyncio.run' to execute the async method.

        Args:
            directory (Path): The directory to sort.
        """
        asyncio.run(self.sort_directory(directory))



================================================
File: src/the_aichemist_codex/backend/file_manager/version_manager.py
================================================
"""
File versioning system for The Aichemist Codex.

This module provides functionality to manage file versions, including
storing, comparing, and restoring different versions of files.
"""

import asyncio
import difflib
import hashlib
import json
import logging
import os
import threading
import time
from enum import Enum
from pathlib import Path

from the_aichemist_codex.backend.config.config_loader import config
from the_aichemist_codex.backend.file_manager.change_detector import ChangeDetector
from the_aichemist_codex.backend.rollback.rollback_manager import RollbackManager
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
from the_aichemist_codex.backend.utils.safety import SafeFileHandler

logger = logging.getLogger(__name__)


def get_data_dir() -> Path:
    """
    Get the data directory path dynamically to avoid circular imports.

    Returns:
        Path: The data directory path
    """
    # Check for environment variable first
    env_data_dir = os.environ.get("AICHEMIST_DATA_DIR")
    if env_data_dir:
        return Path(env_data_dir).resolve()

    # Fallback to a relative path from the current file
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent.parent.parent
    return project_root / "data"


# Define default version storage location
VERSION_STORAGE_DIR = get_data_dir() / "versions"
VERSION_METADATA_DB = get_data_dir() / "version_metadata.json"

# Make sure version directories exist
VERSION_STORAGE_DIR.mkdir(parents=True, exist_ok=True)

# Initialize other managers we'll need
rollback_manager = RollbackManager()


class VersioningPolicy(Enum):
    """Versioning policy options for different file types."""

    FULL_COPY = 1  # Store complete file copy for each version
    DIFF_BASED = 2  # Store only differences between versions for text files
    HYBRID = 3  # Use diff for text files, full copies for binary files


class VersionMetadata:
    """Metadata for a single file version."""

    def __init__(
        self,
        file_path: str,
        version_id: str,
        timestamp: float,
        policy: VersioningPolicy,
        storage_path: str,
        is_diff: bool = False,
        parent_version_id: str | None = None,
        author: str | None = None,
        change_reason: str | None = None,
        file_hash: str | None = None,
    ):
        """
        Initialize version metadata.

        Args:
            file_path: Path to the original file
            version_id: Unique identifier for this version
            timestamp: When the version was created
            policy: Versioning policy used for this version
            storage_path: Where this version is stored
            is_diff: Whether this is a diff or full file
            parent_version_id: ID of the parent version (for diffs)
            author: Who created this version
            change_reason: Why this version was created
            file_hash: Hash of the file content
        """
        self.file_path = file_path
        self.version_id = version_id
        self.timestamp = timestamp
        self.policy = policy
        self.storage_path = storage_path
        self.is_diff = is_diff
        self.parent_version_id = parent_version_id
        self.author = author
        self.change_reason = change_reason
        self.file_hash = file_hash

    def to_dict(self) -> dict:
        """Convert to dictionary for serialization."""
        return {
            "file_path": self.file_path,
            "version_id": self.version_id,
            "timestamp": self.timestamp,
            "policy": self.policy.name,
            "storage_path": self.storage_path,
            "is_diff": self.is_diff,
            "parent_version_id": self.parent_version_id,
            "author": self.author,
            "change_reason": self.change_reason,
            "file_hash": self.file_hash,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "VersionMetadata":
        """Create from dictionary (for deserialization)."""
        # Convert string policy back to enum
        policy_str = data.get("policy", "HYBRID")
        try:
            policy = VersioningPolicy[policy_str]
        except (KeyError, TypeError):
            policy = VersioningPolicy.HYBRID

        return cls(
            file_path=data["file_path"],
            version_id=data["version_id"],
            timestamp=data["timestamp"],
            policy=policy,
            storage_path=data["storage_path"],
            is_diff=data.get("is_diff", False),
            parent_version_id=data.get("parent_version_id"),
            author=data.get("author"),
            change_reason=data.get("change_reason"),
            file_hash=data.get("file_hash"),
        )


class VersionManager:
    """
    Manages file versioning operations.

    Features:
    - Controls the versioning strategy based on file type and configuration
    - Maintains version history with metadata
    - Provides version comparison and restoration capabilities
    - Optimizes storage of versions with different strategies
    """

    _instance = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        """Ensure VersionManager is a singleton."""
        if cls._instance is None:
            cls._instance = super(VersionManager, cls).__new__(cls)
        return cls._instance

    def __init__(
        self,
        storage_dir: Path = VERSION_STORAGE_DIR,
        metadata_path: Path = VERSION_METADATA_DB,
    ):
        """
        Initialize the version manager.

        Args:
            storage_dir: Directory to store version files
            metadata_path: Path to store version metadata
        """
        # Only initialize once due to singleton pattern
        if self._initialized:
            return

        self.storage_dir = storage_dir
        self.metadata_path = metadata_path
        self.version_metadata = {}
        self.change_detector = ChangeDetector()

        # Load existing metadata if it exists
        if self.metadata_path.exists():
            try:
                with open(self.metadata_path) as f:
                    self.version_metadata = json.load(f)
            except (OSError, json.JSONDecodeError) as e:
                logger.error(f"Error loading version metadata: {e}")
                # Create backup of corrupted file
                if self.metadata_path.exists():
                    backup_path = self.metadata_path.with_suffix(
                        f".backup_{int(time.time())}"
                    )
                    self.metadata_path.rename(backup_path)
                    logger.info(f"Created backup of version metadata at {backup_path}")
                # Start with empty metadata
                self.version_metadata = {}

        # Get configuration
        self.default_policy = self._get_default_policy()
        self.max_versions_per_file = config.get("versioning", {}).get(
            "max_versions_per_file", 20
        )

        # Mark as initialized
        self._initialized = True
        logger.info(f"VersionManager initialized with storage at {self.storage_dir}")

    def _get_default_policy(self) -> VersioningPolicy:
        """Get the default versioning policy from configuration."""
        policy_str = (
            config.get("versioning", {}).get("default_policy", "hybrid").upper()
        )
        try:
            return VersioningPolicy[policy_str]
        except KeyError:
            logger.warning(f"Invalid versioning policy {policy_str}, using HYBRID")
            return VersioningPolicy.HYBRID

    async def create_version(
        self,
        file_path: Path,
        change_reason: str | None = None,
        author: str | None = None,
        policy: VersioningPolicy | None = None,
    ) -> str | None:
        """
        Create a new version of a file.

        Args:
            file_path: Path to the file to version
            change_reason: Reason for creating this version
            author: Who is creating the version
            policy: Specific policy to use (overrides default)

        Returns:
            Version ID if successful, None otherwise
        """
        file_path = Path(file_path).resolve()
        if not file_path.exists() or not file_path.is_file():
            logger.error(
                f"Cannot create version: File {file_path} does not exist or is not a file"
            )
            return None

        if SafeFileHandler.should_ignore(file_path):
            logger.info(f"Skipping versioning for ignored file: {file_path}")
            return None

        # Use provided policy or default
        policy = policy or self.default_policy

        # Generate unique version ID
        timestamp = time.time()
        version_id = (
            f"{hashlib.md5(str(file_path).encode()).hexdigest()}_{int(timestamp)}"
        )

        # Determine if this is a text file for hybrid policy
        is_text_file = await self._is_text_file(file_path)

        # Determine whether to use diff-based versioning
        use_diff = False
        parent_version_id = None

        if policy == VersioningPolicy.DIFF_BASED or (
            policy == VersioningPolicy.HYBRID and is_text_file
        ):
            # Find the most recent version to diff against
            versions = self._get_file_versions(file_path)
            if versions:
                latest_version = versions[0]  # Versions are ordered newest to oldest
                parent_version_id = latest_version.version_id
                use_diff = True

        # Create the storage path
        file_dir = self.storage_dir / str(
            hash(str(file_path)) % 100
        )  # Shard by hash to avoid too many files in one dir
        file_dir.mkdir(parents=True, exist_ok=True)

        storage_path = file_dir / f"{version_id}"
        if use_diff:
            storage_path = storage_path.with_suffix(".diff")

        # Create the version
        try:
            if use_diff and parent_version_id is not None:
                # Generate diff against parent version
                diff_content = await self._generate_diff(file_path, parent_version_id)
                success = await AsyncFileIO.write(storage_path, diff_content)
                if not success:
                    raise OSError(f"Failed to write diff to {storage_path}")
            else:
                # Full copy
                success = await AsyncFileIO.copy(file_path, storage_path)
                if not success:
                    raise OSError(f"Failed to copy file to {storage_path}")

            # Calculate file hash
            file_hash = await self._calculate_file_hash(file_path)

            # Create and store metadata
            metadata = VersionMetadata(
                file_path=str(file_path),
                version_id=version_id,
                timestamp=timestamp,
                policy=policy,
                storage_path=str(storage_path),
                is_diff=use_diff,
                parent_version_id=parent_version_id,
                author=author,
                change_reason=change_reason,
                file_hash=file_hash,
            )

            # Add to metadata storage
            if str(file_path) not in self.version_metadata:
                self.version_metadata[str(file_path)] = []

            self.version_metadata[str(file_path)].insert(0, metadata.to_dict())

            # Limit the number of versions per file
            self._prune_old_versions(file_path)

            # Save metadata
            await self._save_metadata()

            logger.info(f"Created version {version_id} for {file_path}")
            return version_id

        except Exception as e:
            logger.error(f"Error creating version for {file_path}: {e}")
            # Clean up if necessary
            if storage_path.exists():
                storage_path.unlink()
            return None

    async def _save_metadata(self) -> bool:
        """Save version metadata to the metadata file."""
        try:
            # Write to a temporary file first to prevent corruption
            temp_path = self.metadata_path.with_suffix(".tmp")
            success = await AsyncFileIO.write_json(temp_path, self.version_metadata)
            if not success:
                logger.error(f"Failed to write metadata to {temp_path}")
                return False

            # Replace the original file
            if temp_path.exists():
                # On Windows, we need to remove the target file first
                if os.name == "nt" and self.metadata_path.exists():
                    self.metadata_path.unlink()
                temp_path.rename(self.metadata_path)
                return True
            return False

        except Exception as e:
            logger.error(f"Error saving version metadata: {e}")
            return False

    def _get_file_versions(self, file_path: Path) -> list[VersionMetadata]:
        """Get all versions for a file, ordered newest first."""
        file_path_str = str(file_path.resolve())
        versions_data = self.version_metadata.get(file_path_str, [])
        return [VersionMetadata.from_dict(v) for v in versions_data]

    def _prune_old_versions(self, file_path: Path) -> None:
        """Limit the number of versions stored for a file."""
        file_path_str = str(file_path.resolve())
        versions = self.version_metadata.get(file_path_str, [])

        if len(versions) <= self.max_versions_per_file:
            return

        # Keep the newest max_versions_per_file
        to_remove = versions[self.max_versions_per_file :]
        self.version_metadata[file_path_str] = versions[: self.max_versions_per_file]

        # Delete the old version files
        for version_data in to_remove:
            try:
                storage_path = Path(version_data["storage_path"])
                if storage_path.exists():
                    storage_path.unlink()
                logger.debug(f"Removed old version {version_data['version_id']}")
            except Exception as e:
                logger.error(
                    f"Error removing old version {version_data['version_id']}: {e}"
                )

    async def _is_text_file(self, file_path: Path) -> bool:
        """Determine if a file is a text file."""
        # Leverage the ChangeDetector's text file detection
        return await self.change_detector._is_text_file(file_path)

    async def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate a hash of the file contents."""
        return await self.change_detector._calculate_file_hash(file_path)

    async def _generate_diff(self, file_path: Path, parent_version_id: str) -> str:
        """Generate a diff between the current file and a parent version."""
        # Get the parent version content
        parent_content = await self.get_version_content(parent_version_id)
        if parent_content is None:
            raise ValueError(
                f"Could not get content for parent version {parent_version_id}"
            )

        # Get the current file content
        current_content = await AsyncFileIO.read_text(file_path)

        # Generate the diff
        parent_lines = parent_content.splitlines(keepends=True)
        current_lines = current_content.splitlines(keepends=True)

        diff = difflib.unified_diff(
            parent_lines,
            current_lines,
            fromfile=f"{file_path}.{parent_version_id}",
            tofile=f"{file_path}.current",
            n=3,  # Context lines
        )

        return "".join(diff)

    async def get_version_content(self, version_id: str) -> str | None:
        """
        Get the content of a specific version.

        For diff-based versions, this reconstructs the full content
        by applying diffs to the original version.

        Args:
            version_id: ID of the version to retrieve

        Returns:
            Content of the version if available, None otherwise
        """
        # Find the version in metadata
        version_metadata = None
        file_path = None

        for file_versions in self.version_metadata.values():
            for version_data in file_versions:
                if version_data["version_id"] == version_id:
                    version_metadata = VersionMetadata.from_dict(version_data)
                    file_path = version_data["file_path"]
                    break
            if version_metadata:
                break

        if not version_metadata:
            logger.error(f"Version {version_id} not found in metadata")
            return None

        storage_path = Path(version_metadata.storage_path)
        if not storage_path.exists():
            logger.error(f"Version file {storage_path} does not exist")
            return None

        try:
            if not version_metadata.is_diff:
                # For full copies, just return the content
                return await AsyncFileIO.read_text(storage_path)
            else:
                # For diffs, we need to reconstruct by applying the diff chain
                return await self._reconstruct_from_diff_chain(version_metadata)

        except Exception as e:
            logger.error(f"Error retrieving version content: {e}")
            return None

    async def _reconstruct_from_diff_chain(
        self, version_metadata: VersionMetadata
    ) -> str | None:
        """Reconstruct file content by applying a chain of diffs."""
        # Start with this diff
        diffs_to_apply = []
        current_metadata = version_metadata

        # Collect all diffs in the chain
        while current_metadata.is_diff and current_metadata.parent_version_id:
            # Add this diff to the list
            diff_content = await AsyncFileIO.read_text(
                Path(current_metadata.storage_path)
            )
            diffs_to_apply.append(diff_content)

            # Find the parent version
            parent_found = False
            for file_versions in self.version_metadata.values():
                for version_data in file_versions:
                    if version_data["version_id"] == current_metadata.parent_version_id:
                        current_metadata = VersionMetadata.from_dict(version_data)
                        parent_found = True
                        break
                if parent_found:
                    break

            if not parent_found:
                logger.error(
                    f"Missing parent version {current_metadata.parent_version_id} in diff chain"
                )
                return None

        # Now we should have the base version
        if current_metadata.is_diff:
            logger.error("Could not find base version in diff chain")
            return None

        # Start with the base version content
        content = await AsyncFileIO.read_text(Path(current_metadata.storage_path))

        # Apply diffs in reverse order (newest diff first in our collection)
        for diff in reversed(diffs_to_apply):
            content = self._apply_diff(content, diff)

        return content

    def _apply_diff(self, content: str, diff: str) -> str:
        """Apply a unified diff to content."""
        # This is a simplified implementation
        # A full implementation would properly parse and apply unified diffs

        # For now, we'll use the external patch command if available
        # Otherwise, fallback to a simplified approach

        # TODO: Implement a proper unified diff parser and applicator
        # For now, this is a placeholder

        lines = content.splitlines()
        diff_lines = diff.splitlines()

        # Skip the diff header lines
        header_lines = 0
        for line in diff_lines:
            if line.startswith("@@"):
                header_lines += 1
                break
            header_lines += 1

        diff_lines = diff_lines[header_lines:]

        # Apply the changes
        result_lines = []
        i = 0
        while i < len(diff_lines):
            line = diff_lines[i]
            if line.startswith(" "):
                # Context line - keep as is
                result_lines.append(line[1:])
            elif line.startswith("+"):
                # Added line
                result_lines.append(line[1:])
            elif line.startswith("-"):
                # Removed line - skip it
                pass
            else:
                # Other lines (shouldn't happen in a proper diff)
                result_lines.append(line)
            i += 1

        return "\n".join(result_lines)

    async def restore_version(self, file_path: Path, version_id: str) -> bool:
        """
        Restore a file to a previous version.

        Args:
            file_path: Path to the file to restore
            version_id: ID of the version to restore

        Returns:
            True if successful, False otherwise
        """
        file_path = Path(file_path).resolve()

        # Check if the version exists
        version_content = await self.get_version_content(version_id)
        if version_content is None:
            logger.error(f"Failed to get content for version {version_id}")
            return False

        # Create a new version of the current file before restoring
        # This allows us to undo the restoration if needed
        await self.create_version(
            file_path,
            change_reason="Auto-saved before version restoration",
            author="System",
        )

        # Write the version content to the file
        success = await AsyncFileIO.write(file_path, version_content)
        if not success:
            logger.error(f"Failed to write version content to {file_path}")
            return False

        logger.info(f"Restored {file_path} to version {version_id}")
        return True

    async def list_versions(self, file_path: Path) -> list[VersionMetadata]:
        """
        List all versions of a file.

        Args:
            file_path: Path to the file

        Returns:
            List of version metadata objects, ordered newest first
        """
        file_path = Path(file_path).resolve()
        return self._get_file_versions(file_path)

    async def get_version_info(self, version_id: str) -> VersionMetadata | None:
        """
        Get detailed information about a specific version.

        Args:
            version_id: ID of the version to get info for

        Returns:
            Version metadata if found, None otherwise
        """
        # Search for the version in all files
        for file_versions in self.version_metadata.values():
            for version_data in file_versions:
                if version_data["version_id"] == version_id:
                    return VersionMetadata.from_dict(version_data)

        logger.error(f"Version {version_id} not found")
        return None

    async def cleanup_old_versions(self, days: float | None = None) -> int:
        """
        Clean up versions older than the specified number of days.

        Args:
            days: Number of days to keep versions for (defaults to
                 version_retention_days in config)

        Returns:
            Number of versions removed
        """
        # Get retention period from config if not specified
        retention_days = days
        if retention_days is None:
            retention_days = config.get("versioning", {}).get(
                "version_retention_days", 30
            )

        # Calculate cutoff time
        cutoff_time = time.time() - (retention_days * 86400)  # 86400 seconds per day
        removed_count = 0

        # For each file, remove versions older than the cutoff
        for file_path, versions in list(self.version_metadata.items()):
            kept_versions = []

            for version_data in versions:
                if version_data["timestamp"] >= cutoff_time:
                    kept_versions.append(version_data)
                else:
                    # Remove the version file
                    try:
                        storage_path = Path(version_data["storage_path"])
                        if storage_path.exists():
                            storage_path.unlink()
                            removed_count += 1
                            logger.debug(
                                f"Removed old version {version_data['version_id']}"
                            )
                    except Exception as e:
                        logger.error(
                            f"Error removing old version {version_data['version_id']}: {e}"
                        )

            # Update the metadata
            if not kept_versions:
                # Remove the file entry if no versions remain
                del self.version_metadata[file_path]
            else:
                self.version_metadata[file_path] = kept_versions

        # Save the updated metadata
        await self._save_metadata()

        logger.info(f"Cleaned up {removed_count} old versions")
        return removed_count

    async def bulk_restore_versions(
        self, restore_map: dict[str, str]
    ) -> dict[str, bool]:
        """
        Restore multiple files to specific versions at once.

        Args:
            restore_map: Dictionary mapping file paths to version IDs

        Returns:
            Dictionary mapping file paths to success status
        """
        results = {}

        for file_path_str, version_id in restore_map.items():
            file_path = Path(file_path_str)
            success = await self.restore_version(file_path, version_id)
            results[file_path_str] = success

        return results


# Create a singleton instance
version_manager = VersionManager()


# Schedule periodic cleanup of old versions
def start_version_cleanup_scheduler():
    """
    Start a background thread to periodically clean up old versions.
    """

    def cleanup_task():
        """Run the cleanup task periodically."""
        while True:
            try:
                # Run cleanup
                asyncio.run(version_manager.cleanup_old_versions())

                # Sleep until next cleanup time
                # Get cleanup interval from config (default: once per day)
                cleanup_interval_hours = config.get("versioning", {}).get(
                    "cleanup_interval_hours", 24
                )
                time.sleep(cleanup_interval_hours * 3600)
            except Exception as e:
                logger.error(f"Error in version cleanup task: {e}")
                # Don't crash the thread, just wait and retry
                time.sleep(3600)  # Wait an hour and try again

    # Start the background thread
    cleanup_thread = threading.Thread(target=cleanup_task, daemon=True)
    cleanup_thread.start()
    logger.info("Version cleanup scheduler started")


# Start the cleanup scheduler
start_version_cleanup_scheduler()

