
================================================
File: src/the_aichemist_codex/backend/metadata/__init__.py
================================================
"""
Metadata extraction framework for The Aichemist Codex.

This module provides functionality for extracting rich metadata from various file types,
enabling intelligent content analysis, auto-tagging, and content categorization.
"""

from .audio_extractor import AudioMetadataExtractor
from .database_extractor import DatabaseMetadataExtractor
from .extractor import BaseMetadataExtractor, MetadataExtractorRegistry
from .image_extractor import ImageMetadataExtractor
from .pdf_extractor import PDFMetadataExtractor
from .video_extractor import VideoMetadataExtractor

__all__ = [
    "BaseMetadataExtractor",
    "MetadataExtractorRegistry",
    "ImageMetadataExtractor",
    "AudioMetadataExtractor",
    "DatabaseMetadataExtractor",
    "PDFMetadataExtractor",
    "VideoMetadataExtractor",
]



================================================
File: src/the_aichemist_codex/backend/metadata/audio_extractor.py
================================================
"""
Audio metadata extraction for The Aichemist Codex.

This module provides functionality to extract metadata from audio files,
including format information, duration, bitrate, sample rate, and other
audio-specific metadata like ID3 tags.
"""

import logging
from pathlib import Path
from typing import Any, TypeVar

# Import File directly from mutagen
import mutagen
from mutagen.flac import FLAC
from mutagen.id3 import ID3
from mutagen.mp3 import MP3
from mutagen.mp4 import MP4
from mutagen.oggvorbis import OggVorbis

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.metadata.extractor import (
    BaseMetadataExtractor,
    MetadataExtractorRegistry,
)
from the_aichemist_codex.backend.utils.cache_manager import CacheManager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

# Type aliases for better type checking
AudioFile = TypeVar("AudioFile")
AudioInfo = TypeVar("AudioInfo")

# Handle optional dependencies
AUDIO_LIBS_AVAILABLE = False
AudioSegment = None  # Initialize as None
try:
    # Attempt to import pydub
    import pydub
    from pydub import AudioSegment

    AUDIO_LIBS_AVAILABLE = True
except ImportError:
    # Dependency not available, will handle gracefully at runtime
    logger = logging.getLogger(__name__)
    logger.warning(
        "pydub or audioop/pyaudioop dependency is missing. Some audio features will be disabled."
    )


logger = logging.getLogger(__name__)


class AudioMetadataExtractor(BaseMetadataExtractor):
    """
    Metadata extractor for audio files.

    This extractor can process various audio formats (MP3, WAV, FLAC, OGG, etc.)
    and extract rich metadata including:
    - File format and encoding
    - Duration, bitrate, sample rate, and channels
    - ID3 tags (artist, album, title, genre, etc.)
    - Album art (if available)
    - Technical audio characteristics
    """

    # Supported MIME types
    SUPPORTED_MIME_TYPES = [
        "audio/mpeg",
        "audio/mp3",
        "audio/wav",
        "audio/wave",
        "audio/x-wav",
        "audio/flac",
        "audio/ogg",
        "audio/aac",
        "audio/mp4",
        "audio/x-m4a",
        "audio/webm",
    ]

    # Mapping from file extensions to pydub formats
    PYDUB_FORMAT_MAP = {
        ".mp3": "mp3",
        ".wav": "wav",
        ".wave": "wav",
        ".flac": "flac",
        ".ogg": "ogg",
        ".aac": "aac",
        ".m4a": "mp4",
        ".webm": "webm",
    }

    def __init__(self, cache_manager: CacheManager | None = None):
        """Initialize the audio metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        super().__init__(cache_manager)
        self.mime_detector = MimeTypeDetector()
        if not AUDIO_LIBS_AVAILABLE:
            logger.warning(
                "Audio libraries not available: pydub or audioop/pyaudioop missing"
            )

    @property
    def supported_mime_types(self) -> list[str]:
        """List of MIME types supported by this extractor.

        Returns:
            list[str]: A list of MIME type strings that this extractor can handle
        """
        return self.SUPPORTED_MIME_TYPES

    async def extract(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """
        Extract metadata from an audio file.

        Args:
            file_path: Path to the audio file
            content: Not used for audio files
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            Dictionary containing extracted audio metadata
        """
        path = Path(file_path)
        if not path.exists():
            logger.warning(f"Audio file not found: {path}")
            return {}

        if not AUDIO_LIBS_AVAILABLE:
            return {
                "error": "Audio libraries not available",
                "file_path": str(path),
                "type": "audio",
                "format": path.suffix.lstrip(".").lower(),
                "duration_seconds": 0,
                "channels": 0,
                "sample_rate": 0,
                "bit_rate": 0,
            }

        # Determine MIME type if not provided
        if mime_type is None:
            mime_type, _ = self.mime_detector.get_mime_type(path)

        # Check if this is a supported audio type
        if mime_type not in self.SUPPORTED_MIME_TYPES:
            logger.debug(f"Unsupported audio type: {mime_type} for file: {path}")
            return {}

        # Try to use cache if available
        if self.cache_manager:
            cache_key = f"audio_metadata:{path}:{path.stat().st_mtime}"
            cached_data = await self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Using cached audio metadata for {path}")
                return cached_data

        # Process the audio file
        try:
            result = await self._process_audio(path)

            # Cache the result if cache manager is available
            if self.cache_manager:
                await self.cache_manager.put(cache_key, result)

            return result
        except Exception as e:
            logger.error(f"Error extracting audio metadata from {path}: {str(e)}")
            return {}

    async def _process_audio(self, path: Path) -> dict[str, Any]:
        """
        Process an audio file to extract metadata.

        Args:
            path: Path to the audio file

        Returns:
            Dictionary containing extracted audio metadata
        """
        result = {
            "metadata_type": "audio",
            "format": {},
            "tags": {},
        }

        # Extract format-specific data using Mutagen
        try:
            mutagen_data = self._extract_mutagen_metadata(path)
            if mutagen_data:
                result.update(mutagen_data)
        except Exception as e:
            logger.debug(f"Error extracting Mutagen metadata: {str(e)}")

        # Extract audio characteristics using PyDub
        try:
            pydub_data = self._extract_pydub_metadata(path)
            if pydub_data:
                # Merge with existing format data
                result["format"].update(pydub_data.get("format", {}))

                # Add analysis data
                if "analysis" in pydub_data:
                    result["analysis"] = pydub_data["analysis"]
        except Exception as e:
            logger.debug(f"Error extracting PyDub metadata: {str(e)}")

        return result

    def _extract_mutagen_metadata(self, path: Path) -> dict[str, Any]:
        """
        Extract audio metadata using Mutagen.

        Args:
            path: Path to the audio file

        Returns:
            Dictionary containing metadata extracted with Mutagen
        """
        result = {"format": {}, "tags": {}}
        file_ext = path.suffix.lower()

        try:
            # Process different audio formats
            if file_ext == ".mp3":
                self._process_mp3(path, result)
            elif file_ext == ".flac":
                self._process_flac(path, result)
            elif file_ext == ".ogg":
                self._process_ogg(path, result)
            elif file_ext in [".m4a", ".mp4", ".aac"]:
                self._process_mp4(path, result)
            else:
                # Generic handling for other formats
                audio_file = mutagen.File(str(path))  # type: ignore
                if audio_file:
                    self._extract_generic_tags(audio_file, result)
                    if hasattr(audio_file, "info"):
                        info = audio_file.info
                        # Safely extract common properties
                        self._safe_extract_audio_info(info, result)
        except Exception as e:
            logger.debug(f"Mutagen processing error for {path}: {str(e)}")

        return result

    def _safe_extract_audio_info(self, info: Any, result: dict[str, Any]) -> None:
        """
        Safely extract common audio information properties.

        Args:
            info: Audio info object from Mutagen
            result: Dictionary to update with extracted information
        """
        # Extract duration which is common in all formats
        if hasattr(info, "length"):
            result["format"]["duration"] = info.length

        # Safely extract other common properties
        for attr in ["bitrate", "sample_rate", "channels", "layer", "bits_per_sample"]:
            if hasattr(info, attr):
                value = getattr(info, attr)
                if value is not None:
                    result["format"][attr] = value

        # Special handling for MP3 mode attribute since it's causing linter issues
        if hasattr(info, "mode") and info.__class__.__name__ == "MPEGInfo":
            # For MP3 files, mode indicates stereo mode
            # 0: Stereo, 1: Joint Stereo, 2: Dual Channel, 3: Mono
            mode_value = info.mode
            result["format"]["channels"] = 1 if mode_value == 3 else 2
            result["format"]["stereo_mode"] = mode_value

    def _process_mp3(self, path: Path, result: dict[str, Any]) -> None:
        """
        Process MP3 file and extract metadata.

        Args:
            path: Path to the MP3 file
            result: Dictionary to update with extracted metadata
        """
        try:
            mp3 = MP3(path)

            # Format information
            result["format"]["encoding"] = "mp3"
            # Use safe extraction with common attributes
            self._safe_extract_audio_info(mp3.info, result)

            # Try to get ID3 tags
            try:
                id3 = ID3(path)
                for key in id3:
                    frame = id3[key]
                    text = None

                    # Extract text from different tag types
                    if hasattr(frame, "text"):
                        text = frame.text
                    elif hasattr(frame, "data"):
                        text = frame.data

                    # Process text data
                    if text:
                        if isinstance(text, list) and len(text) == 1:
                            result["tags"][key] = str(text[0])
                        else:
                            result["tags"][key] = str(text)

                # Common ID3 tags mapping
                tag_mapping = {
                    "TPE1": "artist",
                    "TIT2": "title",
                    "TALB": "album",
                    "TCON": "genre",
                    "TDRC": "date",
                    "TRCK": "tracknumber",
                }

                # Map to common names
                for id3_tag, common_name in tag_mapping.items():
                    if id3_tag in result["tags"]:
                        result["tags"][common_name] = result["tags"][id3_tag]
            except Exception as e:
                logger.debug(f"Error extracting ID3 tags: {str(e)}")

            # Check for album art
            if hasattr(mp3, "tags") and mp3.tags:
                for key in mp3.tags.keys():
                    if key.startswith("APIC:"):
                        result["has_album_art"] = True
                        break
        except Exception as e:
            logger.debug(f"Error processing MP3 file {path}: {str(e)}")

    def _process_flac(self, path: Path, result: dict[str, Any]) -> None:
        """
        Process FLAC file and extract metadata.

        Args:
            path: Path to the FLAC file
            result: Dictionary to update with extracted metadata
        """
        try:
            flac = FLAC(path)

            # Format information
            result["format"]["encoding"] = "flac"
            # Use safe extraction for common attributes
            self._safe_extract_audio_info(flac.info, result)

            # Tags
            for key, value in flac.items():
                if isinstance(value, list) and len(value) == 1:
                    result["tags"][key.lower()] = value[0]
                else:
                    result["tags"][key.lower()] = value

            # Check for pictures/album art
            if hasattr(flac, "pictures") and flac.pictures:
                result["has_album_art"] = True
        except Exception as e:
            logger.debug(f"Error processing FLAC file {path}: {str(e)}")

    def _process_ogg(self, path: Path, result: dict[str, Any]) -> None:
        """
        Process OGG file and extract metadata.

        Args:
            path: Path to the OGG file
            result: Dictionary to update with extracted metadata
        """
        try:
            ogg = OggVorbis(path)

            # Format information
            result["format"]["encoding"] = "vorbis"
            # Use safe extraction for common attributes
            self._safe_extract_audio_info(ogg.info, result)

            # Tags
            for key, value in ogg.items():
                if isinstance(value, list) and len(value) == 1:
                    result["tags"][key.lower()] = value[0]
                else:
                    result["tags"][key.lower()] = value
        except Exception as e:
            logger.debug(f"Error processing OGG file {path}: {str(e)}")

    def _process_mp4(self, path: Path, result: dict[str, Any]) -> None:
        """
        Process MP4/M4A/AAC file and extract metadata.

        Args:
            path: Path to the MP4 file
            result: Dictionary to update with extracted metadata
        """
        try:
            mp4 = MP4(path)

            # Format information
            result["format"]["encoding"] = (
                "aac" if path.suffix.lower() == ".aac" else "mp4"
            )
            # Use safe extraction for common attributes
            self._safe_extract_audio_info(mp4.info, result)

            # Map common MP4 tags to standard names
            tag_mapping = {
                "©nam": "title",
                "©ART": "artist",
                "©alb": "album",
                "©gen": "genre",
                "©day": "date",
                "©wrt": "composer",
                "trkn": "tracknumber",
                "disk": "discnumber",
                "©cmt": "comment",
            }

            # Process tags
            for key, value in mp4.items():
                if key in tag_mapping:
                    tag_name = tag_mapping[key]
                    if isinstance(value, list) and len(value) == 1:
                        result["tags"][tag_name] = value[0]
                    else:
                        result["tags"][tag_name] = value

            # Check for album art
            if "covr" in mp4:
                result["has_album_art"] = True
        except Exception as e:
            logger.debug(f"Error processing MP4 file {path}: {str(e)}")

    def _extract_generic_tags(self, audio_file: Any, result: dict[str, Any]) -> None:
        """
        Extract tags from a generic audio file.

        Args:
            audio_file: Mutagen audio file object
            result: Dictionary to update with extracted tags
        """
        if hasattr(audio_file, "tags") and audio_file.tags:
            for key, value in audio_file.tags.items():
                if hasattr(value, "text"):
                    # Handle ID3 style tags
                    if isinstance(value.text, list) and len(value.text) == 1:
                        result["tags"][key.lower()] = value.text[0]
                    else:
                        result["tags"][key.lower()] = value.text
                else:
                    # Handle simple tag values
                    if isinstance(value, list) and len(value) == 1:
                        result["tags"][key.lower()] = value[0]
                    else:
                        result["tags"][key.lower()] = value

    def _extract_pydub_metadata(self, path: Path) -> dict[str, Any]:
        """
        Extract audio metadata using PyDub.

        Args:
            path: Path to the audio file

        Returns:
            Dictionary containing metadata extracted with PyDub
        """
        if not AUDIO_LIBS_AVAILABLE or AudioSegment is None:
            return {"format": {}}  # Return empty dict when PyDub is not available

        result = {"format": {}}
        file_ext = path.suffix.lower()

        try:
            # Determine PyDub format
            pydub_format = self.PYDUB_FORMAT_MAP.get(file_ext)
            if not pydub_format:
                logger.debug(f"Unsupported PyDub format for extension: {file_ext}")
                return result

            # Load audio with PyDub
            audio = AudioSegment.from_file(str(path), format=pydub_format)

            # Extract basic format info
            result["format"].update(
                {
                    "duration_seconds": len(audio) / 1000,
                    "channels": audio.channels,
                    "sample_rate": audio.frame_rate,
                    "bit_depth": audio.sample_width * 8,
                    "bit_rate": audio.frame_rate * audio.frame_width * 8,
                }
            )

            # Add audio section analysis if the file isn't too large
            if len(audio) < 300000:  # Limit to 5 minutes of audio for performance
                # Instead of calling _analyze_audio, do the analysis directly here
                analysis = {}

                # Calculate dBFS (decibels relative to full scale)
                analysis["max_dBFS"] = audio.max_dBFS
                analysis["dBFS"] = audio.dBFS

                # Add RMS (root mean square)
                analysis["rms"] = audio.rms

                result["analysis"] = analysis

        except Exception as e:
            logger.debug(f"PyDub processing error for {path}: {str(e)}")

        return result

    def supports_file(self, file_path: str | Path) -> bool:
        """
        Check if this extractor supports the given file.

        Args:
            file_path: Path to the file to check

        Returns:
            True if this extractor supports the file, False otherwise
        """
        # Skip if audio libraries are not available
        if not AUDIO_LIBS_AVAILABLE or AudioSegment is None:
            return False

        path = Path(file_path)
        if not path.exists():
            return False

        # Check if the file is a supported audio type
        mime_type, _ = self.mime_detector.get_mime_type(path)
        if mime_type in self.SUPPORTED_MIME_TYPES:
            return True

        # Check file extension
        return path.suffix.lower() in self.PYDUB_FORMAT_MAP


# Register the extractor
MetadataExtractorRegistry.register(AudioMetadataExtractor)



================================================
File: src/the_aichemist_codex/backend/metadata/code_extractor.py
================================================
"""Code metadata extractor for extracting information from code files.

This module provides functionality for analyzing code content to extract
language details, complexity metrics, imports, dependencies, and other code-specific metadata.
"""

# mypy: disable-error-code="return-value"

import logging
import re
import time
from pathlib import Path
from typing import Any

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

from .extractor import BaseMetadataExtractor, MetadataExtractorRegistry

logger = logging.getLogger(__name__)


@MetadataExtractorRegistry.register
class CodeMetadataExtractor(BaseMetadataExtractor):
    """Metadata extractor for code files.

    Analyzes code content to extract programming language, imports,
    function/class definitions, complexity metrics, and other code-specific metadata.
    """

    def __init__(self, cache_manager: CacheManager | None = None):
        """Initialize the code metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        super().__init__(cache_manager)

        # Language detection patterns
        self.language_patterns = {
            "python": [
                r"^import\s+\w+",
                r"^from\s+[\w.]+\s+import",
                r"def\s+\w+\s*\(",
                r"class\s+\w+[:(]",
            ],
            "javascript": [
                r"const\s+\w+\s*=",
                r"let\s+\w+\s*=",
                r"function\s+\w+\s*\(",
                r"import\s+.*from",
            ],
            "typescript": [
                r"interface\s+\w+",
                r"type\s+\w+\s*=",
                r"class\s+\w+",
                r":\s*\w+(\[\])?",
            ],
            "java": [
                r"public\s+class",
                r"private\s+\w+",
                r"protected\s+\w+",
                r"package\s+[\w.]+;",
            ],
            "c": [
                r"#include",
                r"void\s+\w+\s*\(",
                r"int\s+\w+\s*\(",
                r"struct\s+\w+\s*\{",
            ],
            "cpp": [
                r"#include\s*<\w+>",
                r"namespace",
                r"template",
                r"class\s+\w+\s*\{",
            ],
            "csharp": [
                r"using\s+[\w.]+;",
                r"namespace\s+[\w.]+",
                r"public\s+class",
                r"private\s+\w+",
            ],
            "go": [
                r"package\s+\w+",
                r"func\s+\(",
                r"import\s+\(",
                r"type\s+\w+\s+struct",
            ],
            "ruby": [r"require", r"def\s+\w+", r"class\s+\w+", r"module\s+\w+"],
            "php": [
                r"<\?php",
                r"function\s+\w+\s*\(",
                r"class\s+\w+",
                r"namespace\s+[\w\\]+;",
            ],
            "swift": [
                r"import\s+\w+",
                r"func\s+\w+\s*\(",
                r"class\s+\w+",
                r"var\s+\w+\s*:",
            ],
            "rust": [r"fn\s+\w+", r"struct\s+\w+", r"impl", r"use\s+[\w:]+"],
            "kotlin": [r"fun\s+\w+", r"class\s+\w+", r"val\s+\w+", r"var\s+\w+"],
            "html": [r"<!DOCTYPE", r"<html", r"<head", r"<body"],
            "css": [r"[\.\#][\w-]+\s*\{", r"@media", r"@import", r"@keyframes"],
            "sql": [
                r"SELECT",
                r"INSERT\s+INTO",
                r"CREATE\s+TABLE",
                r"UPDATE\s+\w+\s+SET",
            ],
            "shell": [
                r"#!/bin/(ba)?sh",
                r"if\s+\[\s+",
                r"for\s+\w+\s+in",
                r"while\s+\[\s+",
            ],
        }

        # Extensions to language mapping
        self.extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".jsx": "javascript",
            ".tsx": "typescript",
            ".java": "java",
            ".c": "c",
            ".cpp": "cpp",
            ".cc": "cpp",
            ".h": "c",
            ".hpp": "cpp",
            ".cs": "csharp",
            ".go": "go",
            ".rb": "ruby",
            ".php": "php",
            ".swift": "swift",
            ".rs": "rust",
            ".kt": "kotlin",
            ".html": "html",
            ".htm": "html",
            ".css": "css",
            ".sql": "sql",
            ".sh": "shell",
            ".bash": "shell",
        }

        # Import patterns for different languages
        self.import_patterns = {
            "python": r"^\s*(import|from)\s+([\w.]+)",
            "javascript": r"(import|require)\s*\(?[\'\"]([^\'\"]+)",
            "typescript": r"(import|require)\s*\(?[\'\"]([^\'\"]+)",
            "java": r"import\s+([\w.]+)",
            "go": r"import\s+(?:\(\s*)?([\"\w\./]+)",
            "rust": r"use\s+([\w:]+)",
            "kotlin": r"import\s+([\w.]+)",
        }

        # Function/Class definition patterns
        self.function_patterns = {
            "python": r"def\s+(\w+)",
            "javascript": r"function\s+(\w+)|(\w+)\s*=\s*function|(\w+)\s*:\s*function",
            "typescript": r"function\s+(\w+)|(\w+)\s*=\s*function|(\w+)\s*:\s*function",
            "java": r"(public|private|protected)?\s+(static)?\s+\w+\s+(\w+)\s*\(",
            "go": r"func\s+(\w+)",
            "rust": r"fn\s+(\w+)",
            "kotlin": r"fun\s+(\w+)",
        }

        self.class_patterns = {
            "python": r"class\s+(\w+)",
            "javascript": r"class\s+(\w+)",
            "typescript": r"class\s+(\w+)|interface\s+(\w+)",
            "java": r"class\s+(\w+)",
            "go": r"type\s+(\w+)\s+struct",
            "rust": r"struct\s+(\w+)|impl(?:\s+\w+)?\s+for\s+(\w+)",
            "kotlin": r"class\s+(\w+)",
        }

    @property
    def supported_mime_types(self) -> list[str]:
        """List of MIME types supported by this extractor."""
        return [
            "text/x-python",
            "application/javascript",
            "application/x-javascript",
            "text/javascript",
            "application/typescript",
            "text/x-java",
            "text/x-c",
            "text/x-c++",
            "text/x-csharp",
            "text/x-go",
            "text/x-ruby",
            "application/x-php",
            "text/x-swift",
            "text/x-rust",
            "text/x-kotlin",
            "text/html",
            "text/css",
            "text/x-sql",
            "application/x-sh",
        ]

    async def extract(  # type: ignore
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """Extract metadata from a code file.

        Args:
            file_path: Path to the file
            content: Optional pre-loaded file content
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            A dictionary containing extracted metadata
        """
        start_time = time.time()

        # Check if we have cached results
        if self.cache_manager and hasattr(self.cache_manager, "get"):
            cache_key = f"code_metadata:{file_path}"
            # Properly await the async cache manager get method
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result and isinstance(cached_result, dict):
                logger.debug(f"Using cached metadata for {file_path}")
                return cached_result  # type: ignore

        # Convert file_path to Path object
        if isinstance(file_path, str):
            file_path = Path(file_path)

        # Get the content if not provided
        if content is None:
            content = await self._get_content(file_path)  # type: ignore
            if not content:
                return {
                    "error": "Failed to read file content",
                    "extraction_complete": False,
                    "extraction_confidence": 0.0,
                    "extraction_time": time.time() - start_time,
                }

        # Detect language
        language = self._detect_language(file_path, content)

        # Extract code metadata
        extracted_data: dict[str, Any] = {
            "code_language": language,
            "imports": [],
            "functions": [],
            "classes": [],
            "complexity": {},
            "tags": [],
        }

        if language:
            # Extract imports/dependencies
            imports = self._extract_imports(content, language)
            extracted_data["imports"] = imports

            # Extract functions
            functions = self._extract_functions(content, language)
            extracted_data["functions"] = functions

            # Extract classes
            classes = self._extract_classes(content, language)
            extracted_data["classes"] = classes

            # Calculate complexity
            complexity = self._calculate_complexity(content, language)
            extracted_data["complexity"] = complexity

            # Generate tags
            tags = self._generate_tags(
                language, imports, functions, classes, complexity
            )
            extracted_data["tags"] = tags

        # Mark extraction as complete
        extracted_data["extraction_complete"] = True
        extracted_data["extraction_confidence"] = 0.7 if language else 0.3
        extracted_data["extraction_time"] = time.time() - start_time

        # Cache the results if we have a cache manager
        if self.cache_manager and hasattr(self.cache_manager, "put"):
            await self.cache_manager.put(cache_key, extracted_data)  # type: ignore

        return extracted_data

    def _detect_language(self, file_path: Path, content: str) -> str:
        """Detect the programming language of the file.

        Args:
            file_path: Path to the file
            content: File content

        Returns:
            The detected programming language or empty string if unknown
        """
        # First, try to detect by file extension
        extension = file_path.suffix.lower()
        if extension in self.extension_map:
            return self.extension_map[extension]

        # Fallback to content analysis
        language_scores = {}

        # Skip empty content
        if not content.strip():
            return ""

        # Get the first 100 lines of code (or fewer if the file is smaller)
        lines = content.split("\n")[:100]
        code_sample = "\n".join(lines)

        # Check each language's patterns against the code sample
        for language, patterns in self.language_patterns.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, code_sample, re.MULTILINE | re.IGNORECASE)
                score += len(matches)

            if score > 0:
                language_scores[language] = score

        # Return the language with the highest score, or empty string if none match
        if language_scores:
            return max(language_scores.items(), key=lambda x: x[1])[0]

        return ""

    def _extract_imports(self, content: str, language: str) -> list[str]:
        """Extract imports or dependencies from the code.

        Args:
            content: File content
            language: Detected programming language

        Returns:
            A list of imported modules or packages
        """
        imports = []

        if language in self.import_patterns:
            pattern = self.import_patterns[language]
            matches = re.findall(pattern, content, re.MULTILINE)

            # Process matches differently depending on the language
            if language in ["python", "java", "go", "rust", "kotlin"]:
                imports = [
                    match[1] if isinstance(match, tuple) else match for match in matches
                ]
            elif language in ["javascript", "typescript"]:
                imports = [match[1] for match in matches if match[1]]

        # Remove duplicates and return
        return list(set(imports))

    def _extract_functions(self, content: str, language: str) -> list[str]:
        """Extract function definitions from the code.

        Args:
            content: File content
            language: Detected programming language

        Returns:
            A list of function names
        """
        functions = []

        if language in self.function_patterns:
            pattern = self.function_patterns[language]
            matches = re.findall(pattern, content, re.MULTILINE)

            # Process matches differently depending on the language
            if language in ["python", "go", "rust", "kotlin"]:
                functions = [match for match in matches if match]
            elif language in ["javascript", "typescript"]:
                # Extract named functions from multiple capture groups
                for match in matches:
                    if isinstance(match, tuple):
                        for group in match:
                            if group:
                                functions.append(group)
                    elif match:
                        functions.append(match)
            elif language == "java":
                # Extract the last capture group (method name)
                for match in matches:
                    if isinstance(match, tuple) and match[-1]:
                        functions.append(match[-1])

        # Remove duplicates and return
        return list(set(functions))

    def _extract_classes(self, content: str, language: str) -> list[str]:
        """Extract class definitions from the code.

        Args:
            content: File content
            language: Detected programming language

        Returns:
            A list of class names
        """
        classes = []

        if language in self.class_patterns:
            pattern = self.class_patterns[language]
            matches = re.findall(pattern, content, re.MULTILINE)

            # Process matches
            for match in matches:
                if isinstance(match, tuple):
                    for group in match:
                        if group:
                            classes.append(group)
                elif match:
                    classes.append(match)

        # Remove duplicates and return
        return list(set(classes))

    def _calculate_complexity(self, content: str, language: str) -> dict[str, int]:
        """Calculate code complexity metrics.

        This is a simplified implementation that calculates:
        - Line count
        - Comment percentage
        - Cyclomatic complexity estimation

        Args:
            content: File content
            language: Detected programming language

        Returns:
            A dictionary of complexity metrics
        """
        complexity = {
            "line_count": 0,
            "comment_percentage": 0,
            "cyclomatic_complexity": 0,
        }

        # Count lines of code (excluding empty lines)
        lines = content.split("\n")
        non_empty_lines = [line for line in lines if line.strip()]
        complexity["line_count"] = len(non_empty_lines)

        # Count comment lines (language-specific)
        comment_count = 0
        if language in ["python", "ruby", "shell"]:
            comment_count = len(
                [line for line in lines if line.strip().startswith("#")]
            )
        elif language in [
            "javascript",
            "typescript",
            "java",
            "c",
            "cpp",
            "csharp",
            "go",
            "swift",
            "kotlin",
        ]:
            # Single-line comments
            comment_count += len(
                [line for line in lines if line.strip().startswith("//")]
            )

            # Multi-line comments - simplified approach
            comment_blocks = re.findall(r"/\*.*?\*/", content, re.DOTALL)
            for block in comment_blocks:
                comment_count += block.count("\n") + 1

        # Calculate comment percentage
        if complexity["line_count"] > 0:
            complexity["comment_percentage"] = int(
                (comment_count / complexity["line_count"]) * 100
            )

        # Estimate cyclomatic complexity by counting decision points
        decision_points = 0

        # Language-specific patterns for branching structures
        if language in ["python"]:
            decision_points += len(
                re.findall(r"\bif\b|\bfor\b|\bwhile\b|\belif\b|\bexcept\b", content)
            )
        elif language in ["javascript", "typescript", "java", "c", "cpp", "csharp"]:
            decision_points += len(
                re.findall(r"\bif\b|\bfor\b|\bwhile\b|\bcase\b|\bcatch\b|\?", content)
            )
        elif language in ["go"]:
            decision_points += len(
                re.findall(r"\bif\b|\bfor\b|\bswitch\b|\bcase\b|\bselect\b", content)
            )
        elif language in ["ruby"]:
            decision_points += len(
                re.findall(
                    r"\bif\b|\bunless\b|\bwhile\b|\buntil\b|\bcase\b|\brescue\b",
                    content,
                )
            )

        complexity["cyclomatic_complexity"] = max(1, decision_points)

        return complexity

    def _generate_tags(
        self,
        language: str,
        imports: list[str],
        functions: list[str],
        classes: list[str],
        complexity: dict[str, int],
    ) -> list[str]:
        """Generate tags based on code analysis.

        Args:
            language: Detected programming language
            imports: List of imported modules
            functions: List of function names
            classes: List of class names
            complexity: Dictionary of complexity metrics

        Returns:
            A list of tags
        """
        tags = set()

        # Add language tag
        if language:
            tags.add(f"lang:{language}")

        # Add complexity tags
        line_count = complexity.get("line_count", 0)
        if line_count < 50:
            tags.add("size:small")
        elif line_count < 300:
            tags.add("size:medium")
        else:
            tags.add("size:large")

        # Add structural tags
        if classes:
            tags.add("has:classes")

        if len(functions) > 5:
            tags.add("has:many-functions")

        if complexity.get("comment_percentage", 0) > 15:
            tags.add("well-commented")

        if complexity.get("cyclomatic_complexity", 0) > 15:
            tags.add("complex")

        # Add framework/library-specific tags
        if language == "python":
            if any(imp for imp in imports if imp.startswith("django")):
                tags.add("framework:django")
            if any(imp for imp in imports if imp.startswith("flask")):
                tags.add("framework:flask")
            if any(
                imp
                for imp in imports
                if imp in ["numpy", "pandas", "sklearn", "tensorflow", "torch"]
            ):
                tags.add("data-science")

        elif language in ["javascript", "typescript"]:
            if any(imp for imp in imports if "react" in imp.lower()):
                tags.add("framework:react")
            if any(imp for imp in imports if "angular" in imp.lower()):
                tags.add("framework:angular")
            if any(imp for imp in imports if "vue" in imp.lower()):
                tags.add("framework:vue")

        return list(tags)



================================================
File: src/the_aichemist_codex/backend/metadata/database_extractor.py
================================================
"""
Database metadata extraction for The Aichemist Codex.

This module provides functionality to extract metadata from database files,
including schema information, table counts, column details, and other
database-specific metadata.
"""

# Standard library imports
import logging
import re
import sqlite3
from pathlib import Path
from typing import Any

# Third-party imports will be added via pip during implementation
# For now we'll implement with standard library for SQLite
# Later we can add sqlite-utils if needed
# Local application imports
from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.metadata.extractor import (
    BaseMetadataExtractor,
    MetadataExtractorRegistry,
)
from the_aichemist_codex.backend.utils.cache_manager import CacheManager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

# Initialize logger with proper name
logger = logging.getLogger(__name__)


class DatabaseMetadataExtractor(BaseMetadataExtractor):
    """
    Metadata extractor for database files.

    This extractor can process various database formats (SQLite, MySQL dumps,
    PostgreSQL dumps) and extract rich metadata including:
    - Database type and version
    - Schema information (tables, views, indexes)
    - Table statistics (row counts, column counts)
    - Size metrics and performance indicators
    - SQL statement analysis for dump files
    """

    # Supported MIME types
    SUPPORTED_MIME_TYPES = [
        "application/x-sqlite3",
        "application/vnd.sqlite3",
        "text/x-sql",
        "application/sql",
        "application/x-sql",
    ]

    # File extensions to format mappings
    FORMAT_MAP = {
        ".db": "sqlite",
        ".sqlite": "sqlite",
        ".sqlite3": "sqlite",
        ".sql": "sql_dump",
        ".mysql": "mysql_dump",
        ".pgsql": "postgresql_dump",
    }

    @property
    def supported_mime_types(self) -> list[str]:
        """Get the list of MIME types this extractor supports.

        Returns:
            List of supported MIME types
        """
        return self.SUPPORTED_MIME_TYPES

    def __init__(self, cache_manager: CacheManager | None = None) -> None:
        """Initialize the database metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        super().__init__(cache_manager)
        self.mime_detector = MimeTypeDetector()

    async def extract(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """Extract metadata from a database file.

        Args:
            file_path: Path to the database file
            content: File content (used for SQL dumps)
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            Dictionary containing extracted database metadata
        """
        path = Path(file_path)
        if not path.exists():
            logger.warning(f"Database file not found: {path}")
            return {}

        # Determine MIME type if not provided
        if mime_type is None:
            mime_type, _ = self.mime_detector.get_mime_type(path)

        # Check if this is a supported database type
        if mime_type not in self.supported_mime_types:
            # Try checking extension as fallback
            ext = path.suffix.lower()
            if ext not in self.FORMAT_MAP:
                logger.debug(f"Unsupported database type: {mime_type} for file: {path}")
                return {}

        # Try to use cache if available
        if self.cache_manager:
            cache_key = f"db_metadata:{path}:{path.stat().st_mtime}"
            cached_data = await self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Using cached database metadata for {path}")
                return cached_data

        # Process the database file
        try:
            result = await self._process_database(path, content)

            # Cache the result if cache manager is available
            if self.cache_manager:
                await self.cache_manager.put(cache_key, result)

            return result
        except Exception as e:
            logger.error(f"Error extracting database metadata from {path}: {str(e)}")
            return {}

    async def _process_database(
        self, path: Path, content: str | None
    ) -> dict[str, Any]:
        """Process a database file to extract metadata.

        Args:
            path: Path to the database file
            content: File content (used for SQL dumps)

        Returns:
            Dictionary containing extracted database metadata
        """
        result = {
            "metadata_type": "database",
            "format": {},
            "schema": {},
            "statistics": {},
        }

        # Determine database type from extension
        ext = path.suffix.lower()
        db_type = self.FORMAT_MAP.get(ext, "unknown")
        result["format"]["type"] = db_type

        # Process based on database type
        if db_type == "sqlite":
            self._process_sqlite(path, result)
        elif "dump" in db_type:
            self._process_sql_dump(path, content, db_type, result)

        return result

    def _process_sqlite(self, path: Path, result: dict[str, Any]) -> None:
        """Process SQLite database file and extract metadata.

        Args:
            path: Path to the SQLite database file
            result: Dictionary to update with extracted metadata
        """
        try:
            # Record basic file information
            result["format"]["file_size"] = path.stat().st_size

            # Connect to the database in read-only mode
            uri = f"file:{str(path)}?mode=ro"
            conn = sqlite3.connect(uri, uri=True)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # Get SQLite version
            cursor.execute("SELECT sqlite_version()")
            result["format"]["sqlite_version"] = cursor.fetchone()[0]

            # Get list of tables
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' "
                "AND name NOT LIKE 'sqlite_%' ORDER BY name"
            )
            tables = [row[0] for row in cursor.fetchall()]

            # Get list of views
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='view' ORDER BY name"
            )
            views = [row[0] for row in cursor.fetchall()]

            # Store basic schema information
            result["schema"]["tables"] = []
            result["schema"]["table_count"] = len(tables)
            result["schema"]["view_count"] = len(views)
            result["schema"]["total_objects"] = len(tables) + len(views)

            # Extract detailed table information
            total_rows = 0
            total_indexes = 0

            for table_name in tables:
                table_info = {
                    "name": table_name,
                    "columns": [],
                    "indexes": [],
                }

                # Get table columns
                cursor.execute(f"PRAGMA table_info('{table_name}')")
                columns = cursor.fetchall()

                for column in columns:
                    table_info["columns"].append(
                        {
                            "name": column["name"],
                            "type": column["type"],
                            "primary_key": bool(column["pk"]),
                            "not_null": bool(column["notnull"]),
                        }
                    )

                # Get row count
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM '{table_name}'")
                    row_count = cursor.fetchone()[0]
                    table_info["row_count"] = row_count
                    total_rows += row_count
                except sqlite3.Error:
                    # Table might be corrupt or virtual
                    table_info["row_count"] = 0

                # Get indexes
                cursor.execute(f"PRAGMA index_list('{table_name}')")
                indexes = cursor.fetchall()
                table_info["index_count"] = len(indexes)
                total_indexes += len(indexes)

                for idx in indexes:
                    # Get columns in the index
                    cursor.execute(f"PRAGMA index_info('{idx['name']}')")
                    index_columns = [row["name"] for row in cursor.fetchall()]

                    table_info["indexes"].append(
                        {
                            "name": idx["name"],
                            "columns": index_columns,
                            "unique": bool(idx["unique"]),
                        }
                    )

                result["schema"]["tables"].append(table_info)

            # Extract view information
            result["schema"]["views"] = []
            for view_name in views:
                # Get view definition
                cursor.execute(
                    "SELECT sql FROM sqlite_master WHERE type='view' AND name=?",
                    (view_name,),
                )
                view_def = cursor.fetchone()[0]

                result["schema"]["views"].append(
                    {
                        "name": view_name,
                        "definition": view_def,
                    }
                )

            # Add statistics
            result["statistics"]["total_rows"] = total_rows
            result["statistics"]["total_indexes"] = total_indexes
            result["statistics"]["avg_rows_per_table"] = (
                total_rows / len(tables) if tables else 0
            )

            # Database file size analysis
            bytes_per_row = (
                result["format"]["file_size"] / total_rows if total_rows > 0 else 0
            )
            result["statistics"]["bytes_per_row"] = round(bytes_per_row, 2)

            # Add user-friendly summary
            result["summary"] = (
                f"SQLite database with {len(tables)} tables, "
                f"{len(views)} views, and approximately {total_rows} total rows"
            )

            # Close connection
            conn.close()

        except Exception as e:
            logger.debug(f"Error processing SQLite database {path}: {str(e)}")
            # Add info about the error to the result
            result["error"] = f"Error processing SQLite database: {str(e)}"

    def _process_sql_dump(
        self, path: Path, content: str | None, db_type: str, result: dict[str, Any]
    ) -> None:
        """Process SQL dump file and extract metadata.

        Args:
            path: Path to the SQL dump file
            content: File content if already loaded
            db_type: Type of SQL dump (mysql_dump, postgresql_dump, etc.)
            result: Dictionary to update with extracted metadata
        """
        try:
            # Load content if not provided
            if content is None:
                with open(path, encoding="utf-8", errors="replace") as f:
                    content = f.read()

            # Record basic information
            result["format"]["file_size"] = path.stat().st_size
            result["format"]["dump_type"] = db_type

            # Extract information based on SQL statements
            if db_type == "mysql_dump":
                self._analyze_mysql_dump(content, result)
            elif db_type == "postgresql_dump":
                self._analyze_postgresql_dump(content, result)
            else:
                self._analyze_generic_sql(content, result)

        except Exception as e:
            logger.debug(f"Error processing SQL dump {path}: {str(e)}")
            # Add info about the error to the result
            result["error"] = f"Error processing SQL dump: {str(e)}"

    def _analyze_generic_sql(self, content: str, result: dict[str, Any]) -> None:
        """Analyze generic SQL content to extract metadata.

        Args:
            content: SQL content to analyze
            result: Dictionary to update with extracted metadata
        """
        # Extract CREATE TABLE statements
        create_table_pattern = re.compile(
            r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?[\`\[\"\']?(\w+)[\`\[\"\']?",
            re.IGNORECASE,
        )
        tables = create_table_pattern.findall(content)

        # Extract CREATE VIEW statements
        create_view_pattern = re.compile(
            r"CREATE\s+(?:OR\s+REPLACE\s+)?VIEW\s+[\`\[\"\']?(\w+)[\`\[\"\']?",
            re.IGNORECASE,
        )
        views = create_view_pattern.findall(content)

        # Extract INSERT statements
        insert_pattern = re.compile(r"INSERT\s+INTO", re.IGNORECASE)
        inserts = len(insert_pattern.findall(content))

        # Extract ALTER TABLE statements
        alter_pattern = re.compile(r"ALTER\s+TABLE", re.IGNORECASE)
        alters = len(alter_pattern.findall(content))

        # Try to detect database type from content
        db_type = "generic_sql"
        if "ENGINE=InnoDB" in content or "ENGINE=MyISAM" in content:
            db_type = "mysql"
        elif "PostgreSQL database dump" in content or "SET search_path =" in content:
            db_type = "postgresql"
        elif "BEGIN TRANSACTION;" in content and "COMMIT;" in content:
            db_type = "sqlite"

        # Record findings
        result["format"]["detected_type"] = db_type
        result["schema"]["table_count"] = len(tables)
        result["schema"]["view_count"] = len(views)
        result["schema"]["tables"] = tables
        result["schema"]["views"] = views

        result["statistics"]["insert_statements"] = inserts
        result["statistics"]["alter_statements"] = alters
        result["statistics"]["total_statements"] = (
            len(tables) + len(views) + inserts + alters
        )

        # Add user-friendly summary
        result["summary"] = (
            f"SQL dump with {len(tables)} table definitions, "
            f"{len(views)} view definitions, and {inserts} insert statements"
        )

    def _analyze_mysql_dump(self, content: str, result: dict[str, Any]) -> None:
        """Analyze MySQL dump content to extract metadata.

        Args:
            content: MySQL dump content to analyze
            result: Dictionary to update with extracted metadata
        """
        # First run generic SQL analysis
        self._analyze_generic_sql(content, result)

        # Extract MySQL-specific information

        # Try to extract MySQL version
        version_pattern = re.compile(r"-- Server version\s+(\d+\.\d+\.\d+)")
        version_match = version_pattern.search(content)
        if version_match:
            result["format"]["mysql_version"] = version_match.group(1)

        # Look for database name
        db_name_pattern = re.compile(r"-- Current Database: `([^`]+)`")
        db_name_match = db_name_pattern.search(content)
        if db_name_match:
            result["format"]["database_name"] = db_name_match.group(1)

        # Extract character set and collation
        charset_pattern = re.compile(r"DEFAULT CHARSET=(\w+)")
        charset_matches = charset_pattern.findall(content)
        if charset_matches:
            # Use the most common charset
            from collections import Counter

            charset_counts = Counter(charset_matches)
            result["format"]["default_charset"] = charset_counts.most_common(1)[0][0]

        # Update the detected type
        result["format"]["detected_type"] = "mysql"

        # Update summary
        if "database_name" in result["format"]:
            result["summary"] = (
                f"MySQL dump of database '{result['format']['database_name']}' "
                f"with {result['schema']['table_count']} tables, "
                f"{result['schema']['view_count']} views, and "
                f"{result['statistics']['insert_statements']} insert statements"
            )
        else:
            result["summary"] = (
                f"MySQL dump with {result['schema']['table_count']} tables, "
                f"{result['schema']['view_count']} views, and "
                f"{result['statistics']['insert_statements']} insert statements"
            )

    def _analyze_postgresql_dump(self, content: str, result: dict[str, Any]) -> None:
        """Analyze PostgreSQL dump content to extract metadata.

        Args:
            content: PostgreSQL dump content to analyze
            result: Dictionary to update with extracted metadata
        """
        # First run generic SQL analysis
        self._analyze_generic_sql(content, result)

        # Extract PostgreSQL-specific information

        # Try to extract PostgreSQL version
        version_pattern = re.compile(
            r"-- PostgreSQL database dump\s*\n--\s*\n-- PostgreSQL version (\d+\.\d+)"
        )
        version_match = version_pattern.search(content)
        if version_match:
            result["format"]["postgresql_version"] = version_match.group(1)

        # Look for database name
        db_name_pattern = re.compile(r"-- Name: ([^;]+);")
        db_name_match = db_name_pattern.search(content)
        if db_name_match:
            result["format"]["database_name"] = db_name_match.group(1).strip()

        # Extract schemas
        schema_pattern = re.compile(r"CREATE SCHEMA (?:IF NOT EXISTS )?([^\s;]+)")
        schemas = schema_pattern.findall(content)
        result["schema"]["schemas"] = schemas
        result["schema"]["schema_count"] = len(schemas)

        # Look for extensions
        extension_pattern = re.compile(r"CREATE EXTENSION (?:IF NOT EXISTS )?([^\s;]+)")
        extensions = extension_pattern.findall(content)
        result["schema"]["extensions"] = extensions
        result["schema"]["extension_count"] = len(extensions)

        # Update the detected type
        result["format"]["detected_type"] = "postgresql"

        # Update summary
        if "database_name" in result["format"]:
            result["summary"] = (
                f"PostgreSQL dump of database '{result['format']['database_name']}' "
                f"with {result['schema']['table_count']} tables, "
                f"{result['schema']['view_count']} views"
            )
        else:
            result["summary"] = (
                f"PostgreSQL dump with {result['schema']['table_count']} tables, "
                f"{result['schema']['view_count']} views"
            )


# Register the extractor
MetadataExtractorRegistry.register(DatabaseMetadataExtractor)



================================================
File: src/the_aichemist_codex/backend/metadata/document_extractor.py
================================================
"""Document metadata extractor for extracting information from document files.

This module provides functionality for analyzing document content to extract
titles, authors, dates, and other metadata from document files.
"""

# mypy: disable-error-code="return-value"

import logging
import re
import time
from pathlib import Path
from typing import Any

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

from .extractor import BaseMetadataExtractor, MetadataExtractorRegistry

logger = logging.getLogger(__name__)


@MetadataExtractorRegistry.register
class DocumentMetadataExtractor(BaseMetadataExtractor):
    """Metadata extractor for document files.

    Analyzes document content to extract author information, titles, creation dates,
    page counts, and other document-specific metadata.
    """

    def __init__(self, cache_manager: CacheManager | None = None):
        """Initialize the document metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        super().__init__(cache_manager)

        # Common patterns for extracting document metadata
        self.author_patterns = [
            r"Author:?\s*([^,\r\n]+)",
            r"by\s+([^,\r\n]+)",
            r"created\s+by\s+([^,\r\n]+)",
        ]

        self.title_patterns = [
            r"Title:?\s*([^\r\n]+)",
            r"Subject:?\s*([^\r\n]+)",
            r"Document\s+Name:?\s*([^\r\n]+)",
        ]

        self.date_patterns = [
            r"Date:?\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})",
            r"Created:?\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})",
            r"Modified:?\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})",
            r"(\d{1,2}\s+\w+\s+\d{2,4})",  # 15 January 2023 format
        ]

        self.version_patterns = [
            r"Version:?\s*([\d.]+)",
            r"Rev(?:ision)?:?\s*([\d.]+)",
            r"v([\d.]+)",
        ]

    @property
    def supported_mime_types(self) -> list[str]:
        """List of MIME types supported by this extractor."""
        return [
            "application/pdf",
            "application/msword",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            "application/vnd.ms-excel",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-powerpoint",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation",
            "application/epub+zip",
            "application/rtf",
            "text/markdown",
            "text/plain",
            "application/x-latex",
        ]

    async def extract(  # type: ignore
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """Extract metadata from a document file.

        Args:
            file_path: Path to the file
            content: Optional pre-loaded file content
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            A dictionary containing extracted metadata
        """
        start_time = time.time()

        # Check if we have cached results
        if self.cache_manager and hasattr(self.cache_manager, "get"):
            cache_key = f"document_metadata:{file_path}"
            # Properly await the async cache manager get method
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result and isinstance(cached_result, dict):
                logger.debug(f"Using cached metadata for {file_path}")
                return cached_result  # type: ignore

        # Convert file_path to Path object
        if isinstance(file_path, str):
            file_path = Path(file_path)

        # If this is a binary document format, use specialized extraction if available
        # For this implementation, we'll focus on text-based documents

        # Get the content if not provided
        if content is None:
            content = await self._get_content(file_path)  # type: ignore
            if not content:
                return {
                    "error": "Failed to read file content",
                    "extraction_complete": False,
                    "extraction_confidence": 0.0,
                    "extraction_time": time.time() - start_time,
                }

        # Extract document metadata
        extracted_data: dict[str, Any] = {}

        # Try to extract basic document metadata using regular expressions
        authors = self._extract_authors(content)
        extracted_data["authors"] = authors

        title = self._extract_title(content, file_path)
        extracted_data["title"] = title

        date = self._extract_date(content)
        extracted_data["date"] = date

        version = self._extract_version(content)
        extracted_data["version"] = version

        # Calculate document statistics
        statistics = self._calculate_statistics(content)
        extracted_data["statistics"] = statistics

        # Extract keywords and topics if this is a text document
        if mime_type and mime_type.startswith("text/"):
            # For simplicity, we'll use a basic keyword extraction
            keywords = self._extract_keywords(content)
            extracted_data["keywords"] = keywords

        # Mark extraction as complete
        extracted_data["extraction_complete"] = True
        extracted_data["extraction_confidence"] = 0.7  # Reasonable default
        extracted_data["extraction_time"] = time.time() - start_time

        # Cache the results if we have a cache manager
        if self.cache_manager and hasattr(self.cache_manager, "put"):
            await self.cache_manager.put(cache_key, extracted_data)  # type: ignore

        return extracted_data

    def _extract_authors(self, content: str) -> list[str]:
        """Extract author information from document content.

        Args:
            content: Document content

        Returns:
            A list of detected authors
        """
        authors = []
        for pattern in self.author_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            authors.extend([match.strip() for match in matches if match.strip()])

        # Remove duplicates and return
        return list(set(authors))

    def _extract_title(self, content: str, file_path: Path) -> str:
        """Extract title from document content.

        Args:
            content: Document content
            file_path: Path to the file (used as fallback)

        Returns:
            The extracted title or filename if no title found
        """
        # Try to find a title in the content
        for pattern in self.title_patterns:
            matches = re.search(pattern, content, re.IGNORECASE)
            if matches and matches.group(1).strip():
                return matches.group(1).strip()

        # For markdown files, try to extract the first heading
        if file_path.suffix.lower() in [".md", ".markdown"]:
            md_title_match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
            if md_title_match:
                return md_title_match.group(1).strip()

        # If no title found, use the filename (without extension) as a fallback
        return file_path.stem

    def _extract_date(self, content: str) -> str:
        """Extract date from document content.

        Args:
            content: Document content

        Returns:
            The extracted date string or empty string if no date found
        """
        # Extract all dates from the content
        found_dates: list[str] = []
        try:
            # Look for dates in various formats
            date_patterns = [
                r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b",  # MM/DD/YYYY or DD-MM-YYYY
                r"\b\d{4}[/-]\d{1,2}[/-]\d{1,2}\b",  # YYYY-MM-DD
                r"\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b",  # Month DD, YYYY
            ]

            for pattern in date_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                found_dates.extend(matches)

            # Look for a creation date specifically
            creation_match = re.search(
                r"(created|date|published).*?(\d+[/-]\d+[/-]\d+|\d+\s+\w+\s+\d+)",
                content,
                re.IGNORECASE,
            )
            if creation_match:
                return creation_match.group(2).strip()
            elif found_dates:
                # Use the first date found as the creation date
                return found_dates[0]

        except Exception as e:
            logger.error(f"Error extracting date: {e}")

        return ""

    def _extract_version(self, content: str) -> str:
        """Extract version information from document content.

        Args:
            content: Document content

        Returns:
            The extracted version string or empty string if not found
        """
        for pattern in self.version_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return ""

    def _calculate_statistics(self, content: str) -> dict[str, int]:
        """Calculate document statistics.

        Args:
            content: Document content

        Returns:
            A dictionary of document statistics
        """
        stats = {}

        # Word count
        words = re.findall(r"\b\w+\b", content)
        stats["word_count"] = len(words)

        # Page count estimate (assuming ~500 words per page)
        stats["estimated_page_count"] = max(1, len(words) // 500)

        # Character count
        stats["character_count"] = len(content)

        # Paragraph count (estimated by double newlines)
        paragraphs = re.split(r"\n\s*\n", content)
        stats["paragraph_count"] = len(paragraphs)

        # Section count (estimated by headings in markdown or similar docs)
        headings = re.findall(r"^#+\s+", content, re.MULTILINE)
        stats["section_count"] = len(headings) if headings else 0

        return stats

    def _extract_keywords(self, content: str) -> list[str]:
        """Extract keywords from document content.

        Args:
            content: Document content

        Returns:
            A list of extracted keywords
        """
        # For simplicity, we'll use a basic keyword extraction
        words = re.findall(r"\b\w+\b", content)
        return [word.lower() for word in words if len(word) > 3]



================================================
File: src/the_aichemist_codex/backend/metadata/extractor.py
================================================
"""Base classes for metadata extraction.

This module provides the foundation for all metadata extractors, including
the base class and registry for dynamic extractor discovery and selection.
"""

import logging
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

logger = logging.getLogger(__name__)


class BaseMetadataExtractor(ABC):
    """Base class for all metadata extractors.

    Metadata extractors analyze file content to extract meaningful information
    such as keywords, topics, entities, and other content-based metadata.
    """

    def __init__(self, cache_manager: CacheManager | None = None):
        """Initialize the metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        self.cache_manager = cache_manager

    @abstractmethod
    async def extract(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """Extract metadata from a file.

        This method must be implemented by all concrete extractor classes.

        Args:
            file_path: Path to the file
            content: Optional pre-loaded file content
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            A dictionary containing extracted metadata
        """
        pass

    @property
    @abstractmethod
    def supported_mime_types(self) -> list[str]:
        """List of MIME types supported by this extractor.

        Returns:
            list[str]: A list of MIME type strings that this extractor can handle
        """
        pass

    async def _get_content(self, file_path: str | Path) -> str:
        """Read the content of a file.

        Args:
            file_path: Path to the file

        Returns:
            The file content as a string
        """
        try:
            # Convert to Path and ensure correct method signature
            path = Path(file_path)
            return await AsyncFileIO.read_text(path)
        except UnicodeDecodeError:
            # Try with a different encoding if UTF-8 fails
            # Note: We can't specify encoding in AsyncFileIO.read, so this fallback may not work
            logger.warning(
                f"UnicodeDecodeError when reading {file_path}, using default encoding"
            )
            path = Path(file_path)
            return await AsyncFileIO.read_text(path)
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            return ""


class MetadataExtractorRegistry:
    """Registry for metadata extractors.

    This class keeps track of all available extractors and helps select
    the appropriate extractor for a given file.
    """

    _extractors: dict[str, type[BaseMetadataExtractor]] = {}
    _mime_type_map: dict[str, type[BaseMetadataExtractor]] = {}

    @classmethod
    def register(
        cls, extractor_class: type[BaseMetadataExtractor]
    ) -> type[BaseMetadataExtractor]:
        """Register a metadata extractor.

        This method can be used as a decorator to register extractor classes.

        Args:
            extractor_class: The extractor class to register

        Returns:
            The registered extractor class
        """
        cls._extractors[extractor_class.__name__] = extractor_class

        # Map MIME types to this extractor
        # We need to create an instance to access instance properties
        try:
            # Create a temporary instance to access the property
            temp_instance = extractor_class(None)  # Pass None as cache_manager
            # Get the supported mime types from the instance
            mime_types = temp_instance.supported_mime_types

            # Make sure we have a valid list
            if not isinstance(mime_types, list):
                logger.warning(
                    f"supported_mime_types from {extractor_class.__name__} is not a list: {mime_types}"
                )
                mime_types = list(mime_types) if hasattr(mime_types, "__iter__") else []

            # Register the extractor for each supported MIME type
            for mime_type in mime_types:
                cls._mime_type_map[mime_type] = extractor_class

        except Exception as e:
            logger.error(
                f"Error registering MIME types for {extractor_class.__name__}: {e}"
            )

        return extractor_class

    @classmethod
    def get_extractor_for_mime_type(
        cls, mime_type: str
    ) -> type[BaseMetadataExtractor] | None:
        """Get the appropriate extractor for a MIME type.

        Args:
            mime_type: The MIME type to find an extractor for

        Returns:
            An extractor class that supports the given MIME type, or None if not found
        """
        # Try exact match first
        if mime_type in cls._mime_type_map:
            return cls._mime_type_map[mime_type]

        # Try matching main type (e.g., "text/*")
        main_type = mime_type.split("/")[0] + "/*"
        if main_type in cls._mime_type_map:
            return cls._mime_type_map[main_type]

        return None

    @classmethod
    def get_all_extractors(cls) -> list[type[BaseMetadataExtractor]]:
        """Get all registered extractors.

        Returns:
            A list of all registered extractor classes
        """
        return list(cls._extractors.values())



================================================
File: src/the_aichemist_codex/backend/metadata/image_extractor.py
================================================
"""
Image metadata extraction for The Aichemist Codex.

This module provides functionality to extract metadata from image files,
including EXIF data, image dimensions, color profiles, and other image-specific
information.
"""

import logging
from pathlib import Path
from typing import Any

from PIL import Image
from PIL.ExifTags import GPSTAGS, TAGS

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.metadata.extractor import (
    BaseMetadataExtractor,
    MetadataExtractorRegistry,
)
from the_aichemist_codex.backend.utils.cache_manager import CacheManager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

logger = logging.getLogger(__name__)


class ImageMetadataExtractor(BaseMetadataExtractor):
    """
    Metadata extractor for image files.

    This extractor can process various image formats (JPEG, PNG, TIFF, etc.)
    and extract rich metadata including:
    - EXIF data (camera settings, date/time, GPS, etc.)
    - Image dimensions and resolution
    - Color mode and bit depth
    - Format-specific information
    """

    # Supported MIME types
    SUPPORTED_MIME_TYPES = [
        "image/jpeg",
        "image/jpg",
        "image/png",
        "image/tiff",
        "image/gif",
        "image/bmp",
        "image/webp",
    ]

    def __init__(self, cache_manager: CacheManager | None = None):
        """Initialize the image metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        super().__init__(cache_manager)
        self.mime_detector = MimeTypeDetector()

    @property
    def supported_mime_types(self) -> list[str]:
        """List of MIME types supported by this extractor.

        Returns:
            list[str]: A list of MIME type strings that this extractor can handle
        """
        return self.SUPPORTED_MIME_TYPES

    async def extract(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """
        Extract metadata from an image file.

        Args:
            file_path: Path to the image file
            content: Not used for image files
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            Dictionary containing extracted image metadata
        """
        path = Path(file_path)
        if not path.exists():
            logger.warning(f"Image file not found: {path}")
            return {}

        # Determine MIME type if not provided
        if mime_type is None:
            mime_type, _ = self.mime_detector.get_mime_type(path)

        # Check if this is a supported image type
        if mime_type not in self.SUPPORTED_MIME_TYPES:
            logger.debug(f"Unsupported image type: {mime_type} for file: {path}")
            return {}

        # Try to use cache if available
        if self.cache_manager:
            cache_key = f"image_metadata:{path}:{path.stat().st_mtime}"
            cached_data = await self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Using cached image metadata for {path}")
                return cached_data

        # Process the image file
        try:
            result = await self._process_image(path)

            # Cache the result if cache manager is available
            if self.cache_manager:
                await self.cache_manager.put(cache_key, result)

            return result
        except Exception as e:
            logger.error(f"Error extracting image metadata from {path}: {str(e)}")
            return {}

    async def _process_image(self, path: Path) -> dict[str, Any]:
        """
        Process an image file to extract metadata.

        Args:
            path: Path to the image file

        Returns:
            Dictionary containing extracted image metadata
        """
        result = {"metadata_type": "image", "dimensions": {}, "exif": {}, "gps": {}}

        try:
            with Image.open(path) as img:
                # Basic image information
                result["format"] = img.format
                result["mode"] = img.mode
                result["dimensions"] = {
                    "width": img.width,
                    "height": img.height,
                    "aspect_ratio": round(img.width / img.height, 3)
                    if img.height > 0
                    else None,
                }

                # Get resolution if available
                if hasattr(img, "info") and "dpi" in img.info:
                    result["resolution"] = {
                        "dpi_x": img.info["dpi"][0],
                        "dpi_y": img.info["dpi"][1],
                    }

                # Get ICC profile information if available
                if "icc_profile" in img.info:
                    result["has_color_profile"] = True

                # Extract EXIF data if available
                exif_data = self._extract_exif(img)
                if exif_data:
                    result["exif"] = exif_data

                # Check for animation in GIFs - using safe attribute access
                if (
                    img.format == "GIF"
                    and hasattr(img, "is_animated")
                    and getattr(img, "is_animated", False)
                ):
                    # For animated GIFs, try to get frame count
                    frame_count = getattr(img, "n_frames", 0)
                    if frame_count > 0:
                        result["animation"] = {
                            "frame_count": frame_count,
                            "duration": img.info.get("duration", 0),
                        }

                # Extract transparency information
                if "transparency" in img.info:
                    result["has_transparency"] = True
                elif img.mode in ("RGBA", "LA"):
                    result["has_transparency"] = True

                # Check for thumbnail in JPEG
                if img.format == "JPEG" and hasattr(img, "_getexif"):
                    # Use safe method to get EXIF data
                    exif_info = self._safe_get_exif(img)
                    if exif_info and 40094 in exif_info:  # Embedded thumbnail tag
                        result["has_thumbnail"] = True

        except Exception as e:
            logger.error(f"Error processing image {path}: {str(e)}")

        return result

    def _safe_get_exif(self, img: Image.Image) -> dict[Any, Any] | None:
        """
        Safely get EXIF data from an image.

        Args:
            img: PIL Image object

        Returns:
            Dictionary containing EXIF data or None if unavailable
        """
        try:
            if hasattr(img, "_getexif"):
                exif_method = getattr(img, "_getexif", None)
                if callable(exif_method):
                    return exif_method()
            return None
        except Exception as e:
            logger.debug(f"Error getting EXIF data: {str(e)}")
            return None

    def _extract_exif(self, img: Image.Image) -> dict[str, Any]:
        """
        Extract EXIF metadata from an image.

        Args:
            img: PIL Image object

        Returns:
            Dictionary containing EXIF metadata
        """
        exif_data = {}
        gps_data = {}

        # First check if image has exif data
        exif_info = self._safe_get_exif(img)
        if exif_info:
            # Process standard EXIF tags
            for tag_id, value in exif_info.items():
                tag = TAGS.get(tag_id, str(tag_id))

                # Skip binary data and oversized tag values
                if isinstance(value, bytes) and len(value) > 100:
                    exif_data[tag] = f"Binary data ({len(value)} bytes)"
                    continue

                # Handle special tags
                if tag == "GPSInfo" and isinstance(value, dict):
                    for gps_tag_id, gps_value in value.items():
                        gps_tag = GPSTAGS.get(gps_tag_id, str(gps_tag_id))
                        gps_data[gps_tag] = gps_value
                else:
                    # Ensure the value is serializable
                    if isinstance(value, (int, float, str, bool, list, dict, tuple)):
                        exif_data[tag] = value
                    else:
                        exif_data[tag] = str(value)

        # Extract basic date/time info if available
        if "DateTime" in exif_data:
            exif_data["capture_datetime"] = exif_data["DateTime"]

        # Process camera information
        camera_info = {}
        if "Make" in exif_data:
            camera_info["make"] = exif_data["Make"]
        if "Model" in exif_data:
            camera_info["model"] = exif_data["Model"]
        if camera_info:
            exif_data["camera"] = camera_info

        # Process exposure information
        exposure_info = {}
        if "ExposureTime" in exif_data:
            exposure_info["exposure_time"] = exif_data["ExposureTime"]
        if "FNumber" in exif_data:
            exposure_info["f_number"] = exif_data["FNumber"]
        if "ISOSpeedRatings" in exif_data:
            exposure_info["iso"] = exif_data["ISOSpeedRatings"]
        if "ExposureProgram" in exif_data:
            exposure_info["program"] = exif_data["ExposureProgram"]
        if exposure_info:
            exif_data["exposure"] = exposure_info

        # Process lens information
        lens_info = {}
        if "LensMake" in exif_data:
            lens_info["make"] = exif_data["LensMake"]
        if "LensModel" in exif_data:
            lens_info["model"] = exif_data["LensModel"]
        if "FocalLength" in exif_data:
            lens_info["focal_length"] = exif_data["FocalLength"]
        if lens_info:
            exif_data["lens"] = lens_info

        # Format GPS data in a more useful way if it exists
        if gps_data:
            formatted_gps = self._format_gps_data(gps_data)
            if formatted_gps:
                exif_data["gps"] = formatted_gps

        return exif_data

    def _format_gps_data(self, gps_data: dict[str, Any]) -> dict[str, Any]:
        """
        Format GPS data into a more usable structure.

        Args:
            gps_data: Dictionary of raw GPS data from EXIF

        Returns:
            Dictionary with formatted GPS information
        """
        formatted_gps = {}

        # Extract latitude
        if "GPSLatitude" in gps_data and "GPSLatitudeRef" in gps_data:
            try:
                lat = self._convert_gps_coordinate(gps_data["GPSLatitude"])
                lat_ref = gps_data["GPSLatitudeRef"]
                if lat_ref == "S":
                    lat = -lat
                formatted_gps["latitude"] = lat
            except Exception as e:
                logger.debug(f"Error processing latitude: {str(e)}")

        # Extract longitude
        if "GPSLongitude" in gps_data and "GPSLongitudeRef" in gps_data:
            try:
                lon = self._convert_gps_coordinate(gps_data["GPSLongitude"])
                lon_ref = gps_data["GPSLongitudeRef"]
                if lon_ref == "W":
                    lon = -lon
                formatted_gps["longitude"] = lon
            except Exception as e:
                logger.debug(f"Error processing longitude: {str(e)}")

        # Extract altitude
        if "GPSAltitude" in gps_data and "GPSAltitudeRef" in gps_data:
            try:
                alt = float(gps_data["GPSAltitude"].numerator) / float(
                    gps_data["GPSAltitude"].denominator
                )
                alt_ref = gps_data["GPSAltitudeRef"]
                if alt_ref == 1:
                    alt = -alt
                formatted_gps["altitude"] = alt
            except Exception as e:
                logger.debug(f"Error processing altitude: {str(e)}")

        return formatted_gps

    def _convert_gps_coordinate(self, coordinate) -> float:
        """
        Convert GPS coordinates from the EXIF format to decimal degrees.

        Args:
            coordinate: GPS coordinate in EXIF format (degrees, minutes, seconds)

        Returns:
            Float representing the coordinate in decimal degrees
        """
        degrees = float(coordinate[0].numerator) / float(coordinate[0].denominator)
        minutes = float(coordinate[1].numerator) / float(coordinate[1].denominator)
        seconds = float(coordinate[2].numerator) / float(coordinate[2].denominator)

        return degrees + (minutes / 60.0) + (seconds / 3600.0)


# Register the extractor
MetadataExtractorRegistry.register(ImageMetadataExtractor)



================================================
File: src/the_aichemist_codex/backend/metadata/manager.py
================================================
"""Metadata extraction manager.

This module provides a central manager for coordinating metadata extraction
across different file types using appropriate extractors.
"""

import asyncio
import logging
import time
from pathlib import Path
from typing import Any, cast

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.utils.cache_manager import CacheManager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

from .extractor import BaseMetadataExtractor, MetadataExtractorRegistry

logger = logging.getLogger(__name__)


class MetadataManager:
    """Manager for coordinating metadata extraction.

    This class orchestrates the extraction of metadata from files using
    the appropriate extractors based on file type.
    """

    def __init__(self, cache_manager: CacheManager | None = None):
        """Initialize the metadata manager.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        self.cache_manager = cache_manager
        self.mime_detector = MimeTypeDetector()

        # Initialize all available extractors
        self.extractors: dict[str, BaseMetadataExtractor] = {}
        for extractor_class in MetadataExtractorRegistry.get_all_extractors():
            extractor = extractor_class(cache_manager)
            extractor_name = extractor_class.__name__
            self.extractors[extractor_name] = extractor

        logger.info(f"Initialized {len(self.extractors)} metadata extractors")

    async def extract_metadata(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> FileMetadata:
        """Extract metadata from a file.

        Args:
            file_path: Path to the file
            content: Optional pre-loaded file content
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            FileMetadata object with extracted metadata
        """
        start_time = time.time()

        # Convert file_path to Path object
        if isinstance(file_path, str):
            file_path = Path(file_path)

        # Ensure the file exists
        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return FileMetadata(
                path=file_path,  # Use Path object directly
                mime_type="",  # Empty string as default
                size=0,
                extension="",
                preview="",
                error="File not found",
                extraction_complete=False,
            )

        # Create a new metadata object if one was not provided
        if metadata is None:
            metadata = FileMetadata(
                path=file_path,  # Use Path object directly
                mime_type="",  # Will be updated below
                size=0,  # Will be updated below
                extension="",  # Will be updated below
                preview="",  # Default empty preview
            )

        # Detect MIME type if not provided
        detected_mime_type = ""
        if mime_type is None:
            try:
                # The get_mime_type method returns a tuple (mime_type, confidence)
                mime_type_result = self.mime_detector.get_mime_type(file_path)
                detected_mime_type = mime_type_result[0] or "application/octet-stream"
            except Exception as e:
                logger.error(f"Error detecting MIME type: {e}")
                detected_mime_type = "application/octet-stream"  # Default MIME type

            # Use the detected mime type
            mime_type = detected_mime_type

        # Assign mime_type with proper null check
        if mime_type is not None:
            metadata.mime_type = mime_type
        else:
            metadata.mime_type = "application/octet-stream"  # Default MIME type

        # Set basic file attributes
        metadata.size = file_path.stat().st_size
        metadata.extension = file_path.suffix.lower().lstrip(".")

        # Get appropriate extractors for this file type
        extractors = self._get_extractors_for_mime_type(metadata.mime_type)

        if not extractors:
            logger.warning(
                f"No suitable metadata extractors found for {metadata.mime_type}"
            )
            metadata.extraction_complete = False
            metadata.extraction_confidence = 0.0
            return metadata

        # Extract metadata using all applicable extractors
        try:
            # Run extractors in parallel
            extraction_tasks = []
            for extractor in extractors:
                extraction_tasks.append(
                    extractor.extract(file_path, content, metadata.mime_type, metadata)
                )

            # Await all extraction results
            extraction_results = await asyncio.gather(
                *extraction_tasks, return_exceptions=True
            )

            # Process results
            combined_metadata: dict[str, Any] = {}
            extraction_count = 0
            confidence_sum = 0.0

            for result in extraction_results:
                # Skip exceptions in the results
                if isinstance(result, Exception):
                    logger.error(f"Extraction error: {result}")
                    continue

                # Process only dictionary results, cast to ensure type checking
                result_dict = cast(dict[str, Any], result)

                # Merge this extractor's result into the combined metadata
                if result_dict.get("extraction_complete", False):
                    extraction_count += 1
                    confidence_sum += result_dict.get("extraction_confidence", 0.0)

                    # Update with all extracted fields
                    for key, value in result_dict.items():
                        if key not in [
                            "error",
                            "extraction_complete",
                            "extraction_confidence",
                            "extraction_time",
                        ]:
                            # Merge lists
                            if (
                                isinstance(value, list)
                                and key in combined_metadata
                                and isinstance(combined_metadata[key], list)
                            ):
                                combined_metadata[key] = list(
                                    set(combined_metadata[key] + value)
                                )
                            # Merge dictionaries
                            elif (
                                isinstance(value, dict)
                                and key in combined_metadata
                                and isinstance(combined_metadata[key], dict)
                            ):
                                combined_metadata[key].update(value)
                            # Otherwise just overwrite
                            else:
                                combined_metadata[key] = value

            # Calculate average confidence
            avg_confidence = (
                confidence_sum / extraction_count if extraction_count > 0 else 0.0
            )

            # Update metadata object with extracted data
            for key, value in combined_metadata.items():
                if hasattr(metadata, key):
                    setattr(metadata, key, value)

            # Set extraction metadata
            metadata.extraction_complete = extraction_count > 0
            metadata.extraction_confidence = avg_confidence
            metadata.extraction_time = time.time() - start_time

            return metadata

        except Exception as e:
            logger.error(f"Error extracting metadata for {file_path}: {e}")
            metadata.error = str(e)
            metadata.extraction_complete = False
            metadata.extraction_confidence = 0.0
            return metadata

    async def extract_batch(
        self, file_paths: list[str | Path], max_concurrent: int = 5
    ) -> list[FileMetadata]:
        """Extract metadata from multiple files in parallel.

        Args:
            file_paths: List of paths to files
            max_concurrent: Maximum number of concurrent extractions

        Returns:
            List of FileMetadata objects with extracted metadata
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def _extract_with_semaphore(file_path: str | Path) -> FileMetadata:
            async with semaphore:
                return await self.extract_metadata(file_path)

        tasks = [_extract_with_semaphore(path) for path in file_paths]
        return await asyncio.gather(*tasks)

    def _get_extractors_for_mime_type(
        self, mime_type: str
    ) -> list[BaseMetadataExtractor]:
        """Get appropriate extractors for a given MIME type.

        Args:
            mime_type: MIME type of the file

        Returns:
            List of extractors capable of processing this file type
        """
        extractors = []

        # Try to get a specific extractor
        extractor_class = MetadataExtractorRegistry.get_extractor_for_mime_type(
            mime_type
        )
        if extractor_class:
            extractor_name = extractor_class.__name__
            if extractor_name in self.extractors:
                extractors.append(self.extractors[extractor_name])

        # If no specific extractor was found, try with more generic extractors
        if not extractors:
            # Extract main type (e.g., "text" from "text/plain")
            main_type = mime_type.split("/")[0] + "/*"

            extractor_class = MetadataExtractorRegistry.get_extractor_for_mime_type(
                main_type
            )
            if extractor_class:
                extractor_name = extractor_class.__name__
                if extractor_name in self.extractors:
                    extractors.append(self.extractors[extractor_name])

        return extractors



================================================
File: src/the_aichemist_codex/backend/metadata/pdf_extractor.py
================================================
"""
Module for extracting metadata from PDF files.

This module provides functionality for extracting metadata from PDF files,
including basic document properties, content structure, and embedded resources.
"""

import logging
import typing as t
from pathlib import Path

# Third-party imports - pypdf
try:
    from pypdf import PdfReader
    from pypdf.errors import PdfReadError

    PYPDF_AVAILABLE = True
except ImportError:
    PYPDF_AVAILABLE = False
    PdfReader = None
    PdfReadError = Exception  # Fallback type

# Local application imports
from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.metadata.extractor import (
    BaseMetadataExtractor,
    MetadataExtractorRegistry,
)
from the_aichemist_codex.backend.utils.cache_manager import CacheManager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

logger = logging.getLogger(__name__)


@MetadataExtractorRegistry.register
class PDFMetadataExtractor(BaseMetadataExtractor):
    """
    Extractor for PDF file metadata.

    This class handles the extraction of metadata from PDF files, including:
    - Document properties (title, author, creation date)
    - Page count and dimensions
    - Text content analysis
    - Embedded resources (images, fonts)
    - Security settings
    - PDF version information
    """

    # Supported MIME types
    SUPPORTED_MIME_TYPES = ["application/pdf"]

    # File extensions to MIME types mapping
    FILE_EXTENSIONS = {".pdf": "application/pdf"}

    def __init__(self, cache_manager: CacheManager | None = None) -> None:
        """
        Initialize the PDF metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extracted metadata.
        """
        super().__init__(cache_manager)
        self.mime_detector = MimeTypeDetector()

    @property
    def supported_mime_types(self) -> list[str]:
        """Get the list of MIME types this extractor supports.

        Returns:
            List of supported MIME types
        """
        return self.SUPPORTED_MIME_TYPES

    def _extract_pdf_with_pypdf(self, file_path: str) -> dict[str, t.Any]:
        """
        Extract metadata from PDF files using pypdf.

        Args:
            file_path: Path to the PDF file.

        Returns:
            Dictionary containing extracted metadata.

        Raises:
            PdfReadError: If the PDF file cannot be read or is corrupted.
        """
        # Check that PdfReader is available before using it
        if not PYPDF_AVAILABLE or PdfReader is None:
            raise ImportError("pypdf is required for PDF metadata extraction")

        try:
            pdf = PdfReader(file_path)

            # Initialize metadata dictionary
            metadata = {
                "pages": len(pdf.pages),
                "pdf_version": pdf.pdf_header
                if hasattr(pdf, "pdf_header")
                else "Unknown",
                "page_layout": None,
                "author": None,
                "creation_date": None,
                "modification_date": None,
                "producer": None,
                "creator": None,
                "title": None,
                "subject": None,
                "keywords": [],
                "page_sizes": [],
                "encrypted": pdf.is_encrypted,
                "permissions": {},
                "form_fields": False,
                "has_images": False,
                "has_text": False,
                "fonts": set(),
                "xfa_form": False,
                "attachments": [],
            }

            # Extract document info
            if pdf.metadata:
                # Fallback for different pypdf versions
                info = pdf.metadata
                metadata["author"] = info.get("/Author", None)
                metadata["creator"] = info.get("/Creator", None)
                metadata["producer"] = info.get("/Producer", None)
                metadata["subject"] = info.get("/Subject", None)
                metadata["title"] = info.get("/Title", None)

                # Handle dates with try-except as format can vary
                try:
                    # Newer pypdf versions
                    if info.get("/CreationDate"):
                        metadata["creation_date"] = info.get("/CreationDate")
                    if info.get("/ModDate"):
                        metadata["modification_date"] = info.get("/ModDate")
                except Exception:
                    # Older pypdf versions or different access pattern
                    try:
                        if hasattr(info, "creation_date"):
                            metadata["creation_date"] = info.creation_date
                        if hasattr(info, "modification_date"):
                            metadata["modification_date"] = info.modification_date
                    except Exception as e:
                        logger.warning(f"Failed to extract date info: {e}")

                # Keywords handling
                keywords = info.get("/Keywords", "")
                if isinstance(keywords, str) and keywords.strip():
                    metadata["keywords"] = [
                        k.strip() for k in keywords.split(",") if k.strip()
                    ]

            # Extract page sizes
            for page in pdf.pages:
                if hasattr(page, "mediabox"):
                    width = page.mediabox[2] - page.mediabox[0]
                    height = page.mediabox[3] - page.mediabox[1]
                    metadata["page_sizes"].append({"width": width, "height": height})

            # Note: Permissions extraction is pypdf version dependent
            if pdf.is_encrypted:
                permissions = {}
                perm_attrs = [
                    "can_print",
                    "can_modify",
                    "can_extract",
                    "can_annotate",
                ]
                for attr in perm_attrs:
                    if hasattr(pdf, attr):
                        permissions[attr] = getattr(pdf, attr)
                metadata["permissions"] = permissions

            # Check for resources - pypdf version-independent approach
            has_images = False
            has_text = False
            fonts = set()

            # Sample a few pages to detect resources (for performance)
            pages_to_check = min(5, len(pdf.pages))
            for i in range(pages_to_check):
                page = pdf.pages[i]
                if hasattr(page, "get_contents") and callable(page.get_contents):
                    try:
                        content = page.get_contents()
                        if content:
                            content_str = str(content)
                            if "/Image" in content_str:
                                has_images = True
                            if "/Text" in content_str or "BT" in content_str:
                                has_text = True
                    except Exception:
                        pass  # Skip if can't get contents

                # Try to detect fonts without triggering linter errors
                try:
                    # Just check if this page has font information by examining content
                    if hasattr(page, "get_contents") and callable(page.get_contents):
                        content = page.get_contents()
                        if content and "/Font" in str(content):
                            # We know there are fonts but can't extract names without triggering linter
                            fonts.add("font_detected")
                except Exception:
                    pass  # Skip if can't check for fonts

            metadata["has_images"] = has_images
            metadata["has_text"] = has_text
            metadata["fonts"] = list(fonts)

            # Add extraction method for tracking
            metadata["extraction_method"] = "pypdf"
            return metadata

        except PdfReadError as e:
            logger.error(f"Error extracting PDF metadata using pypdf: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in PDF extraction: {e}")
            raise

    async def extract(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, t.Any]:
        """
        Extract metadata from a PDF file.

        Args:
            file_path: Path to the PDF file
            content: Not used for PDF files
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            Dictionary containing extracted PDF metadata

        Raises:
            FileNotFoundError: If the file does not exist.
            ValueError: If the file is not a supported PDF format.
        """
        # Convert to Path for consistent handling
        path = Path(file_path)
        if not path.exists():
            error_message = f"File does not exist: {path}"
            logger.error(error_message)
            raise FileNotFoundError(error_message)

        # Determine MIME type if not provided
        if mime_type is None:
            mime_type, _ = self.mime_detector.get_mime_type(path)

        # Verify that this is a PDF file
        if mime_type not in self.supported_mime_types:
            # Fallback to extension-based detection if MIME type detection fails
            extension = path.suffix.lower()
            if extension in self.FILE_EXTENSIONS:
                logger.debug(
                    f"MIME type detection failed, using extension {extension} "
                    f"for {path}"
                )
            else:
                error_message = (
                    f"Unsupported file format: {mime_type} for {path}. "
                    f"PDFMetadataExtractor supports: "
                    f"{', '.join(self.supported_mime_types)}"
                )
                logger.error(error_message)
                raise ValueError(error_message)

        # Try to use cache if available
        if self.cache_manager:
            cache_key = f"pdf_metadata:{path}:{path.stat().st_mtime}"
            cached_data = await self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Using cached PDF metadata for {path}")
                return cached_data

        try:
            # Extract metadata using pypdf
            result = self._extract_pdf_with_pypdf(str(path))

            # Cache the result if cache manager is available
            if self.cache_manager:
                await self.cache_manager.put(cache_key, result)

            return result
        except Exception as e:
            error_message = f"Failed to extract metadata from {path}: {e}"
            logger.error(error_message)
            # Return basic error information instead of raising
            return {
                "metadata_type": "pdf",
                "error": f"Error extracting PDF metadata: {str(e)}",
            }



================================================
File: src/the_aichemist_codex/backend/metadata/text_extractor.py
================================================
"""Text metadata extractor for extracting information from text files.

This module provides functionality for analyzing text content to extract
keywords, topics, entities, and other metadata from text files.
"""

# mypy: disable-error-code="return-value"

import asyncio
import logging
import re
import time
from collections import Counter
from pathlib import Path
from typing import Any

import numpy as np
from sklearn.feature_extraction.text import (
    CountVectorizer,  # type: ignore
    TfidfVectorizer,
)

from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

from .extractor import BaseMetadataExtractor, MetadataExtractorRegistry

logger = logging.getLogger(__name__)


@MetadataExtractorRegistry.register
class TextMetadataExtractor(BaseMetadataExtractor):
    """Metadata extractor for text files.

    Analyzes text content to extract keywords, topics, entities, and other
    text-based metadata using NLP techniques.
    """

    def __init__(
        self,
        cache_manager: CacheManager | None = None,
        max_keywords: int = 15,
        max_topics: int = 5,
        min_keyword_length: int = 3,
        min_keyword_freq: int = 2,
    ):
        """Initialize the text metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching results
            max_keywords: Maximum number of keywords to extract
            max_topics: Maximum number of topics to extract
            min_keyword_length: Minimum length for a keyword
            min_keyword_freq: Minimum frequency for a keyword
        """
        self.cache_manager = cache_manager
        self.max_keywords = max_keywords
        self.max_topics = max_topics
        self.min_keyword_length = min_keyword_length
        self.min_keyword_freq = min_keyword_freq

        # Common English stop words to filter out
        self.stop_words = {
            "a",
            "an",
            "the",
            "and",
            "or",
            "but",
            "if",
            "because",
            "as",
            "what",
            "when",
            "where",
            "how",
            "who",
            "which",
            "this",
            "that",
            "these",
            "those",
            "then",
            "just",
            "so",
            "than",
            "such",
            "both",
            "through",
            "about",
            "for",
            "is",
            "of",
            "while",
            "during",
            "to",
        }

        # Initialize TF-IDF vectorizer for keyword extraction
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=100,
            stop_words="english",
            ngram_range=(1, 2),
            min_df=min_keyword_freq,
        )

        # Regular expressions for entity extraction
        self.email_pattern = re.compile(
            r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
        )
        self.url_pattern = re.compile(
            r"https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w .-]*(?:\?[=&%\w.-]*)?(?:#[\w-]*)?"
        )
        self.date_pattern = re.compile(
            r"\b\d{1,4}[-/]\d{1,2}[-/]\d{1,4}\b|\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{2,4}\b"
        )
        self.phone_pattern = re.compile(
            r"\b\+?(\d{1,3})?[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b"
        )

        # Initialize simple language detector based on common words
        self.language_markers = {
            "en": [
                "the",
                "and",
                "of",
                "to",
                "in",
                "that",
                "for",
                "it",
                "with",
                "as",
                "was",
                "on",
            ],
            "es": [
                "el",
                "la",
                "de",
                "que",
                "y",
                "en",
                "un",
                "por",
                "con",
                "no",
                "una",
                "para",
            ],
            "fr": [
                "le",
                "la",
                "de",
                "et",
                "à",
                "en",
                "un",
                "une",
                "que",
                "qui",
                "dans",
                "par",
            ],
            "de": [
                "der",
                "die",
                "das",
                "und",
                "zu",
                "in",
                "den",
                "von",
                "mit",
                "auf",
                "für",
                "ist",
            ],
        }

    @property
    def supported_mime_types(self) -> list[str]:
        """List of MIME types supported by this extractor."""
        return [
            "text/*",
            "application/json",
            "application/xml",
            "application/yaml",
            "application/x-yaml",
            "application/toml",
        ]

    async def extract(  # type: ignore
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """Extract metadata from a text file.

        Args:
            file_path: Path to the file
            content: Optional pre-loaded file content
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            A dictionary containing extracted metadata
        """
        start_time = time.time()

        # Check if we have cached results
        if self.cache_manager and hasattr(self.cache_manager, "get"):
            cache_key = f"text_metadata:{file_path}"
            # Properly await the async cache manager get method
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result and isinstance(cached_result, dict):
                logger.debug(f"Using cached metadata for {file_path}")
                return cached_result  # type: ignore

        # Get the content if not provided
        if content is None:
            # Convert file_path to Path object
            path = Path(file_path) if isinstance(file_path, str) else file_path
            content = await self._get_content(path)  # type: ignore
            if not content:
                return {
                    "error": "Failed to read file content",
                    "extraction_complete": False,
                    "extraction_confidence": 0.0,
                    "extraction_time": time.time() - start_time,
                }

        # Detect language (simplified approach)
        language = self._detect_language(content)

        # Extract text metadata
        extracted_data: dict[str, Any] = {}
        extracted_data["language"] = language

        # Only proceed with English content for now
        if language == "en":
            # Extract keywords using TF-IDF
            keywords = await asyncio.to_thread(self._extract_keywords, content)
            extracted_data["keywords"] = keywords

            # Extract topics
            topics = await asyncio.to_thread(self._extract_topics, content)
            extracted_data["topics"] = topics

            # Extract entities
            entities = self._extract_entities(content)
            extracted_data["entities"] = entities

            # Generate potential tags from the extracted information
            tags = self._generate_tags(keywords, topics, entities)
            extracted_data["tags"] = tags

            # Generate a simple summary
            summary = self._generate_summary(content)
            extracted_data["summary"] = summary

        # Mark extraction as complete
        extracted_data["extraction_complete"] = True
        extracted_data["extraction_confidence"] = 0.8  # Reasonable default
        extracted_data["extraction_time"] = time.time() - start_time

        # Cache the results if we have a cache manager
        if self.cache_manager and hasattr(self.cache_manager, "put"):
            await self.cache_manager.put(cache_key, extracted_data)  # type: ignore

        return extracted_data

    def _detect_language(self, content: str) -> str:
        """Detect the language of the content.

        This is a very simplified implementation. In a real-world application,
        you would use a proper language detection library.

        Args:
            content: The text content

        Returns:
            The detected language code (e.g., 'en' for English)
        """
        # Simplified implementation - just assuming English for now
        return "en"

    def _extract_keywords(self, content: str) -> list[str]:
        """Extract keywords from the content using TF-IDF.

        Args:
            content: The text content

        Returns:
            A list of extracted keywords
        """
        try:
            # Fit the vectorizer (this would be done once in a real implementation)
            X = self.tfidf_vectorizer.fit_transform([content])

            # Get feature names
            feature_names = self.tfidf_vectorizer.get_feature_names_out()

            # Convert sparse matrix to numpy array directly
            # This avoids calling .toarray() or .sum() on the spmatrix directly
            X_array = np.asarray(X.todense())  # type: ignore

            # Now we can safely sum with numpy
            sums = np.sum(X_array, axis=0).ravel()

            # Create feature scores
            feature_scores = list(zip(feature_names, sums, strict=False))

            # Sort by score
            sorted_features = sorted(feature_scores, key=lambda x: x[1], reverse=True)

            # Filter out short keywords and return the top max_keywords
            keywords = [
                word
                for word, score in sorted_features
                if len(word) >= self.min_keyword_length
                and word.lower() not in self.stop_words
            ]

            return keywords[: self.max_keywords]
        except Exception as e:
            logger.error(f"Error extracting keywords: {e}")
            return []

    def _extract_topics(self, content: str) -> list[dict[str, float]]:
        """Extract topics from the content.

        In a real implementation, this would use a topic modeling algorithm like LDA.
        For simplicity, we'll use a basic approach based on word co-occurrence.

        Args:
            content: The text content

        Returns:
            A list of topics, each represented as a dictionary mapping words to weights
        """
        try:
            # Split content into sentences
            sentences = content.split(".")

            # Initialize a CountVectorizer for word co-occurrence
            count_vectorizer = CountVectorizer(stop_words="english", ngram_range=(1, 1))

            # Fit the vectorizer
            X = count_vectorizer.fit_transform(sentences)

            # Get feature names
            feature_names = count_vectorizer.get_feature_names_out()

            # For simplicity, let's define topics based on the most common words in different parts of the text
            num_parts = min(self.max_topics, max(1, len(sentences) // 10))
            topics = []

            for i in range(num_parts):
                start_idx = i * len(sentences) // num_parts
                end_idx = (i + 1) * len(sentences) // num_parts

                part_sentences = sentences[start_idx:end_idx]
                part_text = " ".join(part_sentences)

                # Get word counts for this part
                word_counts = Counter(re.findall(r"\b\w+\b", part_text.lower()))

                # Remove stop words
                for word in self.stop_words:
                    if word in word_counts:
                        del word_counts[word]

                # Get the top 5 words for this topic
                top_words = word_counts.most_common(5)

                # Normalize weights
                total_count = sum(count for _, count in top_words) or 1
                topic = {word: count / total_count for word, count in top_words}

                if topic:
                    topics.append(topic)

            return topics
        except Exception as e:
            logger.error(f"Error extracting topics: {e}")
            return []

    def _extract_entities(self, content: str) -> dict[str, list[str]]:
        """Extract named entities from the content.

        In a real implementation, this would use a NER model.
        For simplicity, we'll use regex patterns for common entities.

        Args:
            content: The text content

        Returns:
            A dictionary mapping entity types to lists of entities
        """
        entities: dict[str, list[str]] = {
            "email": [],
            "url": [],
            "date": [],
            "phone": [],
        }

        # Extract emails
        emails = set(self.email_pattern.findall(content))
        entities["email"] = list(emails)

        # Extract URLs
        urls = set(self.url_pattern.findall(content))
        entities["url"] = list(urls)

        # Extract dates
        dates = set(self.date_pattern.findall(content))
        entities["date"] = list(dates)

        # Extract phone numbers
        phones = set(self.phone_pattern.findall(content))
        entities["phone"] = list(phones)

        return entities

    def _generate_tags(
        self,
        keywords: list[str],
        topics: list[dict[str, float]],
        entities: dict[str, list[str]],
    ) -> list[str]:
        """Generate tags from the extracted information.

        Args:
            keywords: List of extracted keywords
            topics: List of extracted topics
            entities: Dictionary of extracted entities

        Returns:
            A list of generated tags
        """
        tags = set()

        # Add the top keywords
        for keyword in keywords[:5]:
            if len(keyword) >= self.min_keyword_length:
                tags.add(keyword)

        # Add top words from each topic
        for topic in topics:
            for word in list(topic.keys())[:2]:
                if len(word) >= self.min_keyword_length:
                    tags.add(word)

        # Add entity-based tags
        if entities.get("url"):
            tags.add("contains-urls")

        if entities.get("email"):
            tags.add("contains-emails")

        return list(tags)

    def _generate_summary(self, content: str, max_length: int = 200) -> str:
        """Generate a simple summary of the content.

        In a real implementation, this would use a more sophisticated
        summarization algorithm.

        Args:
            content: The text content
            max_length: Maximum length of the summary

        Returns:
            A summary of the content
        """
        # Simplistic approach: take the first few sentences
        sentences = content.split(".")
        summary_sentences = []
        current_length = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            if current_length + len(sentence) > max_length:
                break

            summary_sentences.append(sentence)
            current_length += len(sentence) + 1  # +1 for the period

        return ". ".join(summary_sentences) + ("." if summary_sentences else "")



================================================
File: src/the_aichemist_codex/backend/metadata/video_extractor.py
================================================
"""
Video metadata extraction for The Aichemist Codex.

This module provides functionality to extract metadata from video files,
including format information, codec details, duration, resolution, frame rate,
and other video-specific metadata.
"""

import logging
from pathlib import Path
from typing import Any, Protocol, cast

# Local application imports
from the_aichemist_codex.backend.file_reader.file_metadata import FileMetadata
from the_aichemist_codex.backend.metadata.extractor import (
    BaseMetadataExtractor,
    MetadataExtractorRegistry,
)
from the_aichemist_codex.backend.utils.cache_manager import CacheManager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

# Initialize logger with proper name
logger = logging.getLogger(__name__)

# Third-party imports
try:
    from pymediainfo import MediaInfo

    MEDIAINFO_AVAILABLE = True
except ImportError:
    MEDIAINFO_AVAILABLE = False

# Import ffmpeg-python library for video analysis
# The actual module name is 'ffmpeg' but it has a function called 'probe'
FFMPEG_AVAILABLE = False
try:
    import ffmpeg

    # Check if probe function exists in the ffmpeg module
    if hasattr(ffmpeg, "probe"):
        FFMPEG_AVAILABLE = True
    else:
        logger.warning("ffmpeg module found but 'probe' function is not available")
except ImportError:
    pass


# Define a protocol for what we expect from ffmpeg
class FFmpegWithProbe(Protocol):
    def probe(self, path: str) -> dict[str, Any]: ...


class VideoMetadataExtractor(BaseMetadataExtractor):
    """
    Metadata extractor for video files.

    This extractor can process video files and extract rich metadata including:
    - Format information (container, size, bitrate)
    - Video track details (codec, resolution, frame rate)
    - Audio track details (codec, channels, sample rate)
    - Duration and timestamps
    - Metadata tags (title, artist, etc.)
    - Stream information for multi-track videos
    """

    # Supported MIME types
    SUPPORTED_MIME_TYPES = [
        "video/mp4",
        "video/x-msvideo",  # AVI
        "video/x-matroska",  # MKV
        "video/quicktime",  # MOV
        "video/x-ms-wmv",  # WMV
        "video/webm",  # WebM
        "video/x-flv",  # FLV
        "video/mpeg",  # MPEG
        "video/3gpp",  # 3GP
        "video/3gpp2",  # 3G2
        "video/ogg",  # OGV
    ]

    # File extensions mapping (for cases where MIME type detection fails)
    EXTENSION_MAPPING = {
        ".mp4": "video/mp4",
        ".avi": "video/x-msvideo",
        ".mkv": "video/x-matroska",
        ".mov": "video/quicktime",
        ".wmv": "video/x-ms-wmv",
        ".webm": "video/webm",
        ".flv": "video/x-flv",
        ".mpeg": "video/mpeg",
        ".mpg": "video/mpeg",
        ".3gp": "video/3gpp",
        ".3g2": "video/3gpp2",
        ".ogv": "video/ogg",
    }

    @property
    def supported_mime_types(self) -> list[str]:
        """Get the list of MIME types this extractor supports.

        Returns:
            List of supported MIME types
        """
        return self.SUPPORTED_MIME_TYPES

    def __init__(self, cache_manager: CacheManager | None = None) -> None:
        """Initialize the video metadata extractor.

        Args:
            cache_manager: Optional cache manager for caching extraction results
        """
        super().__init__(cache_manager)
        self.mime_detector = MimeTypeDetector()

        # Check if required libraries are available
        if not MEDIAINFO_AVAILABLE and not FFMPEG_AVAILABLE:
            logger.warning(
                "Neither pymediainfo nor ffmpeg are available. "
                "Video metadata extraction will be limited."
            )

    async def extract(
        self,
        file_path: str | Path,
        content: str | None = None,
        mime_type: str | None = None,
        metadata: FileMetadata | None = None,
    ) -> dict[str, Any]:
        """Extract metadata from a video file.

        Args:
            file_path: Path to the video file
            content: Not used for video files
            mime_type: Optional MIME type of the file
            metadata: Optional existing metadata to enhance

        Returns:
            Dictionary containing extracted video metadata
        """
        path = Path(file_path)
        if not path.exists():
            logger.warning(f"Video file not found: {path}")
            return {}

        # Determine MIME type if not provided
        if mime_type is None:
            mime_type, _ = self.mime_detector.get_mime_type(path)

            # Fall back to extension-based detection if needed
            if mime_type not in self.supported_mime_types:
                extension = path.suffix.lower()
                if extension in self.EXTENSION_MAPPING:
                    mime_type = self.EXTENSION_MAPPING[extension]

        # Check if this is a supported video type
        if mime_type not in self.supported_mime_types:
            logger.debug(
                f"Unsupported MIME type for video: {mime_type} for file: {path}"
            )
            return {}

        # Try to use cache if available
        if self.cache_manager:
            cache_key = f"video_metadata:{path}:{path.stat().st_mtime}"
            cached_data = await self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Using cached video metadata for {path}")
                return cached_data

        # Process the video file
        try:
            result = await self._process_video(path)

            # Cache the result if cache manager is available
            if self.cache_manager:
                await self.cache_manager.put(cache_key, result)

            return result
        except Exception as e:
            logger.error(f"Error extracting video metadata from {path}: {str(e)}")
            return {
                "metadata_type": "video",
                "error": f"Error extracting video metadata: {str(e)}",
            }

    async def _process_video(self, path: Path) -> dict[str, Any]:
        """Process a video file to extract metadata.

        Args:
            path: Path to the video file

        Returns:
            Dictionary containing extracted video metadata
        """
        result = {
            "metadata_type": "video",
            "container": {},
            "video_streams": [],
            "audio_streams": [],
            "subtitle_streams": [],
            "other_streams": [],
            "chapters": [],
            "tags": {},
        }

        # Basic file information
        result["container"]["file_size"] = path.stat().st_size
        result["container"]["filename"] = path.name

        # First try MediaInfo if available (usually provides more detailed information)
        if MEDIAINFO_AVAILABLE:
            try:
                self._extract_with_mediainfo(path, result)

                # Successfully extracted with MediaInfo
                result["extraction_method"] = "mediainfo"
                return result
            except Exception as e:
                logger.warning(
                    f"MediaInfo extraction failed: {str(e)}. Trying ffmpeg..."
                )

        # Fall back to ffmpeg if MediaInfo failed or is not available
        if FFMPEG_AVAILABLE:
            try:
                self._extract_with_ffmpeg(path, result)

                # Successfully extracted with ffmpeg
                result["extraction_method"] = "ffmpeg"
                return result
            except Exception as e:
                logger.error(f"ffmpeg extraction failed: {str(e)}")

        # If we get here, both methods failed or were not available
        if not result.get("container", {}).get("format"):
            # Add minimal information from file stats if no extraction method worked
            result["container"]["format"] = "unknown"
            result["container"]["file_extension"] = path.suffix.lower()
            result["error"] = (
                "Could not extract detailed metadata with available methods"
            )

        # Create a summary
        if "duration_seconds" in result["container"]:
            mins, secs = divmod(int(result["container"]["duration_seconds"]), 60)
            hours, mins = divmod(mins, 60)

            if hours > 0:
                duration_str = f"{hours}h {mins}m {secs}s"
            else:
                duration_str = f"{mins}m {secs}s"

            result["summary"] = (
                f"Video: {result['container'].get('format', 'unknown format')}, "
                f"{duration_str}"
            )

            # Add resolution if available
            if result["video_streams"] and "resolution" in result["video_streams"][0]:
                result["summary"] += f", {result['video_streams'][0]['resolution']}"
        else:
            result["summary"] = f"Video file: {path.name}"

        return result

    def _extract_with_mediainfo(self, path: Path, result: dict[str, Any]) -> None:
        """Extract video metadata using MediaInfo.

        Args:
            path: Path to the video file
            result: Dictionary to update with extracted information
        """
        if not MEDIAINFO_AVAILABLE:
            raise ImportError("pymediainfo is not available")

        # Get MediaInfo data
        media_info = MediaInfo.parse(path)

        # Process general track (container info)
        for track in media_info.general_tracks:
            track_dict = track.to_dict()

            # Extract container format information
            result["container"]["format"] = track_dict.get("format", "unknown")

            # Extract duration
            if "duration" in track_dict:
                duration_ms = float(track_dict["duration"])
                result["container"]["duration_seconds"] = duration_ms / 1000
                result["container"]["duration_formatted"] = self._format_duration(
                    duration_ms / 1000
                )

            # Extract overall bitrate
            if "overall_bit_rate" in track_dict:
                result["container"]["bitrate"] = int(track_dict["overall_bit_rate"])

            # Extract container-level tags
            for key, value in track_dict.items():
                if key.startswith("tag_") and value:
                    tag_name = key[4:].lower()  # Remove "tag_" prefix
                    result["tags"][tag_name] = value

        # Process video tracks
        for track in media_info.video_tracks:
            track_dict = track.to_dict()
            video_stream = {
                "index": track_dict.get(
                    "stream_identifier", track_dict.get("track_id", 0)
                ),
                "codec": track_dict.get("codec", track_dict.get("format", "unknown")),
                "codec_profile": track_dict.get("format_profile", ""),
                "bitrate": self._parse_bitrate(track_dict.get("bit_rate", "0")),
            }

            # Resolution and aspect ratio
            if "width" in track_dict and "height" in track_dict:
                width = int(track_dict["width"])
                height = int(track_dict["height"])
                video_stream["width"] = width
                video_stream["height"] = height
                video_stream["resolution"] = f"{width}x{height}"

                # Calculate aspect ratio
                if height > 0:
                    aspect = width / height
                    video_stream["aspect_ratio"] = round(aspect, 2)

                    # Standard aspect ratio detection
                    if 1.30 <= aspect <= 1.37:
                        video_stream["standard_aspect_ratio"] = "4:3"
                    elif 1.75 <= aspect <= 1.80:
                        video_stream["standard_aspect_ratio"] = "16:9"
                    elif 2.35 <= aspect <= 2.40:
                        video_stream["standard_aspect_ratio"] = "2.39:1 (Anamorphic)"

            # Frame rate
            if "frame_rate" in track_dict:
                try:
                    video_stream["frame_rate"] = float(track_dict["frame_rate"])
                except (ValueError, TypeError):
                    pass

            # Color information
            if "color_space" in track_dict:
                video_stream["color_space"] = track_dict["color_space"]

            if "chroma_subsampling" in track_dict:
                video_stream["chroma_subsampling"] = track_dict["chroma_subsampling"]

            if "bit_depth" in track_dict:
                video_stream["bit_depth"] = int(track_dict["bit_depth"])

            # Duration might be track-specific
            if "duration" in track_dict:
                video_stream["duration_seconds"] = float(track_dict["duration"]) / 1000

            # Add the video stream to results
            result["video_streams"].append(video_stream)

        # Process audio tracks
        for track in media_info.audio_tracks:
            track_dict = track.to_dict()
            audio_stream = {
                "index": track_dict.get(
                    "stream_identifier", track_dict.get("track_id", 0)
                ),
                "codec": track_dict.get("codec", track_dict.get("format", "unknown")),
                "codec_profile": track_dict.get("format_profile", ""),
                "bitrate": self._parse_bitrate(track_dict.get("bit_rate", "0")),
            }

            # Channels
            if "channel_s" in track_dict:
                audio_stream["channels"] = int(track_dict["channel_s"])

            # Sampling rate
            if "sampling_rate" in track_dict:
                try:
                    audio_stream["sample_rate"] = int(track_dict["sampling_rate"])
                except (ValueError, TypeError):
                    pass

            # Language
            if "language" in track_dict:
                audio_stream["language"] = track_dict["language"]

            # Duration might be track-specific
            if "duration" in track_dict:
                audio_stream["duration_seconds"] = float(track_dict["duration"]) / 1000

            # Add the audio stream to results
            result["audio_streams"].append(audio_stream)

        # Process subtitle tracks
        for track in media_info.text_tracks:
            track_dict = track.to_dict()
            subtitle_stream = {
                "index": track_dict.get(
                    "stream_identifier", track_dict.get("track_id", 0)
                ),
                "format": track_dict.get("format", "unknown"),
            }

            # Language
            if "language" in track_dict:
                subtitle_stream["language"] = track_dict["language"]

            # Add the subtitle stream to results
            result["subtitle_streams"].append(subtitle_stream)

        # Process chapter information, if available
        for track in media_info.menu_tracks:
            track_dict = track.to_dict()

            # Extract chapters if available
            chapters = []
            for key, value in track_dict.items():
                if key.startswith("_") and key.endswith("_time"):
                    chapter_idx = key[1:-5]  # Extract chapter index
                    try:
                        # Look for corresponding chapter name
                        name_key = f"_{chapter_idx}_string"
                        if name_key in track_dict:
                            chapter_name = track_dict[name_key]
                        else:
                            chapter_name = f"Chapter {chapter_idx}"

                        # Add chapter with timestamp
                        chapters.append(
                            {
                                "index": int(chapter_idx),
                                "name": chapter_name,
                                "timestamp": value,
                            }
                        )
                    except (ValueError, TypeError):
                        pass

            # Add chapters if found
            if chapters:
                result["chapters"] = sorted(chapters, key=lambda x: x["index"])

    def _extract_with_ffmpeg(self, path: Path, result: dict[str, Any]) -> None:
        """Extract video metadata using ffmpeg.

        Args:
            path: Path to the video file
            result: Dictionary to update with extracted information
        """
        if not FFMPEG_AVAILABLE:
            raise ImportError("ffmpeg is not available or probe function is missing")

        # Get ffmpeg probe data
        ffmpeg_typed = cast(FFmpegWithProbe, ffmpeg)
        probe = ffmpeg_typed.probe(str(path))

        # Extract container format information
        if "format" in probe:
            format_info = probe["format"]

            # Format name and file size
            result["container"]["format"] = format_info.get("format_name", "unknown")
            result["container"]["format_long_name"] = format_info.get(
                "format_long_name", ""
            )

            # Duration
            if "duration" in format_info:
                duration_seconds = float(format_info["duration"])
                result["container"]["duration_seconds"] = duration_seconds
                result["container"]["duration_formatted"] = self._format_duration(
                    duration_seconds
                )

            # Bitrate
            if "bit_rate" in format_info:
                try:
                    result["container"]["bitrate"] = int(format_info["bit_rate"])
                except (ValueError, TypeError):
                    pass

            # Extract container-level tags
            if "tags" in format_info:
                for key, value in format_info["tags"].items():
                    result["tags"][key.lower()] = value

        # Process streams
        for stream in probe.get("streams", []):
            stream_type = stream.get("codec_type", "").lower()

            # Common stream properties
            stream_info = {
                "index": stream.get("index", 0),
                "codec": stream.get("codec_name", "unknown"),
                "codec_long_name": stream.get("codec_long_name", ""),
                "codec_tag": stream.get("codec_tag_string", ""),
            }

            # Extract tags for the stream
            if "tags" in stream:
                stream_info["tags"] = stream["tags"]

                # Add language if available
                if "language" in stream["tags"]:
                    stream_info["language"] = stream["tags"]["language"]

            # Process by stream type
            if stream_type == "video":
                # Add video-specific information
                if "width" in stream and "height" in stream:
                    stream_info["width"] = stream["width"]
                    stream_info["height"] = stream["height"]
                    stream_info["resolution"] = f"{stream['width']}x{stream['height']}"

                # Calculate aspect ratio
                if "display_aspect_ratio" in stream:
                    stream_info["display_aspect_ratio"] = stream["display_aspect_ratio"]
                elif "width" in stream and "height" in stream and stream["height"] > 0:
                    aspect = stream["width"] / stream["height"]
                    stream_info["aspect_ratio"] = round(aspect, 2)

                # Frame rate
                if "r_frame_rate" in stream:
                    try:
                        fps_parts = stream["r_frame_rate"].split("/")
                        if len(fps_parts) == 2 and int(fps_parts[1]) > 0:
                            stream_info["frame_rate"] = round(
                                int(fps_parts[0]) / int(fps_parts[1]), 2
                            )
                    except (ValueError, ZeroDivisionError):
                        pass

                # Color information
                if "pix_fmt" in stream:
                    stream_info["pixel_format"] = stream["pix_fmt"]

                if "color_space" in stream:
                    stream_info["color_space"] = stream["color_space"]

                # Duration might be stream-specific
                if "duration" in stream:
                    stream_info["duration_seconds"] = float(stream["duration"])

                # Add to video streams
                result["video_streams"].append(stream_info)

            elif stream_type == "audio":
                # Add audio-specific information
                if "channels" in stream:
                    stream_info["channels"] = stream["channels"]

                if "sample_rate" in stream:
                    try:
                        stream_info["sample_rate"] = int(stream["sample_rate"])
                    except (ValueError, TypeError):
                        pass

                if "channel_layout" in stream:
                    stream_info["channel_layout"] = stream["channel_layout"]

                # Bit rate for audio
                if "bit_rate" in stream:
                    try:
                        stream_info["bitrate"] = int(stream["bit_rate"])
                    except (ValueError, TypeError):
                        pass

                # Duration might be stream-specific
                if "duration" in stream:
                    stream_info["duration_seconds"] = float(stream["duration"])

                # Add to audio streams
                result["audio_streams"].append(stream_info)

            elif stream_type == "subtitle":
                # Add to subtitle streams
                result["subtitle_streams"].append(stream_info)

            else:
                # Unknown stream type
                result["other_streams"].append(stream_info)

        # Extract chapters if available
        if "chapters" in probe:
            chapters = []
            for idx, chapter in enumerate(probe["chapters"]):
                chapter_info = {
                    "index": idx,
                    "start_time": float(chapter.get("start_time", 0)),
                    "end_time": float(chapter.get("end_time", 0)),
                }

                # Extract chapter title from tags if available
                if "tags" in chapter and "title" in chapter["tags"]:
                    chapter_info["name"] = chapter["tags"]["title"]
                else:
                    chapter_info["name"] = f"Chapter {idx + 1}"

                chapters.append(chapter_info)

            result["chapters"] = chapters

    def _parse_bitrate(self, bitrate_str: str) -> int:
        """Parse bitrate string to integer value.

        Args:
            bitrate_str: Bitrate string to parse

        Returns:
            Bitrate as integer in bits per second
        """
        try:
            # Try direct conversion first
            return int(bitrate_str)
        except (ValueError, TypeError):
            # Handle case where bitrate is not a direct integer
            if isinstance(bitrate_str, str):
                # Remove any non-numeric characters
                num_str = "".join(c for c in bitrate_str if c.isdigit())
                if num_str:
                    return int(num_str)
            return 0

    def _format_duration(self, seconds: float) -> str:
        """Format duration in seconds to human-readable string.

        Args:
            seconds: Duration in seconds

        Returns:
            Formatted duration string (HH:MM:SS)
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)

        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"


# Register the extractor
MetadataExtractorRegistry.register(VideoMetadataExtractor)


