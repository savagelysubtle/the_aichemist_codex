
================================================
File: src/the_aichemist_codex/backend/config/__init__.py
================================================
"""
Configuration module for The Aichemist Codex.

This module provides configuration settings and utilities for the application,
including settings for file paths, permissions, and application behavior.

Available configuration options:
- Base directories (PROJECT_ROOT, DATA_DIR, etc.)
- Security settings
- Logging configuration
- File processing limits and rules

Data Directory Configuration:
The data directory can be configured using environment variables:
- AICHEMIST_ROOT_DIR: Set the project root directory
- AICHEMIST_DATA_DIR: Set the data directory directly

If not specified, the system will automatically detect appropriate directories.
"""

# ✅ Ensure consistent import reference
from .config_loader import config
from .logging_config import setup_logging

__all__ = ["config", "setup_logging"]



================================================
File: src/the_aichemist_codex/backend/config/config_loader.py
================================================
"""Handles loading of project configuration settings."""

import logging
from pathlib import Path

import tomli

CONFIG_FILE = Path(__file__).resolve().parent / ".codexconfig"

# Default configuration values to avoid circular imports
DEFAULT_IGNORE_PATTERNS = [
    "__pycache__",
    "*.pyc",
    ".git",
    ".vscode",
    ".idea",
    "venv",
    ".env",
    ".venv",
    "node_modules",
]
DEFAULT_MAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB
DEFAULT_MAX_TOKENS = 4000


class CodexConfig:
    """Loads configuration settings for The Aichemist Codex."""

    def __init__(self):
        """Initialize with default settings."""
        # Use default values defined at module level to avoid circular imports
        self.settings = {
            "ignore_patterns": DEFAULT_IGNORE_PATTERNS,
            "max_file_size": DEFAULT_MAX_FILE_SIZE,
            "max_tokens": DEFAULT_MAX_TOKENS,
        }
        self._load_config_file()

    def _load_config_file(self):
        """Loads user-defined settings from `.codexconfig`."""
        if CONFIG_FILE.exists():
            try:
                with open(CONFIG_FILE, "rb") as f:
                    user_config = tomli.load(f).get("codex", {})
                    self.settings.update(user_config)
            except Exception as e:
                logging.error(f"Error loading config: {e}")

    def get(self, key, default=None):
        """Retrieve a configuration setting."""
        return self.settings.get(key, default)


# ✅ Singleton instance to use across the project
config = CodexConfig()



================================================
File: src/the_aichemist_codex/backend/config/logging_config.py
================================================
"""Logging configuration for The Aichemist Codex."""

import logging
import sys

from .settings import LOG_DIR

# Ensure log directory exists
LOG_DIR.mkdir(parents=True, exist_ok=True)

LOG_FILE = LOG_DIR / "project.log"


def setup_logging():
    """Configures global logging settings."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )


setup_logging()
logger = logging.getLogger(__name__)
logger.info("Logging initialized.")



================================================
File: src/the_aichemist_codex/backend/config/rules_engine.py
================================================
"""Handles file organization rules for The Aichemist Codex."""

import json
import logging
from pathlib import Path

from .config_loader import config

logger = logging.getLogger(__name__)


class RulesEngine:
    """Manages dynamic file movement rules."""

    def __init__(self):
        self.rules = []
        self._load_rules()

    def _load_rules(self):
        """Loads file handling rules from a configuration file."""
        rules_file = Path(__file__).resolve().parent / "default_ignore_patterns.json"
        if rules_file.exists():
            try:
                with open(rules_file, encoding="utf-8") as f:
                    self.rules = json.load(f).get("rules", [])
            except Exception as e:
                logger.error(f"Failed to load rules: {e}")

    def should_ignore(self, file_path: str) -> bool:
        """Checks if a file should be ignored based on defined patterns."""
        ignore_patterns = config.get("ignore_patterns")
        return any(file_path.endswith(pattern) for pattern in ignore_patterns)


rules_engine = RulesEngine()



================================================
File: src/the_aichemist_codex/backend/config/schemas.py
================================================
file_tree_schema = {
    "type": "object",
    "patternProperties": {".*": {"type": ["object", "null"]}},
}

code_summary_schema = {
    "type": "object",
    "patternProperties": {
        ".*": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "type": {"type": "string"},
                    "name": {"type": "string"},
                    "args": {"type": "array", "items": {"type": "string"}},
                    "lineno": {"type": "integer"},
                },
                "required": ["type", "name", "lineno"],
            },
        }
    },
}



================================================
File: src/the_aichemist_codex/backend/config/secure_config.py
================================================
"""Provides secure configuration management with encryption."""

import base64
import json
import logging
import os
import platform
from pathlib import Path
from typing import Any

from cryptography.fernet import Fernet

from the_aichemist_codex.backend.config.settings import DATA_DIR

logger = logging.getLogger(__name__)


class SecureConfigManager:
    """Manages secure storage and retrieval of sensitive configuration values."""

    def __init__(self, config_file: Path = DATA_DIR / "secure_config.enc"):
        """
        Initialize secure configuration manager.

        Args:
            config_file: Path to the encrypted configuration file
        """
        self.config_file = config_file
        self._config: dict[str, Any] = {}
        self._key = self._get_or_create_key()
        self._fernet = Fernet(self._key)
        self._load_config()

    def _get_or_create_key(self) -> bytes:
        """Get or create encryption key from environment or key file."""
        key_file = DATA_DIR / ".encryption_key"

        # Try to get key from environment
        env_key = os.environ.get("AICHEMIST_ENCRYPTION_KEY")
        if env_key:
            try:
                return base64.urlsafe_b64decode(env_key)
            except Exception as e:
                logger.error(f"Invalid encryption key in environment: {e}")

        # Try to load from key file
        if key_file.exists():
            try:
                with open(key_file, "rb") as f:
                    return f.read()
            except Exception as e:
                logger.error(f"Error reading key file: {e}")

        # Generate new key
        logger.info("Generating new encryption key")
        key = Fernet.generate_key()

        # Save new key
        try:
            key_file.parent.mkdir(parents=True, exist_ok=True)
            with open(key_file, "wb") as f:
                f.write(key)

            # Set secure permissions if not on Windows
            if platform.system() != "Windows":
                os.chmod(key_file, 0o600)  # Secure permissions
            else:
                # On Windows, we can't easily set 0o600 equivalent
                # but we can try to make it readable only by the current user
                try:
                    import ntsecuritycon as con
                    import win32con
                    import win32security

                    # Get current user SID
                    username = os.environ.get("USERNAME")
                    if username:
                        sd = win32security.GetFileSecurity(
                            str(key_file), win32security.DACL_SECURITY_INFORMATION
                        )
                        dacl = win32security.ACL()

                        # Add current user with read/write access
                        user_sid, _, _ = win32security.LookupAccountName(None, username)
                        dacl.AddAccessAllowedAce(
                            win32security.ACL_REVISION,
                            con.FILE_GENERIC_READ | con.FILE_GENERIC_WRITE,
                            user_sid,
                        )

                        # Set the DACL
                        sd.SetSecurityDescriptorDacl(1, dacl, 0)
                        win32security.SetFileSecurity(
                            str(key_file), win32security.DACL_SECURITY_INFORMATION, sd
                        )
                except ImportError:
                    logger.warning(
                        "pywin32 not installed, cannot set Windows file permissions"
                    )
                except Exception as e:
                    logger.error(f"Error setting Windows file permissions: {e}")

        except Exception as e:
            logger.error(f"Error saving key file: {e}")

        return key

    def _load_config(self):
        """Load and decrypt configuration from file."""
        if not self.config_file.exists():
            self._config = {}
            return

        try:
            with open(self.config_file, "rb") as f:
                encrypted_data = f.read()

            if encrypted_data:
                decrypted_data = self._fernet.decrypt(encrypted_data)
                self._config = json.loads(decrypted_data)
            else:
                self._config = {}
        except Exception as e:
            logger.error(f"Error loading secure configuration: {e}")
            self._config = {}

    def _save_config(self):
        """Encrypt and save configuration to file."""
        try:
            encrypted_data = self._fernet.encrypt(json.dumps(self._config).encode())

            self.config_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_file, "wb") as f:
                f.write(encrypted_data)

            # Set secure permissions if not on Windows
            if platform.system() != "Windows":
                os.chmod(self.config_file, 0o600)  # Secure permissions
        except Exception as e:
            logger.error(f"Error saving secure configuration: {e}")

    def get(self, key: str, default: Any = None) -> Any:
        """
        Get a configuration value.

        Args:
            key: Configuration key
            default: Default value if key not found

        Returns:
            Configuration value or default
        """
        return self._config.get(key, default)

    def set(self, key: str, value: Any) -> None:
        """
        Set a configuration value.

        Args:
            key: Configuration key
            value: Configuration value
        """
        self._config[key] = value
        self._save_config()

    def delete(self, key: str) -> bool:
        """
        Delete a configuration value.

        Args:
            key: Configuration key

        Returns:
            True if key existed and was deleted, False otherwise
        """
        if key in self._config:
            del self._config[key]
            self._save_config()
            return True
        return False

    def get_all(self) -> dict[str, Any]:
        """
        Get all configuration values.

        Returns:
            Dictionary containing all configuration values
        """
        return self._config.copy()

    def clear(self) -> None:
        """Clear all configuration values."""
        self._config.clear()
        self._save_config()

    def rotate_key(self) -> None:
        """Generate a new encryption key and re-encrypt configuration."""
        # Save current config
        current_config = self._config.copy()

        # Generate new key
        self._key = Fernet.generate_key()
        self._fernet = Fernet(self._key)

        # Save new key
        key_file = DATA_DIR / ".encryption_key"
        try:
            with open(key_file, "wb") as f:
                f.write(self._key)

            # Set secure permissions if not on Windows
            if platform.system() != "Windows":
                os.chmod(key_file, 0o600)
        except Exception as e:
            logger.error(f"Error saving new key file: {e}")
            return

        # Re-encrypt config with new key
        self._config = current_config
        self._save_config()


# Create a singleton instance for application-wide use
secure_config = SecureConfigManager()



================================================
File: src/the_aichemist_codex/backend/config/settings.py
================================================
"""Global settings and configuration constants."""

import logging
import os
from pathlib import Path
from typing import Any

from the_aichemist_codex.backend.file_manager.directory_manager import DirectoryManager

logger = logging.getLogger(__name__)


# Project root detection - more robust approach
def determine_project_root() -> Path:
    """
    Determine the project root directory using multiple methods.

    First checks for environment variable, then tries to detect based on
    repository structure, with a fallback to the parent of the backend directory.

    Returns:
        Path: The detected project root directory
    """
    # 1. First priority: Check environment variable
    env_root = os.environ.get("AICHEMIST_ROOT_DIR")
    if env_root:
        root_dir = Path(env_root).resolve()
        if root_dir.exists():
            logger.info(f"Using root directory from environment: {root_dir}")
            return root_dir
        else:
            logger.warning(f"Environment root directory doesn't exist: {root_dir}")

    # 2. Second priority: Look for repository indicators
    current_file = Path(__file__).resolve()
    potential_root = (
        current_file.parent.parent.parent.parent
    )  # Go up to the project root

    # Check for indicators of project root (like README.md, pyproject.toml, etc.)
    root_indicators = ["README.md", "pyproject.toml", ".git"]
    if any((potential_root / indicator).exists() for indicator in root_indicators):
        logger.info(f"Detected project root at: {potential_root}")
        return potential_root

    # 3. Fallback: Use parent of backend directory
    backend_parent = current_file.parent.parent.parent  # This is backend/
    logger.info(f"Using backend parent as root: {backend_parent}")
    return backend_parent


# Initialize the DirectoryManager for standardized data directory access
# Use previously defined functions to determine base directory
def initialize_directory_manager():
    """
    Initialize the DirectoryManager with the appropriate data directory.

    Returns:
        DirectoryManager: Configured directory manager instance
    """
    # Check for environment variable override first
    env_data_dir = os.environ.get("AICHEMIST_DATA_DIR")
    if env_data_dir:
        data_dir = Path(env_data_dir).resolve()
        logger.info(f"Using data directory from environment: {data_dir}")
        return DirectoryManager(data_dir)

    # If no env var, use data/ subdirectory in project root
    data_dir = PROJECT_ROOT / "data"
    logger.info(f"Using default data directory: {data_dir}")
    return DirectoryManager(data_dir)


# Base directories
PROJECT_ROOT = determine_project_root()

# Initialize the directory manager
directory_manager = initialize_directory_manager()

# Direct access to standard directories through the directory manager
DATA_DIR = directory_manager.base_dir
CACHE_DIR = directory_manager.get_dir("cache")
LOG_DIR = directory_manager.get_dir("logs")
EXPORT_DIR = directory_manager.get_dir("exports")
VERSION_DIR = directory_manager.get_dir("versions")
BACKUP_DIR = directory_manager.get_dir("backup")
TRASH_DIR = directory_manager.get_dir("trash")

# No need to manually create directories or set permissions since the
# DirectoryManager already handles this in its initialization

# File settings
DEFAULT_JSON_INDENT = 4
DEFAULT_IGNORE_PATTERNS: list[str] = [
    # Python
    "*.pyc",
    "*.pyo",
    "*.pyd",
    "__pycache__",
    ".pytest_cache",
    ".coverage",
    ".tox",
    ".nox",
    ".mypy_cache",
    ".ruff_cache",
    ".hypothesis",
    "poetry.lock",
    "Pipfile.lock",
    # JavaScript/Node
    "node_modules",
    "bower_components",
    "package-lock.json",
    "yarn.lock",
    ".npm",
    ".yarn",
    ".pnpm-store",
    # Java
    "*.class",
    "*.jar",
    "*.war",
    "*.ear",
    "*.nar",
    "target/",
    ".gradle/",
    "build/",
    ".settings/",
    ".project",
    ".classpath",
    "gradle-app.setting",
    "*.gradle",
    # C/C++
    "*.o",
    "*.obj",
    "*.so",
    "*.dll",
    "*.dylib",
    "*.exe",
    "*.lib",
    "*.out",
    "*.a",
    "*.pdb",
    # Swift/Xcode
    ".build/",
    "*.xcodeproj/",
    "*.xcworkspace/",
    "*.pbxuser",
    "*.mode1v3",
    "*.mode2v3",
    "*.perspectivev3",
    "*.xcuserstate",
    "xcuserdata/",
    ".swiftpm/",
    # Ruby
    "*.gem",
    ".bundle/",
    "vendor/bundle",
    "Gemfile.lock",
    ".ruby-version",
    ".ruby-gemset",
    ".rvmrc",
    # Rust
    "target/",
    "Cargo.lock",
    "**/*.rs.bk",
    # Go
    "bin/",
    "pkg/",
    # .NET/C#
    "bin/",
    "obj/",
    "*.suo",
    "*.user",
    "*.userosscache",
    "*.sln.docstates",
    "packages/",
    "*.nupkg",
    # Version control
    ".git",
    ".svn",
    ".hg",
    ".gitignore",
    ".gitattributes",
    ".gitmodules",
    # Images and media
    "*.svg",
    "*.png",
    "*.jpg",
    "*.jpeg",
    "*.gif",
    "*.ico",
    "*.pdf",
    "*.mov",
    "*.mp4",
    "*.mp3",
    "*.wav",
    # Virtual environments
    "venv",
    ".venv",
    "env",
    ".env",
    "virtualenv",
    # IDEs and editors
    ".idea",
    ".vscode",
    ".vs",
    "*.swp",
    "*.swo",
    "*.swn",
    ".settings",
    ".project",
    ".classpath",
    "*.sublime-*",
    # Temporary and cache files
    "*.log",
    "*.bak",
    "*.swp",
    "*.tmp",
    "*.temp",
    ".cache",
    ".sass-cache",
    ".eslintcache",
    ".DS_Store",
    "Thumbs.db",
    "desktop.ini",
    # Build directories and artifacts
    "build",
    "dist",
    "target",
    "out",
    "*.egg-info",
    "*.egg",
    "*.whl",
    "*.so",
    "*.dylib",
    "*.dll",
    "*.class",
    # Documentation
    "site-packages",
    ".docusaurus",
    ".next",
    ".nuxt",
    # Other common patterns
    ## Minified files
    "*.min.js",
    "*.min.css",
    ## Source maps
    "*.map",
    ## Terraform
    ".terraform",
    "*.tfstate*",
    ## Dependencies in various languages
    "vendor/",
]

# Cache settings
CACHE_TTL = 3600  # 1 hour in seconds
MAX_CACHE_SIZE = 1024 * 1024 * 100  # 100MB
MAX_MEMORY_CACHE_ITEMS = 1000

# File processing settings
MAX_FILE_SIZE = 1024 * 1024 * 50  # 50MB
CHUNK_SIZE = 1024 * 64  # 64KB chunks for file operations
MAX_BATCH_SIZE = 100  # Maximum items in a batch operation
MAX_TOKENS = 8000  # Token limit for analysis

# Search settings
MAX_SEARCH_RESULTS = 1000
SEARCH_CACHE_TTL = 300  # 5 minutes
MIN_SEARCH_TERM_LENGTH = 3

# Regex search settings
REGEX_MAX_COMPLEXITY = 1000  # Maximum complexity score for regex patterns
REGEX_TIMEOUT_MS = 500  # Timeout for regex search operations in milliseconds
REGEX_CACHE_TTL = 300  # 5 minutes
REGEX_MAX_RESULTS = 100  # Maximum number of results to return

# Similarity search settings
SIMILARITY_THRESHOLD = 0.75  # Minimum similarity score (0.0-1.0)
SIMILARITY_MAX_RESULTS = 50  # Maximum number of similar files to return
SIMILARITY_CACHE_TTL = 300  # 5 minutes
SIMILARITY_MIN_GROUP_SIZE = 2  # Minimum number of files to form a group
SIMILARITY_GROUP_THRESHOLD = 0.8  # Threshold for group membership

# Security settings
PASSWORD_MIN_LENGTH = 12
PASSWORD_COMPLEXITY = {
    "min_lowercase": 1,
    "min_uppercase": 1,
    "min_digits": 1,
    "min_special": 1,
}
TOKEN_EXPIRY = 3600  # 1 hour in seconds
MAX_LOGIN_ATTEMPTS = 5
LOGIN_COOLDOWN = 300  # 5 minutes

# Logging settings
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"
LOG_LEVEL = "INFO"
MAX_LOG_SIZE = 1024 * 1024 * 10  # 10MB
MAX_LOG_FILES = 5

# API settings
API_VERSION = "v1"
API_PREFIX = f"/api/{API_VERSION}"
DEFAULT_PAGE_SIZE = 50
MAX_PAGE_SIZE = 100

# Performance settings
THREAD_POOL_SIZE = os.cpu_count() or 4
TASK_QUEUE_SIZE = 1000
RATE_LIMIT = {"default": 60, "search": 30, "batch": 10}  # requests per minute

# Feature flags
FEATURES = {
    "enable_caching": True,
    "enable_compression": True,
    "enable_rate_limiting": True,
    "enable_batch_processing": True,
    "enable_async_processing": True,
    "enable_regex_search": True,  # Enable regex search functionality
    "enable_similarity_search": True,  # Enable similarity search functionality
    "enable_semantic_search": False,  # Disable semantic search by default
}

# Metadata Extraction Settings
ENABLE_ENHANCED_METADATA = True
METADATA_CACHE_TTL = 3600  # 1 hour cache TTL for metadata
METADATA_MAX_CONCURRENT = 5  # Maximum concurrent metadata extraction tasks
METADATA_CONFIDENCE_THRESHOLD = (
    0.5  # Minimum confidence threshold for metadata extraction
)

# Versioning settings
DEFAULT_VERSIONING_SETTINGS = {
    "auto_create_versions": True,  # Automatically create versions on file changes
    "version_on_modify": True,  # Create versions when files are modified
    "max_versions_per_file": 20,  # Maximum number of versions to keep per file
    "default_policy": "HYBRID",  # Default versioning policy (FULL_COPY, DIFF_BASED, HYBRID)
    "version_retention_days": 30,  # Number of days to keep versions before cleanup
    "compression_enabled": True,  # Use compression for stored versions
    "include_patterns": [  # File patterns to include in versioning
        "*.py",
        "*.js",
        "*.ts",
        "*.html",
        "*.css",
        "*.md",
        "*.txt",
        "*.json",
        "*.yaml",
        "*.yml",
        "*.xml",
        "*.csv",
        "*.sql",
    ],
    "exclude_patterns": [  # File patterns to exclude from versioning
        "*.log",
        "*.tmp",
        "*.temp",
        "*.swp",
        "*.bak",
        "*.backup",
        "*.pyc",
        "*.class",
        "*.o",
        "*.obj",
    ],
}

# Notification system settings
NOTIFICATION_SETTINGS = {
    "enabled": True,  # Master toggle for the notification system
    "retention_days": 30,  # How long to keep notifications (in days)
    "max_notifications_per_type": 1000,  # Maximum number of notifications to keep per type
    "notification_levels": [
        "INFO",
        "WARNING",
        "ERROR",
        "CRITICAL",
    ],  # Valid notification levels
    "default_level": "INFO",  # Default notification level
    "channels": {
        "log": {
            "enabled": True,  # Whether to log notifications to the log file
            "min_level": "INFO",  # Minimum level to log
        },
        "email": {
            "enabled": False,  # Whether to send email notifications
            "min_level": "WARNING",  # Minimum level to send email
            "recipients": [],  # List of email recipients
            "from_address": "",  # From email address
            "subject_prefix": "[Aichemist Codex] ",  # Prefix for email subjects
        },
        "webhook": {
            "enabled": False,  # Whether to send webhook notifications
            "min_level": "WARNING",  # Minimum level to send webhook
            "endpoints": [],  # List of webhook endpoints
            "headers": {},  # Headers to send with webhook requests
        },
        "database": {
            "enabled": True,  # Whether to store notifications in the database
            "min_level": "INFO",  # Minimum level to store
            "max_age_days": 30,  # Maximum age of notifications to keep
        },
    },
    "throttling": {
        "enabled": True,  # Whether to throttle notifications
        "window_seconds": 60,  # Time window for throttling (in seconds)
        "max_similar": 5,  # Maximum number of similar notifications in the window
    },
}


def get_settings() -> dict[str, Any]:
    """
    Get all settings as a dictionary.

    Returns:
        Dict[str, Any]: Dictionary containing all settings from this module
    """
    settings = {}
    for key, value in globals().items():
        # Only include uppercase variables and not built-ins
        if key.isupper() and not key.startswith("_"):
            settings[key] = value
    return settings



================================================
File: src/the_aichemist_codex/backend/config/sorting_rules.yaml
================================================
rules:
  - name: "Organize Markdown Files"
    extensions: [".md", ".markdown"]
    target_dir: "docs/organized_markdown"
    description: "Moves all Markdown files to a dedicated directory for better organization"
    preserve_path: true



================================================
File: src/the_aichemist_codex/backend/config/.codexconfig
================================================
# The Aichemist Codex Configuration
# This file allows you to customize the behavior of the Aichemist Codex.

[codex]
# Directories to monitor for file changes
# Format: each entry can be a simple path string or a table with advanced settings
directories_to_watch = [
    # Simple format examples:
    # "D:/Downloads",
    # "/home/user/Documents"

    # Advanced format examples with per-directory settings:
    # { path = "D:/Important", priority = "high", recursive_depth = 5, throttle = 0 },
    # { path = "D:/Downloads", priority = "low", recursive_depth = 2, throttle = 2 }
]

# Global monitoring settings (can be overridden per directory)
default_priority = "normal"  # Priority levels: "critical", "high", "normal", "low"
default_recursive_depth = 3  # How deep to scan subdirectories (0 = no recursion)
default_throttle = 1.0       # Seconds to wait between processing files in high-activity dirs (0 = no throttling)

# Auto-registration of new directories
auto_discover_directories = false    # Whether to automatically add new directories
auto_discover_parent_dirs = []       # Parent directories to scan for new directories
auto_discover_interval = 3600        # Seconds between auto-discovery scans (1 hour)
auto_discover_depth = 1              # How deep to scan for new directories

# Sorting interval in seconds (how often to run scheduled sorting)
sorting_interval = 300  # 5 minutes

# Change history settings
change_history_retention_days = 30  # How long to keep change history records
change_debounce_interval = 2.0      # Seconds to wait for rapid sequential changes

# Change severity thresholds (percentage of file changed)
minor_change_threshold = 0.05       # 5% changed = minor
moderate_change_threshold = 0.25    # 25% changed = moderate
major_change_threshold = 0.50       # 50% changed = major
                                    # >50% changed = critical

# Additional patterns to ignore beyond the default ones
# These use glob patterns that are matched against file paths
ignore_patterns = [
    # Add custom patterns here, for example:
    # "*.tmp",
    # "temp*.*"
]

# Specific directories to ignore completely
# These can be absolute paths or relative to monitored directories
ignored_directories = [
    # Add directories to ignore here, for example:
    # "D:/Projects/Private",
    # "temp_files"
]

# Maximum file size to process (in bytes)
max_file_size = 52428800  # 50MB

# Token limit for analysis
max_tokens = 8000

# Versioning settings
[versioning]
# Whether to automatically create versions when files change
auto_create_versions = true

# Create versions when files are modified (vs. only on creation or specific operations)
version_on_modify = true

# Maximum number of versions to keep per file
max_versions_per_file = 20

# Default versioning policy (FULL_COPY, DIFF_BASED, HYBRID)
# - FULL_COPY: Store complete file copy for each version
# - DIFF_BASED: Store only differences between versions for text files
# - HYBRID: Use diff for text files, full copies for binary files
default_policy = "HYBRID"

# Number of days to keep versions before cleanup
version_retention_days = 30

# Use compression for stored versions to save space
compression_enabled = true

# File patterns to include in versioning (glob patterns)
include_patterns = [
    "*.py",
    "*.js",
    "*.ts",
    "*.html",
    "*.css",
    "*.md",
    "*.txt",
    "*.json",
    "*.yaml",
    "*.yml",
    "*.xml",
    "*.csv",
    "*.sql"
]

# File patterns to exclude from versioning (glob patterns)
exclude_patterns = [
    "*.log",
    "*.tmp",
    "*.temp",
    "*.swp",
    "*.bak",
    "*.backup",
    "*.pyc",
    "*.class",
    "*.o",
    "*.obj"
]

# Notification system settings
[notification]
# Master toggle for the notification system
enabled = true

# Time-based cleanup for notifications
retention_days = 30

# Maximum number of notifications to store per type
max_notifications_per_type = 1000

# Valid notification levels in increasing severity
notification_levels = ["INFO", "WARNING", "ERROR", "CRITICAL"]

# Default level for notifications
default_level = "INFO"

# Configure notification channels
[notification.channels]

# Logging channel configuration
[notification.channels.log]
enabled = true
min_level = "INFO"  # Minimum level to log

# Email channel configuration
[notification.channels.email]
enabled = false  # Off by default, enable by setting to true
min_level = "WARNING"  # Only warnings and above are emailed
recipients = []  # Add email addresses like ["user@example.com"]
from_address = ""  # Email sender
subject_prefix = "[Aichemist Codex] "  # Prefix for email subjects

# Webhook channel configuration
[notification.channels.webhook]
enabled = false  # Off by default
min_level = "WARNING"  # Only warnings and above trigger webhooks
endpoints = []  # Add webhook URLs
headers = {}  # Add custom headers for webhook requests

# Database storage configuration
[notification.channels.database]
enabled = true
min_level = "INFO"
max_age_days = 30  # Automatic cleanup of old notifications

# Throttling to prevent notification flooding
[notification.throttling]
enabled = true
window_seconds = 60  # Time window for throttling (in seconds)
max_similar = 5  # Maximum number of similar notifications in the window

