# Core Architectural Principles

### 1. Layered Architecture Design

Restructure the codebase into distinct layers with clear, unidirectional dependencies:

```
core/               # Zero external dependencies
  ├── models/       # Base data models
  ├── interfaces/   # Abstract interfaces
  ├── constants/    # Configuration constants
  ├── exceptions/   # Custom exceptions
  └── utils/        # Pure utility functions

infrastructure/     # Depends only on core
  ├── io/           # File operations, async I/O
  ├── config/       # Configuration management
  ├── persistence/  # Storage mechanisms
  └── security/     # Safety and validation

services/           # Depends on core and infrastructure
  ├── file_mgmt/    # File management services
  ├── metadata/     # Metadata extraction services
  ├── search/       # Search services
  └── cache/        # Caching services

domain/             # Domain-specific functionality
  ├── file_reader/  # File reading logic
  ├── processors/   # Content processors
  └── analyzers/    # Content analysis

api/                # Application interfaces
  ├── rest/         # REST API endpoints
  ├── cli/          # Command-line interfaces
  └── external/     # Third-party integrations
```

### 2. Dependency Inversion Pattern

Each component should depend on abstractions, not concrete implementations:

```python
# Before
from the_aichemist_codex.backend.utils.cache_manager import CacheManager

# After
from the_aichemist_codex.backend.core.interfaces import ICacheProvider
```

### 3. Registry Pattern for Singletons

Create a centralized registry for singleton instances:

```python
# registry.py
from the_aichemist_codex.backend.core.interfaces import ICacheProvider, IFileManager
from the_aichemist_codex.backend.services.cache.cache_manager import CacheManager
from the_aichemist_codex.backend.services.file_mgmt.file_manager import FileManager

# Registry provides interface-based access to singletons
class Registry:
    _instance = None

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = Registry()
        return cls._instance

    def __init__(self):
        self._cache_provider = None
        self._file_manager = None

    @property
    def cache_provider(self) -> ICacheProvider:
        if self._cache_provider is None:
            self._cache_provider = CacheManager()
        return self._cache_provider

    @property
    def file_manager(self) -> IFileManager:
        if self._file_manager is None:
            self._file_manager = FileManager()
        return self._file_manager
```

## Detailed Solutions for Each Circular Dependency

### 1. settings.py and directory_manager.py

**Current Issue:**
- `settings.py` imports `DirectoryManager`
- Both need project path information

**Solution:**
1. Create `core/paths.py` for path resolution
2. Define `ProjectPaths` interface in `core/interfaces.py`
3. Implement in `infrastructure/config/paths.py`

```python
# core/interfaces.py
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional

class ProjectPaths(ABC):
    @abstractmethod
    def get_project_root(self) -> Path:
        """Return the project root directory."""
        pass

    @abstractmethod
    def get_cache_dir(self) -> Path:
        """Return the cache directory."""
        pass

    @abstractmethod
    def resolve_path(self, path: str, base_dir: Optional[Path] = None) -> Path:
        """Resolve a path relative to the specified base directory."""
        pass

# infrastructure/config/paths.py
import os
from pathlib import Path
from typing import Optional

from the_aichemist_codex.backend.core.interfaces import ProjectPaths

class ProjectPathsImpl(ProjectPaths):
    def get_project_root(self) -> Path:
        """Determine the project root directory."""
        # Implementation moved from settings.py
        current_dir = Path(__file__).resolve().parent
        while current_dir.name != "the_aichemist_codex" and current_dir.parent != current_dir:
            current_dir = current_dir.parent
        return current_dir

    def get_cache_dir(self) -> Path:
        """Return the cache directory."""
        return self.get_project_root() / ".cache"

    def resolve_path(self, path: str, base_dir: Optional[Path] = None) -> Path:
        """Resolve a path relative to the specified base directory."""
        base = base_dir or self.get_project_root()
        return (base / path).resolve()

# registry.py - Add to the registry
@property
def project_paths(self) -> ProjectPaths:
    if self._project_paths is None:
        self._project_paths = ProjectPathsImpl()
    return self._project_paths
```

**Updates to dependent modules:**

```python
# infrastructure/config/settings.py
from the_aichemist_codex.backend.registry import Registry

def get_project_root():
    return Registry.get_instance().project_paths.get_project_root()

# infrastructure/file_mgmt/directory_manager.py
from the_aichemist_codex.backend.registry import Registry

class DirectoryManager:
    def __init__(self):
        self._paths = Registry.get_instance().project_paths

    def ensure_directory_exists(self, directory_path):
        path = self._paths.resolve_path(directory_path)
        os.makedirs(path, exist_ok=True)
        return path
```

### 2. async_io.py, safety.py, and config_loader.py

**Current Issue:**
- Circular chain: async_io → safety → config_loader → (potentially back to async_io)

**Solution:**
1. Create file validation interfaces in core
2. Move pure file safety operations to infrastructure
3. Use registry for configuration access

```python
# core/interfaces.py
class FileValidator(ABC):
    @abstractmethod
    def is_safe_path(self, path: str, base_dir: Optional[Path] = None) -> bool:
        """Check if a path is safe to access."""
        pass

    @abstractmethod
    def sanitize_filename(self, filename: str) -> str:
        """Sanitize a filename to be safe for file operations."""
        pass

class ConfigProvider(ABC):
    @abstractmethod
    def get_config(self, section: str, key: str, default=None):
        """Get a configuration value."""
        pass

# infrastructure/security/file_validator.py
import os
import re
from pathlib import Path
from typing import Optional

from the_aichemist_codex.backend.core.interfaces import FileValidator
from the_aichemist_codex.backend.registry import Registry

class FileValidatorImpl(FileValidator):
    def __init__(self):
        self._paths = Registry.get_instance().project_paths

    def is_safe_path(self, path: str, base_dir: Optional[Path] = None) -> bool:
        """Check if a path is safe to access."""
        base = base_dir or self._paths.get_project_root()
        try:
            # Resolve to absolute path
            target_path = self._paths.resolve_path(path, base)
            # Check if the resolved path is within the base directory
            common_path = os.path.commonpath([base, target_path])
            return common_path == str(base)
        except (ValueError, TypeError):
            return False

    def sanitize_filename(self, filename: str) -> str:
        """Sanitize a filename to be safe for file operations."""
        # Remove characters that aren't allowed in filenames
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # Limit length
        return sanitized[:255]

# infrastructure/config/config_provider.py
import json
from pathlib import Path

from the_aichemist_codex.backend.core.interfaces import ConfigProvider
from the_aichemist_codex.backend.registry import Registry

class ConfigProviderImpl(ConfigProvider):
    def __init__(self):
        self._paths = Registry.get_instance().project_paths
        self._config_cache = {}
        self._load_config()

    def _load_config(self):
        config_path = self._paths.get_project_root() / "config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                self._config_cache = json.load(f)

    def get_config(self, section: str, key: str, default=None):
        if section in self._config_cache and key in self._config_cache[section]:
            return self._config_cache[section][key]
        return default

# registry.py - Add to the registry
@property
def file_validator(self) -> FileValidator:
    if self._file_validator is None:
        self._file_validator = FileValidatorImpl()
    return self._file_validator

@property
def config_provider(self) -> ConfigProvider:
    if self._config_provider is None:
        self._config_provider = ConfigProviderImpl()
    return self._config_provider
```

**Updates to dependent modules:**

```python
# infrastructure/io/async_io.py
import aiofiles
from the_aichemist_codex.backend.registry import Registry

class AsyncFileIO:
    def __init__(self):
        self._validator = Registry.get_instance().file_validator

    async def read_file(self, path, encoding='utf-8'):
        if not self._validator.is_safe_path(path):
            raise ValueError(f"Unsafe path: {path}")

        try:
            async with aiofiles.open(path, mode='r', encoding=encoding) as f:
                return await f.read()
        except Exception as e:
            # Handle exception
            raise

# No more import from safety.py - using registry instead
```

### 3. cache_manager.py and async_io.py

**Current Issue:**
- cache_manager imports AsyncFileIO
- Potential back-reference from async_io

**Solution:**
1. Define a CacheProvider interface
2. Make AsyncFileIO injectable into CacheManager
3. Use registry for access

```python
# core/interfaces.py
class CacheProvider(ABC):
    @abstractmethod
    async def get(self, key: str, default=None):
        """Get a value from the cache."""
        pass

    @abstractmethod
    async def set(self, key: str, value, ttl=None):
        """Set a value in the cache."""
        pass

    @abstractmethod
    async def delete(self, key: str):
        """Delete a value from the cache."""
        pass

class AsyncIO(ABC):
    @abstractmethod
    async def read_file(self, path: str, encoding: str = 'utf-8'):
        """Read a file asynchronously."""
        pass

    @abstractmethod
    async def write_file(self, path: str, content, encoding: str = 'utf-8'):
        """Write to a file asynchronously."""
        pass

# services/cache/cache_manager.py
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional

from the_aichemist_codex.backend.core.interfaces import CacheProvider, AsyncIO
from the_aichemist_codex.backend.registry import Registry

class CacheManager(CacheProvider):
    def __init__(self):
        self._paths = Registry.get_instance().project_paths
        self._io = Registry.get_instance().async_io
        self._cache_dir = self._paths.get_cache_dir()
        os.makedirs(self._cache_dir, exist_ok=True)

    def _get_cache_path(self, key: str) -> Path:
        # Convert key to a valid filename
        sanitized_key = Registry.get_instance().file_validator.sanitize_filename(key)
        return self._cache_dir / f"{sanitized_key}.json"

    async def get(self, key: str, default=None):
        path = self._get_cache_path(key)
        if not path.exists():
            return default

        try:
            content = await self._io.read_file(str(path))
            data = json.loads(content)

            # Check if entry is expired
            if 'expiry' in data and datetime.fromisoformat(data['expiry']) < datetime.now():
                await self.delete(key)
                return default

            return data.get('value', default)
        except Exception:
            return default

    async def set(self, key: str, value, ttl=None):
        path = self._get_cache_path(key)
        data = {'value': value}

        if ttl is not None:
            expiry = datetime.now() + timedelta(seconds=ttl)
            data['expiry'] = expiry.isoformat()

        await self._io.write_file(str(path), json.dumps(data))

    async def delete(self, key: str):
        path = self._get_cache_path(key)
        if path.exists():
            os.remove(path)

# infrastructure/io/async_io_impl.py
import aiofiles
from the_aichemist_codex.backend.core.interfaces import AsyncIO
from the_aichemist_codex.backend.registry import Registry

class AsyncIOImpl(AsyncIO):
    def __init__(self):
        self._validator = Registry.get_instance().file_validator

    async def read_file(self, path: str, encoding: str = 'utf-8'):
        if not self._validator.is_safe_path(path):
            raise ValueError(f"Unsafe path: {path}")

        try:
            async with aiofiles.open(path, mode='r', encoding=encoding) as f:
                return await f.read()
        except Exception as e:
            # Handle exception
            raise

    async def write_file(self, path: str, content, encoding: str = 'utf-8'):
        if not self._validator.is_safe_path(path):
            raise ValueError(f"Unsafe path: {path}")

        try:
            async with aiofiles.open(path, mode='w', encoding=encoding) as f:
                await f.write(content)
        except Exception as e:
            # Handle exception
            raise

# registry.py - Add to the registry
@property
def async_io(self) -> AsyncIO:
    if self._async_io is None:
        self._async_io = AsyncIOImpl()
    return self._async_io

@property
def cache_provider(self) -> CacheProvider:
    if self._cache_provider is None:
        self._cache_provider = CacheManager()
    return self._cache_provider
```

### 4. environment.py and settings.py

**Current Issue:**
- environment.py imports from settings.py
- settings.py may import from utils (which includes environment.py)

**Solution:**
1. Move environment detection to core
2. Make settings depend on environment, not vice versa

```python
# core/environment.py
import os
import sys
from enum import Enum
from typing import Dict, Any

class Environment(Enum):
    DEVELOPMENT = "development"
    TESTING = "testing"
    PRODUCTION = "production"

def detect_environment() -> Environment:
    """Detect the current environment."""
    env_var = os.environ.get("AICHEMIST_ENV", "").lower()

    if env_var == "prod" or env_var == "production":
        return Environment.PRODUCTION
    elif env_var == "test" or env_var == "testing":
        return Environment.TESTING
    elif "pytest" in sys.modules:
        return Environment.TESTING
    else:
        return Environment.DEVELOPMENT

# infrastructure/config/settings.py
from the_aichemist_codex.backend.core.environment import Environment, detect_environment
from the_aichemist_codex.backend.registry import Registry

class Settings:
    def __init__(self):
        self._paths = Registry.get_instance().project_paths
        self._config = Registry.get_instance().config_provider
        self._environment = detect_environment()
        self._settings = self._load_settings()

    def _load_settings(self) -> Dict[str, Any]:
        settings = {
            # Default settings for all environments
            "base_settings": {
                "timeout": 30,
                "max_retries": 3,
            },
            # Environment-specific overrides
            Environment.DEVELOPMENT: {
                "debug": True,
                "log_level": "DEBUG",
            },
            Environment.TESTING: {
                "debug": True,
                "log_level": "DEBUG",
                "use_mocks": True,
            },
            Environment.PRODUCTION: {
                "debug": False,
                "log_level": "INFO",
            }
        }

        # Combine base settings with environment-specific ones
        result = settings["base_settings"].copy()
        if self._environment in settings:
            result.update(settings[self._environment])

        return result

    @property
    def environment(self) -> Environment:
        return self._environment

    @property
    def debug(self) -> bool:
        return self._settings.get("debug", False)

    @property
    def log_level(self) -> str:
        return self._settings.get("log_level", "INFO")

    @property
    def project_root(self):
        return self._paths.get_project_root()

# registry.py - Add to the registry
@property
def settings(self) -> Settings:
    if self._settings is None:
        self._settings = Settings()
    return self._settings
```

### 5. file_tree.py and cache_manager.py

**Current Issue:**
- file_tree.py imports cache_manager
- Potential circular dependency if cache_manager needs file_tree

**Solution:**
1. Define FileTree interface in core
2. Inject cache dependency via registry
3. Use abstract interfaces for both components

```python
# core/interfaces.py
class FileTree(ABC):
    @abstractmethod
    async def get_directory_tree(self, directory: str, max_depth: int = None):
        """Get a tree representation of a directory."""
        pass

    @abstractmethod
    async def find_files(self, directory: str, pattern: str):
        """Find files matching a pattern in a directory."""
        pass

# services/file_mgmt/file_tree.py
import os
import re
from pathlib import Path
from typing import Dict, List, Optional

from the_aichemist_codex.backend.core.interfaces import FileTree
from the_aichemist_codex.backend.registry import Registry

class FileTreeImpl(FileTree):
    def __init__(self):
        self._paths = Registry.get_instance().project_paths
        self._validator = Registry.get_instance().file_validator
        self._cache = Registry.get_instance().cache_provider

    async def get_directory_tree(self, directory: str, max_depth: int = None):
        # Generate cache key
        cache_key = f"dir_tree_{directory}_{max_depth}"

        # Try to get from cache first
        cached = await self._cache.get(cache_key)
        if cached:
            return cached

        # Not in cache, generate tree
        base_path = self._paths.resolve_path(directory)
        if not base_path.exists() or not base_path.is_dir():
            return None

        result = await self._build_tree(base_path, base_path, 1, max_depth)

        # Cache the result
        await self._cache.set(cache_key, result, ttl=3600)  # Cache for 1 hour

        return result

    async def _build_tree(self, base_path: Path, current_path: Path,
                         current_depth: int, max_depth: Optional[int]):
        if max_depth is not None and current_depth > max_depth:
            return {"name": current_path.name, "type": "directory", "children": []}

        result = {
            "name": current_path.name,
            "type": "directory",
            "children": []
        }

        try:
            entries = sorted(os.listdir(current_path))

            for entry in entries:
                entry_path = current_path / entry

                # Skip hidden files/directories
                if entry.startswith('.'):
                    continue

                if entry_path.is_dir():
                    child = await self._build_tree(
                        base_path, entry_path, current_depth + 1, max_depth
                    )
                    result["children"].append(child)
                else:
                    result["children"].append({
                        "name": entry,
                        "type": "file",
                        "path": str(entry_path.relative_to(base_path))
                    })
        except (PermissionError, FileNotFoundError):
            # Handle access errors
            pass

        return result

    async def find_files(self, directory: str, pattern: str):
        # Generate cache key
        cache_key = f"find_files_{directory}_{pattern}"

        # Try to get from cache first
        cached = await self._cache.get(cache_key)
        if cached:
            return cached

        # Not in cache, search files
        base_path = self._paths.resolve_path(directory)
        if not base_path.exists() or not base_path.is_dir():
            return []

        pattern_compiled = re.compile(pattern)
        results = []

        for root, _, files in os.walk(base_path):
            root_path = Path(root)

            for file in files:
                if pattern_compiled.search(file):
                    file_path = root_path / file
                    results.append(str(file_path.relative_to(base_path)))

        # Cache the result
        await self._cache.set(cache_key, results, ttl=1800)  # Cache for 30 minutes

        return results

# registry.py - Add to the registry
@property
def file_tree(self) -> FileTree:
    if self._file_tree is None:
        self._file_tree = FileTreeImpl()
    return self._file_tree
```

### 6. file_reader.py and metadata.py

**Current Issue:**
- Bidirectional dependency: file_reader.py imports MetadataManager, metadata.py imports FileMetadata

**Solution:**
1. Create shared models in core
2. Use dependency injection for metadata extractors
3. Define clear interfaces for both components

```python
# core/models.py
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any

@dataclass
class FileMetadata:
    """Metadata for a file."""
    path: str
    filename: str
    extension: str
    size: int
    created_time: datetime
    modified_time: datetime
    content_type: str

    # Metadata extracted from file content
    title: Optional[str] = None
    author: Optional[str] = None
    created_date: Optional[datetime] = None
    keywords: List[str] = field(default_factory=list)
    summary: Optional[str] = None

    # File-type specific metadata
    specific_metadata: Dict[str, Any] = field(default_factory=dict)

# core/interfaces.py
class FileReader(ABC):
    @abstractmethod
    async def read_file(self, path: str) -> str:
        """Read the content of a file."""
        pass

    @abstractmethod
    async def get_file_metadata(self, path: str) -> FileMetadata:
        """Get metadata for a file."""
        pass

class MetadataExtractor(ABC):
    @abstractmethod
    async def extract_metadata(self, file_path: str, content: Optional[str] = None) -> FileMetadata:
        """Extract metadata from a file."""
        pass

    @abstractmethod
    def supports_file_type(self, file_extension: str) -> bool:
        """Check if this extractor supports the given file type."""
        pass

class MetadataManager(ABC):
    @abstractmethod
    async def get_metadata(self, file_path: str) -> FileMetadata:
        """Get metadata for a file."""
        pass

    @abstractmethod
    def register_extractor(self, extractor: MetadataExtractor):
        """Register a metadata extractor."""
        pass

# services/file_reader/file_reader_impl.py
import os
from datetime import datetime
from mimetypes import guess_type
from pathlib import Path

from the_aichemist_codex.backend.core.interfaces import FileReader
from the_aichemist_codex.backend.core.models import FileMetadata
from the_aichemist_codex.backend.registry import Registry

class FileReaderImpl(FileReader):
    def __init__(self):
        self._io = Registry.get_instance().async_io
        self._validator = Registry.get_instance().file_validator
        self._paths = Registry.get_instance().project_paths

    async def read_file(self, path: str) -> str:
        full_path = self._paths.resolve_path(path)
        if not self._validator.is_safe_path(str(full_path)):
            raise ValueError(f"Unsafe path: {path}")

        return await self._io.read_file(str(full_path))

    async def get_file_metadata(self, path: str) -> FileMetadata:
        # Get basic file metadata
        full_path = self._paths.resolve_path(path)
        if not self._validator.is_safe_path(str(full_path)):
            raise ValueError(f"Unsafe path: {path}")

        file_stats = os.stat(full_path)
        filename = full_path.name
        extension = full_path.suffix.lower()[1:] if full_path.suffix else ""
        content_type, _ = guess_type(str(full_path)) or ("application/octet-stream", None)

        # Create basic metadata
        metadata = FileMetadata(
            path=str(full_path),
            filename=filename,
            extension=extension,
            size=file_stats.st_size,
            created_time=datetime.fromtimestamp(file_stats.st_ctime),
            modified_time=datetime.fromtimestamp(file_stats.st_mtime),
            content_type=content_type
        )

        # Get additional metadata from the metadata manager
        try:
            metadata_manager = Registry.get_instance().metadata_manager
            enhanced_metadata = await metadata_manager.get_metadata(str(full_path))

            # Update with enhanced metadata
            for field_name in [
                'title', 'author', 'created_date', 'keywords',
                'summary', 'specific_metadata'
            ]:
                enhanced_value = getattr(enhanced_metadata, field_name, None)
                if enhanced_value is not None:
                    setattr(metadata, field_name, enhanced_value)

        except Exception:
            # If metadata extraction fails, just return basic metadata
            pass

        return metadata

# services/metadata/manager_impl.py
from typing import Dict, List, Optional

from the_aichemist_codex.backend.core.interfaces import MetadataExtractor, MetadataManager
from the_aichemist_codex.backend.core.models import FileMetadata
from the_aichemist_codex.backend.registry import Registry

class MetadataManagerImpl(MetadataManager):
    def __init__(self):
        self._extractors: Dict[str, List[MetadataExtractor]] = {}
        self._cache = Registry.get_instance().cache_provider
        self._io = Registry.get_instance().async_io
        self._paths = Registry.get_instance().project_paths

    def register_extractor(self, extractor: MetadataExtractor):
        """Register a metadata extractor."""
        for ext in [ext.lower() for ext in extractor.supported_extensions]:
            if ext not in self._extractors:
                self._extractors[ext] = []
            self._extractors[ext].append(extractor)

    async def get_metadata(self, file_path: str) -> FileMetadata:
        """Get metadata for a file."""
        # Generate cache key
        cache_key = f"metadata_{file_path}"

        # Try to get from cache first
        cached = await self._cache.get(cache_key)
        if cached:
            # Convert dict back to FileMetadata
            return FileMetadata(**cached)

        # Not in cache, extract metadata
        path = self._paths.resolve_path(file_path)
        extension = path.suffix.lower()[1:] if path.suffix else ""

        # Default basic metadata
        file_reader = Registry.get_instance().file_reader
        metadata = await file_reader.get_file_metadata(str(path))

        # Get content for extractors if needed
        content = None
        if extension in self._extractors:
            try:
                content = await self._io.read_file(str(path))
            except Exception:
                # If we can't read the file content, proceed with just the basic metadata
                pass

            # Apply each extractor in sequence
            for extractor in self._extractors[extension]:
                try:
                    enhanced_metadata = await extractor.extract_metadata(str(path), content)

                    # Update metadata with extracted values (only if they're not None)
                    for field_name in [
                        'title', 'author', 'created_date', 'keywords',
                        'summary', 'specific_metadata'
                    ]:
                        enhanced_value = getattr(enhanced_metadata, field_name, None)
                        if enhanced_value is not None:
                            current_value = getattr(metadata, field_name, None)
                            if current_value is None or enhanced_value:
                                setattr(metadata, field_name, enhanced_value)

                except Exception:
                    # Continue with next extractor if one fails
                    continue

        # Cache the result (convert to dict for serialization)
        await self._cache.set(cache_key, metadata.__dict__, ttl=3600)

        return metadata

# registry.py - Add to the registry
@property
def file_reader(self) -> FileReader:
    if self._file_reader is None:
        self._file_reader = FileReaderImpl()
    return self._file_reader

@property
def metadata_manager(self) -> MetadataManager:
    if self._metadata_manager is None:
        self._metadata_manager = MetadataManagerImpl()
    return self._metadata_manager
```

### 7. search_engine.py and multiple utilities

**Current Issue:**
- search_engine.py has many dependencies
- Risk of circular dependencies with these modules

**Solution:**
1. Define clear search interfaces
2. Use dependency injection for all components
3. Implement a facade pattern to simplify access

```python
# core/interfaces.py
class SearchEngine(ABC):
    @abstractmethod
    async def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for documents matching the query."""
        pass

    @abstractmethod
    async def index_file(self, file_path: str):
        """Index a file in the search engine."""
        pass

    @abstractmethod
    async def remove_file_from_index(self, file_path: str):
        """Remove a file from the search index."""
        pass

# services/search/search_engine_impl.py
from typing import Dict, List, Any, Optional

from the_aichemist_codex.backend.core.interfaces import SearchEngine
from the_aichemist_codex.backend.registry import Registry

class SearchEngineImpl(SearchEngine):
    def __init__(self):
        self._file_reader = Registry.get_instance().file_reader
        self._metadata_manager = Registry.get_instance().metadata_manager
        self._cache = Registry.get_instance().cache_provider
        self._paths = Registry.get_instance().project_paths
        self._settings = Registry.get_instance().settings

        # In a real implementation, you might initialize a vector database or other search backend
        self._index = {}

    async def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for documents matching the query."""
        # In a real implementation, this would perform vector search, keyword search, etc.
        # For now, simplifying to avoid introducing external dependencies
        results = []

        for file_path, data in self._index.items():
            # Simple scoring logic for demonstration
            score = 0

            # Check title match
            if data.get("title") and query.lower() in data["title"].lower():
                score += 10

            # Check content match
            if data.get("content") and query.lower() in data["content"].lower():
                score += 5

            # Check keyword match
            keywords = data.get("keywords", [])
            if query.lower() in [k.lower() for k in keywords]:
                score += 8

            if score > 0:
                results.append({
                    "path": file_path,
                    "score": score,
                    "title": data.get("title", ""),
                    "snippet": self._get_snippet(data.get("content", ""), query)
                })

        # Sort by score and limit results
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:limit]

    def _get_snippet(self, content: str, query: str, context_size: int = 50) -> str:
        """Get a snippet of text showing the query in context."""
        if not content or not query:
            return ""

        query_pos = content.lower().find(query.lower())
        if query_pos == -1:
            return content[:100] + "..."

        start = max(0, query_pos - context_size)
        end = min(len(content), query_pos + len(query) + context_size)

        prefix = "..." if start > 0 else ""
        suffix = "..." if end < len(content) else ""

        return prefix + content[start:end] + suffix

    async def index_file(self, file_path: str):
        """Index a file in the search engine."""
        try:
            # Get file content
            content = await self._file_reader.read_file(file_path)

            # Get metadata
            metadata = await self._metadata_manager.get_metadata(file_path)

            # Add to index
            self._index[file_path] = {
                "content": content,
                "title": metadata.title or file_path.split("/")[-1],
                "keywords": metadata.keywords,
                "metadata": metadata
            }

            # In a real implementation, you'd build embeddings and store in a vector DB

        except Exception as e:
            # Log error
            print(f"Error indexing file {file_path}: {str(e)}")

    async def remove_file_from_index(self, file_path: str):
        """Remove a file from the search index."""
        if file_path in self._index:
            del self._index[file_path]

# registry.py - Add to the registry
@property
def search_engine(self) -> SearchEngine:
    if self._search_engine is None:
        self._search_engine = SearchEngineImpl()
    return self._search_engine
```

### 8. metadata extractors and cache_manager

**Current Issue:**
- Metadata extractors depend on CacheManager
- Potential circular dependency if CacheManager uses metadata

**Solution:**
1. Use dependency injection for CacheManager in extractors
2. Define clear extractor interfaces
3. Register extractors through registry

```python
# core/interfaces.py
class PDFExtractor(ABC):
    @abstractmethod
    async def extract_metadata(self, file_path: str, content: Optional[str] = None) -> FileMetadata:
        """Extract metadata from a PDF file."""
        pass

class ImageExtractor(ABC):
    @abstractmethod
    async def extract_metadata(self, file_path: str, content: Optional[bytes] = None) -> FileMetadata:
        """Extract metadata from an image file."""
        pass

# services/metadata/extractors/pdf_extractor.py
import re
from datetime import datetime
from typing import Optional, Dict, Any

from the_aichemist_codex.backend.core.interfaces import MetadataExtractor
from the_aichemist_codex.backend.core.models import FileMetadata
from the_aichemist_codex.backend.registry import Registry

class PDFExtractorImpl(MetadataExtractor):
    def __init__(self):
        self._cache = Registry.get_instance().cache_provider

    @property
    def supported_extensions(self):
        return ["pdf"]

    def supports_file_type(self, file_extension: str) -> bool:
        return file_extension.lower() == "pdf"

    async def extract_metadata(self, file_path: str, content: Optional[str] = None) -> FileMetadata:
        # Try to get from cache
        cache_key = f"pdf_metadata_{file_path}"
        cached = await self._cache.get(cache_key)
        if cached:
            return FileMetadata(**cached)

        # Not in cache, extract metadata
        try:
            # In a real implementation, you'd use a PDF library like PyPDF2 or pdfminer
            # For this example, we'll create placeholder metadata
            metadata = FileMetadata(
                path=file_path,
                filename=file_path.split("/")[-1],
                extension="pdf",
                size=0,  # Would get real size
                created_time=datetime.now(),
                modified_time=datetime.now(),
                content_type="application/pdf",
                title="Sample PDF Document",
                author="Unknown Author",
                created_date=datetime.now(),
                keywords=["pdf", "sample"],
                summary="This is a sample PDF document",
                specific_metadata={
                    "page_count": 10,
                    "is_encrypted": False
                }
            )

            # Cache the result
            await self._cache.set(cache_key, metadata.__dict__, ttl=3600)

            return metadata

        except Exception as e:
            # Log error
            print(f"Error extracting PDF metadata from {file_path}: {str(e)}")

            # Return basic metadata
            return FileMetadata(
                path=file_path,
                filename=file_path.split("/")[-1],
                extension="pdf",
                size=0,
                created_time=datetime.now(),
                modified_time=datetime.now(),
                content_type="application/pdf"

