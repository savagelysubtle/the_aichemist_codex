# Comprehensive Circular Import Analysis and Resolution Plan

## Identified Circular Dependencies

### 1. settings.py and directory_manager.py

- In `backend/config/settings.py`, there's an import of `DirectoryManager` from
  `backend.file_manager.directory_manager`
- This creates a potential circular dependency since other modules might import
  settings, which imports directory_manager
- **Detailed Analysis**: `settings.py` uses `DirectoryManager` for file path
  resolution, while directory_manager might need settings for configuration
  values. The direct import creates a tight coupling between these foundational
  modules.

### 2. async_io.py and safety.py (Three-Way Dependency)

- In `backend/utils/async_io.py`, there's an import from `backend.utils.safety`
  for `SafeFileHandler`
- In `backend/utils/safety.py`, there's an import from
  `backend.config.config_loader` for `config`
- This creates a complex three-way dependency chain: async_io → safety →
  config_loader → (potentially back to utils)
- **Detailed Analysis**: The `SafeFileHandler` is used for path validation
  before file operations, while config_loader provides configuration values for
  safety checks

### 3. cache_manager.py and async_io.py

- In `backend/utils/cache_manager.py`, there's an import from
  `backend.utils.async_io` for `AsyncFileIO`
- This could cause issues if async_io ever tries to use the cache_manager
- **Detailed Analysis**: `cache_manager.py` uses `AsyncFileIO` for file
  operations, but defines `get_cache_dir()` at the top level to avoid circular
  imports. The `cache_manager` is instantiated as a singleton at the module
  level, which could cause issues if async_io ever tries to access it.

### 4. environment.py and settings.py

- In `backend/utils/environment.py`, there's an import from
  `backend.config.settings` for `determine_project_root`
- This creates a circular dependency when `settings.py` imports anything from
  the `utils` module
- This is a particularly concerning circular dependency as both modules are
  foundational to the project
- **Detailed Analysis**: environment.py only imports a single function from
  settings.py (`determine_project_root`), making this a good candidate for
  function extraction. This dependency is particularly problematic because these
  are core modules used throughout the application.

### 5. file_tree.py and cache_manager.py

- In `backend/file_manager/file_tree.py`, there's an import of `cache_manager`
  from `backend.utils.cache_manager`
- This could create a circular dependency if the cache_manager ever imports from
  the file_manager module
- The `cache_manager` is instantiated as a singleton in the `cache_manager.py`
  file itself
- **Detailed Analysis**: file_tree.py uses the singleton cache_manager instance
  directly, creating a tight coupling. If cache_manager ever needs to handle
  file trees or import file management utilities, this would create a circular
  dependency.

### 6. file_reader.py and metadata.py

- In `backend/file_reader/file_reader.py`, there's an import from
  `backend.metadata.manager` for `MetadataManager`
- In `backend/metadata/manager.py`, there's import of file_reader components
  like `FileMetadata`
- This creates a bidirectional dependency between two major subsystems
- **Detailed Analysis**: These modules have a direct bidirectional dependency.
  file_reader.py needs metadata extraction capabilities, while
  metadata.manager.py needs the FileMetadata model from file_reader. This is a
  classic case where a shared data model should be extracted.

### 7. search_engine.py and multiple utilities

- `backend/search/search_engine.py` imports from several modules:
  - `backend.config.settings`
  - `backend.file_reader.file_metadata`
  - `backend.models.embeddings`
  - `backend.utils` (multiple utilities)
- This creates a risk of circular dependencies with any of these modules that
  might import from search
- **Detailed Analysis**: The search engine is a high-risk module that could
  easily introduce circular dependencies as other modules evolve. It imports
  from across the codebase, including config, file_reader, models, and utils
  modules.

### 8. metadata extractors and cache_manager

- Many metadata extractors (pdf_extractor.py, image_extractor.py, etc.) import
  `CacheManager` from `backend.utils.cache_manager`
- This creates potential circular dependencies if cache_manager ever needs to
  use metadata functionality
- **Detailed Analysis**: All extractors use the cache_manager singleton,
  creating a dependency from metadata to utils. If cache_manager ever needs to
  understand file types or metadata information, this would create a circular
  dependency.

## Detailed Implementation Plan

### Phase 1: Create Core Module (1-2 weeks)

#### 1. Core Module Structure

```
the_aichemist_codex/
└── backend/
    └── core/
        ├── __init__.py
        ├── paths.py           # Path resolution, project root detection
        ├── models/            # Shared data models
        │   ├── __init__.py
        │   ├── file_data.py   # FileMetadata and related models
        │   └── config_data.py # Configuration data structures
        ├── io/                # Basic I/O operations
        │   ├── __init__.py
        │   ├── async_base.py  # Base async I/O functions
        │   └── file_ops.py    # Basic file operations
        ├── validation/        # Validation logic
        │   ├── __init__.py
        │   ├── path_safety.py # Path validation logic
        │   └── validators.py  # Generic validators
        └── registry.py        # Central registry for singletons
```

#### 2. Implementation Details for Core Module

**a. core/paths.py**

```python
"""Core path resolution functionality."""
import os
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

def determine_project_root() -> Path:
    """
    Determine the project root directory using multiple methods.
    Extracted from settings.py to avoid circular imports.

    Returns:
        Path: The detected project root directory
    """
    # 1. First priority: Check environment variable
    env_root = os.environ.get("AICHEMIST_ROOT_DIR")
    if env_root:
        root_dir = Path(env_root).resolve()
        if root_dir.exists():
            logger.info(f"Using root directory from environment: {root_dir}")
            return root_dir
        else:
            logger.warning(f"Environment root directory doesn't exist: {root_dir}")

    # 2. Second priority: Look for repository indicators
    current_file = Path(__file__).resolve()
    # Go up several levels from core/paths.py to the project root
    potential_root = current_file.parent.parent.parent.parent

    # Check for indicators of project root (like README.md, pyproject.toml, etc.)
    indicators = ["README.md", "pyproject.toml", "setup.py", ".git"]
    for indicator in indicators:
        if (potential_root / indicator).exists():
            logger.info(f"Using detected project root: {potential_root}")
            return potential_root

    # 3. Fallback: Use the parent directory of the backend directory
    fallback_root = current_file.parent.parent.parent
    logger.warning(f"Using fallback project root: {fallback_root}")
    return fallback_root

def get_data_directory(create_if_missing: bool = True) -> Path:
    """
    Resolve the data directory path.
    Extracted from both settings.py and directory_manager.py.

    Args:
        create_if_missing: Whether to create the directory if it doesn't exist

    Returns:
        Path: The data directory path
    """
    # 1. First priority: Check environment variable
    env_data_dir = os.environ.get("AICHEMIST_DATA_DIR")
    if env_data_dir:
        data_dir = Path(env_data_dir).resolve()
    else:
        # 2. Second priority: Use standard location under project root
        project_root = determine_project_root()
        data_dir = project_root / "data"

    # Create directory if needed
    if create_if_missing and not data_dir.exists():
        data_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Created data directory: {data_dir}")

    return data_dir
```

**b. core/models/file_data.py**

```python
"""Core file data models shared across modules."""
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

@dataclass
class FileMetadata:
    """
    Common file metadata model used by both file_reader and metadata modules.
    Extracted from file_reader/file_metadata.py to avoid circular imports.
    """
    path: Path
    mime_type: str = ""
    size: int = 0
    modified_time: datetime = field(default_factory=datetime.now)
    created_time: datetime = field(default_factory=datetime.now)
    content_preview: str = ""
    extracted_text: str = ""
    extraction_successful: bool = False
    tags: List[str] = field(default_factory=list)
    additional_metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Update file stats if available."""
        if self.path.exists() and self.path.is_file():
            stats = self.path.stat()
            if not self.size:
                self.size = stats.st_size
            if not self.modified_time or self.modified_time == datetime.now():
                self.modified_time = datetime.fromtimestamp(stats.st_mtime)
            if not self.created_time or self.created_time == datetime.now():
                self.created_time = datetime.fromtimestamp(stats.st_ctime)
```

**c. core/io/async_base.py**

```python
"""Core asynchronous I/O operations."""
import logging
import os
from pathlib import Path
from typing import List, Optional, Union

import aiofiles

from the_aichemist_codex.backend.core.validation.path_safety import is_safe_path

logger = logging.getLogger(__name__)

async def read_text_file(file_path: Path, encoding: str = "utf-8") -> str:
    """
    Read text file asynchronously. Base implementation extracted from utils/async_io.py.

    Args:
        file_path: Path to the file
        encoding: Text encoding to use

    Returns:
        str: File content as text
    """
    try:
        async with aiofiles.open(file_path, "r", encoding=encoding) as file:
            return await file.read()
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        return ""

async def write_text_file(
    file_path: Path, content: str, encoding: str = "utf-8", create_dirs: bool = True
) -> bool:
    """
    Write text file asynchronously. Base implementation extracted from utils/async_io.py.

    Args:
        file_path: Path to write to
        content: Text content to write
        encoding: Text encoding to use
        create_dirs: Whether to create parent directories

    Returns:
        bool: Success status
    """
    try:
        if create_dirs:
            os.makedirs(file_path.parent, exist_ok=True)

        async with aiofiles.open(file_path, "w", encoding=encoding) as file:
            await file.write(content)
        return True
    except Exception as e:
        logger.error(f"Error writing to file {file_path}: {e}")
        return False
```

**d. core/validation/path_safety.py**

```python
"""Core path validation functionality."""
import logging
from pathlib import Path
from typing import List, Set

logger = logging.getLogger(__name__)

DEFAULT_IGNORE_PATTERNS = [
    "__pycache__",
    "*.pyc",
    ".git",
    ".vscode",
    ".idea",
    "venv",
    ".env",
    ".venv",
    "node_modules",
]

def is_safe_path(target: Path, base: Path) -> bool:
    """
    Ensures that a target path is within the base directory.
    Extracted from utils/safety.py to avoid circular imports.

    Args:
        target: The path to check
        base: The base directory that should contain the target

    Returns:
        bool: True if the path is safe, False otherwise
    """
    try:
        return base.resolve() in target.resolve().parents
    except (FileNotFoundError, RuntimeError):
        # If there's an error resolving paths, consider it unsafe
        return False

def should_ignore(file_path: Path, ignore_patterns: Set[str] = None) -> bool:
    """
    Checks if a file should be ignored based on patterns.
    Extracted from utils/safety.py to avoid circular imports.

    Args:
        file_path: The path to check
        ignore_patterns: Optional custom ignore patterns

    Returns:
        bool: True if the file should be ignored, False otherwise
    """
    if ignore_patterns is None:
        ignore_patterns = set(DEFAULT_IGNORE_PATTERNS)

    # Convert path to string for pattern matching
    path_str = str(file_path)

    # Check exact matches (directories)
    for pattern in ignore_patterns:
        if pattern in file_path.parts:
            return True

    # TODO: Add more sophisticated pattern matching as needed
    return False
```

**e. core/registry.py**

```python
"""Central registry for singletons and shared resources."""
import logging
from typing import Any, Dict, Optional

logger = logging.getLogger(__name__)

class Registry:
    """
    Central registry for managing singletons and shared resources.
    This helps avoid circular imports caused by direct imports of singletons.
    """

    _instance = None
    _registry: Dict[str, Any] = {}

    def __new__(cls):
        """Ensure singleton pattern for the registry itself."""
        if cls._instance is None:
            cls._instance = super(Registry, cls).__new__(cls)
            cls._instance._registry = {}
        return cls._instance

    def register(self, name: str, instance: Any) -> None:
        """
        Register an instance in the registry.

        Args:
            name: Name to register the instance under
            instance: The instance to register
        """
        self._registry[name] = instance
        logger.debug(f"Registered {name} in the registry")

    def get(self, name: str, default: Any = None) -> Any:
        """
        Get an instance from the registry.

        Args:
            name: Name of the registered instance
            default: Default value if not found

        Returns:
            The registered instance or default
        """
        return self._registry.get(name, default)

    def clear(self) -> None:
        """Clear all registrations."""
        self._registry.clear()

    def contains(self, name: str) -> bool:
        """Check if a name is registered."""
        return name in self._registry

# Create the singleton instance
registry = Registry()

# Function to get the registry instance
def get_registry() -> Registry:
    """Get the global registry instance."""
    return registry
```

### Phase 2: Refactor Config and Utils Modules (2-3 weeks)

#### 1. Update settings.py

```python
"""Global settings and configuration constants."""
import logging
import os
from pathlib import Path
from typing import Any, Dict, Optional

# Import from core instead of file_manager
from the_aichemist_codex.backend.core.paths import determine_project_root, get_data_directory

logger = logging.getLogger(__name__)

# Project root path
PROJECT_ROOT = determine_project_root()

# Data directory
DATA_DIR = get_data_directory()

# Other derived directories
CACHE_DIR = DATA_DIR / "cache"
LOG_DIR = DATA_DIR / "logs"
EXPORT_DIR = DATA_DIR / "exports"
BACKUP_DIR = DATA_DIR / "backup"

# Import remaining settings code...
```

#### 2. Update directory_manager.py

```python
"""Directory management utilities."""
import asyncio
import logging
import os
from pathlib import Path
from typing import List, Optional

# Import from core instead of circular dependency with settings
from the_aichemist_codex.backend.core.paths import get_data_directory

from the_aichemist_codex.backend.rollback.rollback_manager import RollbackManager

logger = logging.getLogger(__name__)
rollback_manager = RollbackManager()

class DirectoryManager:
    """
    Central manager for data directory access.

    This class provides standardized access to project data directories,
    handling creation, validation, and path resolution.
    """

    # Standard directory types
    STANDARD_DIRS = ["cache", "logs", "versions", "exports", "backup", "trash"]

    def __init__(self, base_dir: Optional[Path] = None):
        """
        Initialize with optional base directory override.

        Args:
            base_dir: Optional custom base directory path
        """
        # Now uses core function instead of a potential circular import
        self.base_dir = base_dir or get_data_directory()
        self._ensure_directories_exist()

    # Remaining implementation...
```

#### 3. Update async_io.py

```python
"""Asynchronous file operations for The Aichemist Codex."""
import asyncio
import logging
import os
from collections.abc import AsyncGenerator, AsyncIterable, Callable
from pathlib import Path
from typing import Any, Optional, List, Union, cast

import aiofiles

# Import from core instead of from safety.py
from the_aichemist_codex.backend.core.validation.path_safety import is_safe_path
from the_aichemist_codex.backend.core.io.async_base import read_text_file, write_text_file

logger = logging.getLogger(__name__)

class AsyncFileIO:
    """Provides comprehensive asynchronous file I/O utilities."""

    @staticmethod
    async def read_text(file_path: Path) -> str:
        """
        Reads file content asynchronously.

        Args:
            file_path: Path to the file to read

        Returns:
            The file content as a string, or an error message if the file can't be read
        """
        # Delegate to the core implementation
        return await read_text_file(file_path)

    # More implementation...
```

#### 4. Update safety.py

```python
"""Ensures file operations remain within safe directories and ignore patterns."""
import logging
from pathlib import Path
from typing import Set, List

# Import from core instead of from config_loader
from the_aichemist_codex.backend.core.validation.path_safety import (
    is_safe_path as core_is_safe_path,
    should_ignore as core_should_ignore,
    DEFAULT_IGNORE_PATTERNS,
)

logger = logging.getLogger(__name__)

class SafeFileHandler:
    """Provides validation utilities to ensure safe file operations."""

    @staticmethod
    def is_safe_path(target: Path, base: Path) -> bool:
        """
        Ensures that a target path is within the base directory.

        Args:
            target: The path to check
            base: The base directory that should contain the target

        Returns:
            bool: True if the path is safe, False otherwise
        """
        return core_is_safe_path(target, base)

    @staticmethod
    def should_ignore(file_path: Path, ignore_patterns: Set[str] = None) -> bool:
        """
        Checks if a file should be ignored based on patterns.

        Args:
            file_path: The path to check
            ignore_patterns: Optional custom ignore patterns

        Returns:
            bool: True if the file should be ignored, False otherwise
        """
        if ignore_patterns is None:
            # Get patterns from config
            from the_aichemist_codex.backend.config.config_loader import config
            custom_patterns = config.get("ignore_patterns", DEFAULT_IGNORE_PATTERNS)
            ignore_patterns = set(custom_patterns)

        return core_should_ignore(file_path, ignore_patterns)
```

#### 5. Update environment.py

```python
"""Environment detection utilities for dual-mode operation."""
import os
from pathlib import Path

# Import from core.paths instead of settings
from the_aichemist_codex.backend.core.paths import determine_project_root

def is_development_mode() -> bool:
    """
    Detect if running in development mode (not installed as package).

    This checks if we're running from source directories or as an installed package.

    Returns:
        bool: True if running from source, False if installed as package
    """
    # Check explicit environment variable first
    if os.environ.get("AICHEMIST_DEV_MODE"):
        return True

    # Check if running from source directory structure
    module_path = Path(__file__).resolve()
    src_parent = "src"

    # If we're in a src/the_aichemist_codex structure, we're in development mode
    return src_parent in module_path.parts

# Remaining implementation...
```

### Phase 3: Implement the Registry Pattern (1-2 weeks)

#### 1. Update cache_manager.py

```python
"""Provides caching capabilities for performance optimization."""
import logging
import os
import re
import time
from collections import OrderedDict
from pathlib import Path
from typing import Any, Dict, Optional

# Import from core.io instead of async_io
from the_aichemist_codex.backend.core.io.async_base import read_text_file, write_text_file
# Import registry
from the_aichemist_codex.backend.core.registry import get_registry

logger = logging.getLogger(__name__)

def get_cache_dir() -> Path:
    """
    Get the cache directory path dynamically to avoid circular imports.

    Returns:
        Path: The cache directory path
    """
    # Check for environment variable first
    env_cache_dir = os.environ.get("AICHEMIST_CACHE_DIR")
    if env_cache_dir:
        return Path(env_cache_dir).resolve()

    # Get from core paths
    from the_aichemist_codex.backend.core.paths import get_data_directory
    data_dir = get_data_directory()
    return data_dir / "cache"

# LRUCache and CacheManager implementation...

# Instead of direct singleton, register with central registry
def register_cache_manager():
    """Register the cache manager singleton with the central registry."""
    cache_mgr = CacheManager()
    get_registry().register("cache_manager", cache_mgr)
    return cache_mgr

# Initialize the cache manager, but allow it to be replaced in the registry
cache_manager = register_cache_manager()

# Function to get the cache manager from the registry
def get_cache_manager() -> "CacheManager":
    """Get the global cache manager instance."""
    registry = get_registry()
    return registry.get("cache_manager", cache_manager)
```

#### 2. Update all uses of cache_manager

Instead of directly importing `cache_manager`, update code to use:

```python
from the_aichemist_codex.backend.utils.cache_manager import get_cache_manager

# Then in code
cache_manager = get_cache_manager()
```

### Phase 4: Refactor File Reader and Metadata Modules (2 weeks)

#### 1. Update file_reader/file_metadata.py

```python
"""File metadata models."""
# Re-export from core to maintain backward compatibility
from the_aichemist_codex.backend.core.models.file_data import FileMetadata

# Add any additional file_reader specific metadata models
```

#### 2. Update file_reader/file_reader.py

```python
"""
File reading and parsing module for The Aichemist Codex.
"""
import asyncio
import logging
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Dict, List, Optional

# Import python-magic safely
try:
    import magic
except ImportError:
    magic = None

from the_aichemist_codex.backend.config import settings
# Import FileMetadata from core instead
from the_aichemist_codex.backend.core.models.file_data import FileMetadata
from the_aichemist_codex.backend.utils.async_io import AsyncFileIO
# Use get_cache_manager instead of direct import
from the_aichemist_codex.backend.utils.cache_manager import get_cache_manager

from .parsers import get_parser_for_mime_type

logger = logging.getLogger(__name__)

class FileReader:
    """Main class for reading and parsing files with MIME type detection."""

    def __init__(
        self,
        max_workers: int = 2,
        preview_length: int = 100,
        cache_manager = None,
    ):
        """Initialize FileReader."""
        self.max_workers = max_workers
        self.preview_length = preview_length
        self.cache_manager = cache_manager or get_cache_manager()

    # Implementation continues...

    async def get_metadata_manager(self):
        """
        Get a metadata manager instance when needed.
        This function-level import avoids circular dependencies.
        """
        from the_aichemist_codex.backend.metadata.manager import MetadataManager
        return MetadataManager(cache_manager=self.cache_manager)

    async def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        Extract metadata for a file using the appropriate extractor.
        Uses function-level import to avoid circular dependency.
        """
        metadata_manager = await self.get_metadata_manager()
        return await metadata_manager.extract_metadata(file_path)
```

#### 3. Update metadata/manager.py

```python
"""Metadata extraction manager."""
import asyncio
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

# Import from core instead of file_reader
from the_aichemist_codex.backend.core.models.file_data import FileMetadata
# Use get_cache_manager instead of direct import
from the_aichemist_codex.backend.utils.cache_manager import get_cache_manager
from the_aichemist_codex.backend.utils.mime_type_detector import MimeTypeDetector

from .extractor import BaseMetadataExtractor, MetadataExtractorRegistry

logger = logging.getLogger(__name__)

class MetadataManager:
    """Manager for coordinating metadata extraction."""

    def __init__(self, cache_manager = None):
        """Initialize the metadata manager."""
        self.cache_manager = cache_manager or get_cache_manager()
        self.mime_detector = MimeTypeDetector()

        # Initialize all available extractors
        self.extractors: Dict[str, BaseMetadataExtractor] = {}
        for extractor_class in MetadataExtractorRegistry.get_all_extractors():
            extractor = extractor_class(cache_manager=self.cache_manager)
            for mime_type in extractor.supported_mime_types:
                self.extractors[mime_type] = extractor
```

### Phase 5: Refactor Search Module (1-2 weeks)

Update search_engine.py to reduce import dependencies:

```python
"""Search engine implementation."""
import asyncio
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

# Import from core instead of settings
from the_aichemist_codex.backend.core.models.file_data import FileMetadata

# Function-level imports for providers
def _get_regex_provider():
    from the_aichemist_codex.backend.search.providers.regex_provider import RegexSearchProvider
    return RegexSearchProvider()

def _get_similarity_provider():
    from the_aichemist_codex.backend.search.providers.similarity_provider import SimilarityProvider
    return SimilarityProvider()

# Function-level import for settings
def _get_settings():
    from the_aichemist_codex.backend.config.settings import (
        FEATURES,
        REGEX_MAX_RESULTS,
        SIMILARITY_MAX_RESULTS,
    )
    return FEATURES, REGEX_MAX_RESULTS, SIMILARITY_MAX_RESULTS

# Get cache manager using function
from the_aichemist_codex.backend.utils.cache_manager import get_cache_manager

# Implementation continues...
```

## Testing and Verification Strategy

### 1. Static Import Analysis

Create a script that analyzes import dependencies and detects potential circular
imports:

```python
import ast
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple

def get_imports(file_path: Path) -> Tuple[List[str], List[str]]:
    """Extract imports from a Python file."""
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read())

    imports = []
    from_imports = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for name in node.names:
                imports.append(name.name)
        elif isinstance(node, ast.ImportFrom):
            if node.module:  # ignore relative imports with no module
                for name in node.names:
                    from_imports.append(f"{node.module}.{name.name}")

    return imports, from_imports

def build_dependency_graph(root_dir: Path) -> Dict[str, Set[str]]:
    """Build a dependency graph for all Python files."""
    graph = {}
    file_to_module = {}

    # Map file paths to module names
    for file_path in root_dir.glob("**/*.py"):
        rel_path = file_path.relative_to(root_dir)
        module_parts = list(rel_path.with_suffix("").parts)
        if module_parts[-1] == "__init__":
            module_parts.pop()  # Remove __init__
        module_name = ".".join(module_parts)
        file_to_module[file_path] = module_name
        graph[module_name] = set()

    # Build the graph
    for file_path, module_name in file_to_module.items():
        imports, from_imports = get_imports(file_path)

        for imp in imports:
            # Check if this is a project module
            if imp in graph:
                graph[module_name].add(imp)

        for imp in from_imports:
            # Get the module part
            parts = imp.split(".")
            for i in range(len(parts), 0, -1):
                prefix = ".".join(parts[:i])
                if prefix in graph:
                    graph[module_name].add(prefix)
                    break

    return graph

def find_cycles(graph: Dict[str, Set[str]]) -> List[List[str]]:
    """Find cycles in the dependency graph using DFS."""
    visited = set()
    rec_stack = set()
    cycles = []

    def dfs(node, path=None):
        if path is None:
            path = []

        visited.add(node)
        rec_stack.add(node)
        path = path + [node]

        for neighbor in graph.get(node, []):
            if neighbor not in visited:
                cycle = dfs(neighbor, path)
                if cycle:
                    cycles.append(cycle)
            elif neighbor in rec_stack:
                # Found a cycle
                cycle_start = path.index(neighbor)
                cycles.append(path[cycle_start:] + [neighbor])

        rec_stack.remove(node)
        return None

    for node in graph:
        if node not in visited:
            dfs(node)

    return cycles

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <project_root>")
        sys.exit(1)

    root_dir = Path(sys.argv[1])
    graph = build_dependency_graph(root_dir)
    cycles = find_cycles(graph)

    if cycles:
        print("Found circular dependencies:")
        for i, cycle in enumerate(cycles, 1):
            print(f"{i}. {' -> '.join(cycle)}")
    else:
        print("No circular dependencies found!")
```

### 2. Runtime Verification

Create a test script that dynamically verifies imports work without circular
dependencies:

```python
import importlib
import sys
from pathlib import Path
from typing import List, Set

def collect_modules(root_dir: Path) -> List[str]:
    """Collect all module names from Python files."""
    modules = []
    for file_path in root_dir.glob("**/*.py"):
        rel_path = file_path.relative_to(root_dir)
        if file_path.name == "__init__.py":
            # For __init__.py, use the directory as module
            module_name = ".".join(rel_path.parent.parts)
        else:
            # For other files, include the file name without extension
            module_name = ".".join(list(rel_path.parent.parts) + [file_path.stem])

        if module_name:
            modules.append(module_name)

    return modules

def test_imports(modules: List[str]) -> List[str]:
    """Try importing each module and report errors."""
    failed_modules = []

    for module_name in modules:
        try:
            importlib.import_module(module_name)
            print(f"✓ Successfully imported {module_name}")
        except Exception as e:
            print(f"✗ Failed to import {module_name}: {e}")
            failed_modules.append(f"{module_name}: {e}")

    return failed_modules

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <project_root>")
        sys.exit(1)

    root_dir = Path(sys.argv[1])
    sys.path.insert(0, str(root_dir))

    modules = collect_modules(root_dir)
    failed = test_imports(modules)

    if failed:
        print("\nThe following modules failed to import:")
        for failure in failed:
            print(f"  - {failure}")
        sys.exit(1)
    else:
        print("\nAll modules imported successfully!")
```

## Timeline and Phasing

### Week 1-2: Core Module Development

- Create core module structure
- Implement core path and validation functionality
- Create shared data models
- Set up registry pattern

### Week 3-4: Refactor Config and Utils

- Update settings.py and directory_manager.py
- Refactor async_io.py and safety.py
- Implement registry for cache_manager

### Week 5-6: Refactor File Reader and Metadata

- Update file_metadata.py to use core models
- Update file_reader.py to use function-level imports
- Update metadata manager

### Week 7-8: Refactor Search and Final Integration

- Update search_engine.py to reduce dependencies
- Refactor metadata extractors to use registry
- Integrate all changes
- Run verification tests

## Conclusion

This comprehensive refactoring plan addresses all identified circular
dependencies through several key strategies:

1. **Core Module:** Creating a foundational layer with minimal dependencies
2. **Shared Models:** Moving shared data structures to a central location
3. **Registry Pattern:** Removing singleton-related circular dependencies
4. **Function-level Imports:** Using delayed imports for specific circular cases
5. **Dependency Injection:** Passing dependencies rather than importing them

The implementation plan provides specific code examples and a phased approach to
minimize disruption while addressing the underlying architectural issues.

By following this plan, the_aichemist_codex will gain:

- Cleaner architecture with clear dependency directions
- Improved maintainability and testability
- Elimination of runtime errors from circular imports
- Better separation of concerns
- More scalable codebase for future development
